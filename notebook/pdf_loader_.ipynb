{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34a6fd7",
   "metadata": {},
   "source": [
    "### RAG Pipelines - Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "887db5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d9e19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF Files to process\n",
      "\n",
      "Processing: attention.pdf\n",
      " Loaded 15 pages\n",
      "\n",
      "Processing: embeddings.pdf\n",
      " Loaded 27 pages\n",
      "\n",
      "Processing: objectdetection.pdf\n",
      " Loaded 21 pages\n",
      "\n",
      "Total documents loaded: 63\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs(pdf_directory):\n",
    "    all_documents=[]\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF Files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\" Loaded {len(documents)} pages\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04494024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The\\ndata transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciﬁc training strate gies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-\\ning initial retrieval-focused pretraining followed by ful l-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more div erse data is crucial for\\nadvancing retrieval model performance, and that leveragin g LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-\\ntor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhan ced agent systems\\n∗ https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on ﬁne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiﬁcant performance improvements[ 4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ”signal states” repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe ﬁnal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance\\nof up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neﬃcient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiﬁcation. Our framework comprises two core components : 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc require ments of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extractio n from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s charact eristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and g eneralization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the ﬁrst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pa ir classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—suc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]—combined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model’s dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constrainin g tokens’ ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted in to a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ”Question/Query/User” portion is converted into que ries, while the\\n”Answer/Response/Assistant” portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical va lue of 1, while\\n”no”/”false” are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diﬀeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by us ing the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diﬀerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to\\nto enhance model training eﬃciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = − 1\\nn\\n∑\\ni\\nlog esim(qi,d +\\ni )/τ\\nesim(qi,d +\\ni )/τ + ∑\\nj esim(qi,d −\\nj )/τ + ∑\\nj̸=i esim(qi,q j )/τ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n∑\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) − sim(xi, x j)\\nτ )) (2)\\n3.3.3 CLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiﬁcation datasets with limited categories may lead to false neg atives from items\\nof diﬀerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = − 1\\nn\\n∑\\ni\\nlog esim(ti,t +\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/τ +\\n∑\\nn\\nMASK(ti, t −\\ni,n ) ·esim(ti,t −\\ni,n )/τ +\\n∑\\nj̸=i\\nMASK(ti, t j ) ·esim(ti,t j )/τ +\\n∑\\nj̸=i\\n∑\\nn\\nMASK(ti, t −\\nj,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃcu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must\\naccurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantic ally equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcat ions yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lα\\ni∑ m\\nj=1 lα\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed’s[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusive ly uses retrieval-\\noriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for η,\\nwhile non-retrieval data constitutes 1 − η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scalin g factor α , a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ ηRET ·lα\\ni\\nSret\\nif di ∈ RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface’s sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-\\nproximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations f or all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bﬂoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding\\nsecured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/c lassiﬁcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1\\nSeed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99\\nxiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transform ation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we d eveloped a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eﬀective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’9 4: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine le arning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-s hot learners.”\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”F ine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th Int ernational ACM\\nSIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large languag e models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jaspe r and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv :2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embedd ings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ ıc Magne, and Nils Reimers . ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embed ding: Gen-\\neral text embedding with more and better negative samples.” arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–id f measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGI R’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with tran sfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embeddin g.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer norma lization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ”Glu variants improve transformer.” arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapor e. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345–2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unn atural in-\\nstructions: Tuning language models with (almost) no human labor.” ar Xiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis\\nplatform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation .” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and\\nMichael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558–3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association fo r Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriﬁcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The Faerie Queene’, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork it wasn’t an epic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conﬂicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer’s\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1\\nObject Detection With Deep Learning: A Review\\nZhong-Qiu Zhao , Member, IEEE, Peng Zheng, Shou-Tao Xu, and Xindong Wu , Fellow, IEEE\\nAbstract— Due to object detection’s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by construct-\\ning complex ensembles that combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently in\\nnetwork architecture, training strategy, and optimization func-\\ntion. In this paper, we provide a review of deep learning-based\\nobject detection frameworks. Our review begins with a brief\\nintroduction on the history of deep learning and its representative\\ntool, namely, the convolutional neural network. Then, we focus\\non typical generic object detection architectures along with some\\nmodiﬁcations and useful tricks to improve detection performance\\nfurther. As distinct speciﬁc detection tasks exhibit different\\ncharacteristics, we also brieﬂy survey several speciﬁc tasks,\\nincluding salient object detection, face detection, and pedestrian\\ndetection. Experimental analyses are also provided to compare\\nvarious methods and draw some meaningful conclusions. Finally,\\nseveral promising directions and tasks are provided to serve as\\nguidelines for future work in both object detection and relevant\\nneural network-based learning systems.\\nIndex Terms— Deep learning, neural network, object detection.\\nI. I NTRODUCTION\\nT\\nO GAIN a complete image understanding, we should\\nnot only concentrate on classifying different images but\\nalso try to precisely estimat e the concepts and locations\\nof objects contained in each image. This task is referred\\nas object detection [1], [S1], which usually consists of dif-\\nferent subtasks such as face detection [2], [S2], pedestrian\\ndetection [3], [S2], and skeleton detection [4], [S3]. As one of\\nthe fundamental computer vision problems, object detection\\nis able to provide valuable information for semantic under-\\nstanding of images and videos and is related to many applica-\\ntions, including image classiﬁcation [5], [6], human behavior\\nanalysis [7], [S4], face recognition [8], [S5], and autonomous\\ndriving [9], [10]. Meanwhile, inheriting from neural networks\\nManuscript received September 8, 2017; revised March 3, 2018 and\\nJuly 12, 2018; accepted October 15, 2018. This work was supported in part\\nby the National Natural Scienc e Foundation of China under Grant 61672203,\\nGrant 61375047, and Grant 91746209, in part by the National Key Research\\nand Development Program of China under Grant 2016YFB1000901, and in\\npart by the Anhui Natural Science F unds for Distinguished Young Scholar\\nunder Grant 170808J08. (Corresponding author: Zhong-Qiu Zhao.)\\nZ.-Q. Zhao, P. Zheng, and S.-T. Xu are with the College of Computer\\nScience and Information Engineering, Hefei University of Technology, Hefei\\n230009, China (e-mail: zhongqiuzhao@gmail.com).\\nX. Wu is with the School of Computing and Informatics, University of\\nLouisiana at Lafayette, Lafayette, LA 70504 USA.\\nThis paper has supplementary downloadable material available at\\nhttp://ieeexplore.ieee.org, provided by the authors.\\nColor versions of one or more of the ﬁgures in this paper are available\\nonline at http://ieeexplore.ieee.org.\\nDigital Object Identiﬁer 10.1109/TNNLS.2018.2876865\\nand related learning systems, the progress in these ﬁelds\\nwill develop neural network algorithms and will also have\\ngreat impacts on object detection techniques that can be\\nconsidered as learning systems [11]–[14], [S6]. However, due\\nto large variations in viewpoints, poses, occlusions, and light-\\ning conditions, it is difﬁcult to perfectly accomplish object\\ndetection with an additional object localization task. Therefore,\\nmuch attention has been attr acted to this ﬁeld in recent\\nyears [15]–[18].\\nThe problem deﬁnition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object be longs to (object classiﬁca-\\ntion). Therefore, the pipeline of traditional object detection\\nmodels can be mainly divided into three stages: informative\\nregion selection, feature extraction, and classiﬁcation.\\nA. Informative Region Selection\\nAs different objects may appear in any positions of the\\nimage and have different aspect ratios or sizes, it is a natural\\nchoice to scan the whole image with a multiscale sliding\\nwindow. Although this exhaustive strategy can ﬁnd out all\\npossible positions of the objects, its shortcomings are also\\nobvious. Due to a large number of candidate windows, it is\\ncomputationally expensive and produces too many redundant\\nwindows. However, if only a ﬁxed number of sliding window\\ntemplates is applied, unsatisfactory regions may be produced.\\nB. Feature Extraction\\nTo recognize different objects, we need to extract visual\\nfeatures that can provide a semantic and robust represen-\\ntation. Scale-invariant feature transform [19], histograms of\\noriented gradients (HOG) [20], and Haar-like [21] features are\\nthe representative ones. This is due to the fact that these\\nfeatures can produce representations associated with complex\\ncells in human brain [19]. However, due to the diversity of\\nappearances, illumination conditions, and backgrounds, it is\\ndifﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nC. Classiﬁcation\\nBesides, a classiﬁer is needed to distinguish a target object\\nfrom all the other categories and to make the representations\\nmore hierarchical, semantic, and informative for visual recog-\\nnition. Usually, the supported vector machine (SVM) [22],\\nAdaBoost [23], and deformable part-based model (DPM) [24]\\nare good choices. Among these classiﬁers, the DPM is a\\nﬂexible model by combining object parts with deformation\\ncost to handle severe deformations. In DPM, with the aid\\nof a graphical model, carefully designed low-level features\\nand kinematically inspired part decompositions are combined.\\n2162-237X © 2019 IEEE. Personal u se is perm itted, but republication/redistribution requires IEEE permission.\\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nFig. 1. Application dom ains of object detection.\\nDiscriminative learning of graphical models allows for build-\\ning high-precision part-based models for a variety of object\\nclasses.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state-of-the-art results have\\nbeen obtained on PASCAL visual object classes (VOC) object\\ndetection competition [25] and real-time embedded systems\\nhave been obtained with a low burden on hardware. However,\\nsmall gains are obtained during 2010–2012 by only building\\nensemble systems and employing minor variants of successful\\nmethods [15]. This fact is due to the following reasons: 1) the\\ngeneration of candidate bounding boxes (BBs) with a sliding\\nwindow strategy is redundant, inefﬁcient, and inaccurate and\\n2) the semantic gap cannot be bridged by the combination\\nof manually engineered low-level descriptors and discrimina-\\ntively trained shallow models.\\nThanks to the emergency of deep neural networks\\n(DNNs) [6], [26], [S7], a more signiﬁcant gain is obtained\\nwith the introduction of regions with convolutional neural\\nnetwork (CNN) features (R-CNN) [15]. DNNs, or the most\\nrepresentative CNNs, act in a quite different way from tra-\\nditional approaches. They have d eeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso, the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [27].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including fast R-CNN that\\njointly optimizes classiﬁcation and bounding box regres-\\nsion tasks [16], faster R-CNN that takes an additional sub-\\nnetwork to generate region proposals [17], and you only\\nlook once (YOLO) that accomplishes object detection via a\\nﬁxed-grid regression [18]. All of them bring different degrees\\nof detection performance improvements over the primary\\nR-CNN and make real-time and accurate object detection more\\nachievable.\\nIn this paper, a systematic review is provided to\\nsummarize representative models and their different char-\\nacteristics in several application domains, including generic\\nobject detection [15]–[17], salient object detection [28], [29],\\nface detection [30]–[32], and pedestrian detection [33], [34].\\nTheir relationships are depicted in Fig. 1. Based on basic\\nCNN architectures, the generic object detection is achieved\\nwith bounding box regression, while salient object detec-\\ntion is accomplished with local contrast enhancement and\\npixel-level segmentation. Face detection and pedestrian detec-\\ntion are closely related to ge neric object detection and\\nmainly accomplished with multiscale adaption and multi-\\nfeature fusion/boosting forest, respectively. The dotted lines\\nindicate that the corresponding domains are associated with\\neach other under certain conditions. It should be noticed\\nthat the covered domains are diversiﬁed. Pedestrian and face\\nimages have regular structures, while general objects and scene\\nimages have more complex variations in geometric structures\\nand layouts. Therefore, different deep models are required by\\nvarious images.\\nThere has been a relevant pion eer effort [35] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classiﬁcation and object detection but\\npays little attention on detailing speciﬁc algorithms. Different\\nfrom it, our work not only reviews deep learning-based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section II,\\na brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section III. Then, reviews\\nof CNN applied in several speciﬁc tasks, including salient\\nobject detection, face detection, and pedestrian detection, are\\nexhibited in Section IV–VI, respectively. Several promising\\nfuture directions are proposed in Section VII. At last, some\\nconcluding remarks are presented in Section VIII.\\nII. B\\nRIEF OVERVIEW OF DEEP LEARNING\\nPrior to an overview on deep learning-based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. History: Birth, Decline, and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date\\nback to the 1940s [36], and the original intention was to\\nsimulate the human brain system to solve general learning\\nproblems in a principled way. It was popular in the 1980s and\\n1990s with the proposal of the back-propagation algorithm\\nby Rumelhart et al. [37]. However, due to the overﬁtting of\\ntraining, lack of large-scale training data, limited computation\\npower, and insigniﬁcance in performance compared with other\\nmachine learning tools, neural networks fell out of fashion in\\nthe early 2000s.\\nDeep learning has become popular since 2006 [26], [S7],\\nwith a breakthrough in speech recognition [38]. The recovery\\nof deep learning can be attributed to the following factors.\\n1) The emergence of large-scale annotated training data,\\nsuch as ImageNet [39], to fully exhibit its very large\\nlearning capacity.\\n2) Fast development of high-performance parallel comput-\\ning systems, such as GPU clusters.\\n3) Signiﬁcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npretraining guided by autoencoder [40] or restricted\\nBoltzmann machine [41], a good initialization is pro-\\nvided. With dropout and data augmentation, the over-\\nﬁtting problem in training has been relieved [6], [42].'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 3\\nWith batch normalization (BN), the training of very\\nDNNs becomes quite efﬁcient [43]. Meanwhile, various\\nnetwork structures, such as AlexNet [6], Overfeat [44],\\nGoogLeNet [45], Visual Geometry Group (VGG) [46],\\nand Residual Net (ResNet) [47], have been extensively\\nstudied to improve the performance.\\nWhat prompts deep learning to have a huge impact on\\nthe entire academic commun ity? It may owe to the con-\\ntribution of Hinton’s group, whose continuous efforts have\\ndemonstrated that deep learning would bring a revolutionary\\nbreakthrough on grand challenges rather than just obvious\\nimprovements on small data sets. Their success results from\\ntraining a large CNN on 1.2 million labeled images together\\nwith a few techniques [6] [e.g., rectiﬁed linear unit (ReLU)\\noperation [48] and “dropout” regularization].\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [27].\\nA typical CNN architecture, which is referred to as VGG16,\\ncan be found in Fig. S1 in the supplementary material. Each\\nlayer of CNN is known as a feature map. The feature map\\nof the input layer is a 3-D matrix of pixel intensities for\\ndifferent color channels (e.g., RGB). The feature map of\\nany internal layer is an induced multichannel image, whose\\n“pixel” can be viewed as a speciﬁc feature. Every neu-\\nron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive ﬁeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as ﬁltering and pooling. Filtering (convolution)\\noperation convolutes a ﬁlter matrix (learned weights) with\\nthe values of a receptive ﬁeld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain ﬁnal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling, and local contrast normalization [52],\\nsummarizes the responses of a receptive ﬁeld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an ini-\\ntial feature hierarchy is constructed, which can be ﬁne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the ﬁnal layer with different activation functions [6]\\nis added to get a speciﬁc conditional probability for each\\noutput neuron. The whole network can be optimized on an\\nobjective function (e.g., mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 co nvolutional (conv) layers,\\n3 FC layers, 3 max-pooling layers, and a softmax classiﬁcation\\nlayer. The conv feature maps are produced by convoluting\\n3*3 ﬁlter windows, and feature map resolutions are reduced\\nwith 2 stride max-pooling layers. An arbitrary test image of the\\nsame size as training samples can be processed with the trained\\nnetwork. Rescaling or cropping operations may be needed if\\ndifferent sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarized as follows.\\n1) Hierarchical feature rep resentation, which is the\\nmultilevel representations from pixel to high-level\\nsemantic features learned by a hierarchical multistage\\nFig. 2. Two types of frameworks: region proposal based and\\nregression/classiﬁcation based. SPP: spatial pyramid pooling [64], FRCN:\\nfaster R-CNN [16], RPN: region pr oposal network [17], FCN: fully con-\\nvolutional network [65], BN: batch nor malization [43], and Deconv layers:\\ndeconvolution layers [54] .\\nstructure [15], [53], can be learned from data automati-\\ncally and hidden factors of input data can be disentan-\\ngled through multilevel nonlinear mappings.\\n2) Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expres-\\nsive capability.\\n3) The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g., fast\\nR-CNN combines classiﬁcation and bounding box\\nregression into a multitask learning manner).\\n4) Beneﬁtting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can\\nbe recast as high-dimensiona l data transform problems\\nand solved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research ﬁelds, such as image superresolu-\\ntion reconstruction [54], [55], image classiﬁcation [5], [56],\\nimage retrieval [57], [58], face recognition [8], [S5], pedes-\\ntrian detection [59]–[61], and video analysis [62], [63].\\nIII. G\\nENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image and labeling them with\\nrectangular BBs to show the conﬁdences of existence. The\\nframeworks of generic object detection methods can mainly\\nbe categorized into two types (see Fig. 2). One follows the tra-\\nditional object detection pipeline, generating region proposals\\nat ﬁrst and then classifying each proposal into different object\\ncategories. The other regards object detection as a regression\\nor classiﬁcation problem, adopting a uniﬁed framework to\\nachieve ﬁnal results (categories and locations) directly. The\\nregion proposal-based methods mainly include R-CNN [15],\\nspatial pyramid pooling (SPP)-net [64], Fast R-CNN [16],\\nFaster R-CNN [17], region-based fully convolutional network\\n(R-FCN) [65], feature pyramid networks (FPN) [66], and\\nMask R-CNN [67], some of which are correlated with each\\nother (e.g., SPP-net modiﬁes R-CNN with an SPP layer).\\nThe regression /classiﬁcation-based methods mainly include\\nMultiBox [68], AttentionNet [69], G-CNN [70], YOLO [18],\\nSingle Shot MultiBox Detector (SSD) [71], YOLOv2 [72],\\ndeconvolutional single shot detector (DSSD) [73], and deeply\\nsupervised object detectors (DSOD) [74]. The correlations\\nbetween these two pipelines are bridged by the anchors\\nintroduced in Faster R-CNN. Details of these methods are as\\nfollows.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n4 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nFig. 3. Flowchart of R-CNN [15], which consists of three stages: 1) extracts\\nBU region proposals, 2) computes features for each proposal using a CNN,\\nand then 3) classiﬁes each region with class-speciﬁc linear SVMs.\\nA. Region Proposal-Based Framework\\nThe region proposal-based framework, a two-step process,\\nmatches the attentional mechanism of the human brain to\\nsome extent, which gives a coarse scan of the whole scenario\\nﬁrst and then focuses on regions of interest (RoIs). Among\\nthe prerelated works [44], [75], [76], the most representative\\none is Overfeat [44]. This model inserts CNN into the sliding\\nwindow method, which predicts BBs directly from locations\\nof the topmost feature map after obtaining the conﬁdences of\\nunderlying object categories.\\n1) R-CNN: It is of signiﬁcance to improve the quality\\nof candidate BBs and to take a deep architecture to extract\\nhigh-level features. To solve these problems, R-CNN was\\nproposed by Girshick et al. [15] and obtained a mean average\\nprecision (mAP) of 53 .3% with more than 30% improvement\\nover the previous best result (DPM histograms of sparse\\ncodes [77]) on PASCAL VOC 2012. Fig. 3 shows the ﬂow-\\nchart of R-CNN, which can be divided into three stages as\\nfollows.\\na) Region Proposal Generation: The R-CNN adopts\\nselective search [78] to generate about 2000 region proposals\\nfor each image. The selective search method relies on simple\\nbottom-up (BU) grouping and saliency cues to provide more\\naccurate candidate boxes of arbitrary sizes quickly and to\\nreduce the searching space in object detection [24], [39].\\nb) CNN-Based Deep Feature Extraction: In this stage,\\neach region proposal is warped or cropped into a ﬁxed\\nresolution, and the CNN module in [6] is utilized to extract\\na 4096-dimensional feature as the ﬁnal representation. Due\\nto large learning capacity, dominant expressive power, and\\nhierarchical structure of CNNs, a high-level, semantic, and\\nrobust feature representation for each region proposal can be\\nobtained.\\nc) Classiﬁcation and Localization: With pretrained\\ncategory-speciﬁc linear SVMs for multiple classes, different\\nregion proposals are scored on a set of positive regions and\\nbackground (negative) regions. The scored regions are then\\nadjusted with bounding box regression and ﬁltered with a\\ngreedy nonmaximum suppression (NMS) to produce ﬁnal BBs\\nfor preserved object locations.\\nWhen there are scarce or insufﬁcient labeled data,\\npretraining is usually conducted. Instead of unsupervised\\npretraining [79], R-CNN ﬁrst conducts supervised pretraining\\non ImageNet Large-Scale Visual Recognition Competition,\\na very large auxiliary data set, and then takes a domain-speciﬁc\\nﬁne-tuning. This scheme has been adopted by most of the\\nsubsequent approaches [16], [17].\\nIn spite of its improvements over traditional methods and\\nsigniﬁcance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n1) Due to the existence of FC layers, the CNN requires a\\nﬁxed size (e.g., 227 × 227) input image, which directly\\nleads to the recomputation of the whole CNN for each\\nevaluated region, taking a great deal of time in the testing\\nperiod.\\n2) Training of R-CNN is a multistage pipeline. At ﬁrst,\\na convolutional network (ConvNet) on object proposals\\nis ﬁne-tuned. Then, the softmax classiﬁer learned by\\nﬁne-tuning is replaced by SVMs to ﬁt in with ConvNet\\nfeatures. Finally, bounding-box regressors are trained.\\n3) Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on\\nthe disk. It will take a long time to process a relatively\\nsmall training set with very deep networks, such as\\nVGG16. At the same time, the storage memory required\\nby these features should also be a matter of concern.\\n4) Although selective search can generate region propos-\\nals with relatively high recalls, the obtained region\\nproposals are still redundant and this procedure is\\ntime-consuming (around 2 s to extract 2000 region\\nproposals).\\nTo solve these problems, many methods have been\\nproposed. Geodesic object proposals [80] takes a much faster\\ngeodesic-based segmentation to replace traditional graph\\ncuts. Mutiscale combinatorial grouping [81] searches different\\nscales of the image for multiple hierarchical segmentations and\\ncombinatorially groups different regions to produce proposals.\\nInstead of extracting visually distinct segments, the edge boxes\\nmethod [82] adopts the idea that objects are more likely to\\nexist in BBs with fewer contours straggling their boundaries.\\nAlso, some studies tried to r erank or reﬁne preextracted\\nregion proposals to remove unn ecessary ones and obtained a\\nlimited number of valuable ones, such as DeepBox [83] and\\nSharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na Bayesian optimization-based search algorithm to guide\\nthe regressions of different BBs sequentially and trained\\nclass-speciﬁc CNN classiﬁers with a structured loss to penal-\\nize the localization inaccuracy explicitly. Gupta et al. [86]\\nimproved object detection for RGB-D images with seman-\\ntically rich image and depth features and learned a new\\ngeocentric embedding for dep th images to encode each pixel.\\nThe combination of object detectors and superpixel classi-\\nﬁcation framework gains a promising result on the seman-\\ntic scene segmentation task. Ouyang et al. [87] proposed a\\ndeformable deep CNN (DeepID-Net) that introduces a novel\\ndeformation constrained pooling (def-pooling) layer to impose\\ngeometric penalty on the deformation of various object parts\\nand makes an ensemble of models with different settings.\\nLenc and Vedaldi [88] provided an analysis on the role of\\nproposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth BBs with K -means clustering.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 5\\nFig. 4. Architecture of SPP-net for object detection [64].\\nHowever, more candidate boxes are required to achieve com-\\nparable results to those of R-CNN.\\n2) SPP-Net: FC layers must take a ﬁxed-size input. That\\nis why R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. [64] took the theory of\\nspatial pyramid matching (SPM) [89], [90] into consideration\\nand proposed a novel CNN architecture named SPP-net. SPM\\ntakes several ﬁner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Fig. 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the ﬁfth conv layer (conv5) to project region\\nproposals of arbitrary sizes to ﬁxed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses but also have relationships with their spatial\\npositions [64]. The layer after the ﬁnal conv layer is referred to\\nas the SPP layer. If the number of feature maps in conv5 is 256,\\ntaking a three-level pyramid, the ﬁnal feature vector for each\\nregion proposal obtained after the SPP layer has a dimension\\nof 256 × (1\\n2 + 22 + 42) = 5376.\\nSPP-net not only gains better results with a correct estima-\\ntion of different region proposals in their corresponding scales\\nbut also improves detection efﬁciency in the testing period\\nwith the sharing of computation cost before SPP layer among\\ndifferent proposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and efﬁciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost the\\nsame multistage pipeline as R-CNN, including feature extrac-\\ntion, network ﬁne-tuning, SVM training, and bounding-box\\nregressor ﬁtting. Therefore, an additional expense on storage\\nspace is still required. In addition, the conv layers preceding\\nthe SPP layer cannot be updated with the ﬁne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multitask loss on classiﬁca tion and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Fig. 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a ﬁxed-length feature\\nvector is extracted from each region proposal with an RoI\\nFig. 5. Architecture of Fast R-CNN [16].\\npooling layer. The RoI pooling l ayer is a special case of the\\nSPP layer, which has only one pyramid level. Each feature\\nvector is then fed into a sequence of FC layers before ﬁnally\\nbranching into two sibling output layers. One output layer is\\nresponsible for producing softmax probab ilities for all C + 1\\ncategories ( C object classes plus one “background” class)\\nand the other output layer encodes reﬁned bounding-box\\npositions with four real-valued numbers. All parameters in\\nthese procedures (except the generation of region proposals)\\nare optimized via a multitask loss in an end-to-end way.\\nThe multitasks loss L is deﬁned in the following to jointly\\ntrain classiﬁcation and bounding-box regression:\\nL(p,u,t\\nu ,v) = Lcls(p,u) + λ[u ≥ 1]Lloc(tu ,v) (1)\\nwhere Lcls(p,u) =− log pu calculates the log loss for ground\\ntruth class u,a n d pu is driven from the discrete probability\\ndistribution p = (p0,··· , pC ) over the C +1 outputs from the\\nlast FC layer. Lloc(tu ,v) is deﬁned over the predicted offsets\\ntu = (tu\\nx ,tu\\ny ,tu\\nw,tu\\nh ) and ground-truth bounding-box regression\\ntargets v = (vx ,v y ,v w,v h ),w h e r ex, y,w, and h denote\\nthe two coordinates of the box center, width, and height,\\nrespectively. Each t\\nu adopts the parameter settings in [15] to\\nspecify an object proposal with a log-space height/width shift\\nand scale-invariant translati on. The Iverson bracket indicator\\nfunction [u ≥ 1] is employed to omit all background RoIs.\\nTo provide more robustness against outliers and eliminate the\\nsensitivity in exploding gradients, a smooth L\\n1 loss is adopted\\nto ﬁt bounding-box regressors as follows:\\nLloc(tu,v) =\\n∑\\ni∈x,y,w,h\\nsmoothL1\\n(tu\\ni − vi\\n) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if |x| < 1\\n|x|−0 .5o t h e r w i s e. (3)\\nTo accelerate the pipeline of Fast R-CNN, another two\\ntricks are of necessity. On the one hand, if training sam-\\nples (i.e., RoIs) come from different images, backpropagation\\nthrough the SPP layer becomes highly inefﬁcient. Fast R-CNN\\nsamples minibatches hi erarchically, namely, N images sam-\\npled randomly at ﬁrst and then R/N RoIs sampled in each\\nimage, where R represents the number of RoIs. Critically,\\ncomputation and memory are shared by RoIs from the same\\nimage in the forward and backward pass. On the other hand,\\nmuch time is spent in computing the FC layers during the\\nforward pass [16]. The truncated singular value decomposition\\n(SVD) [91] can be utilized to compress large FC layers and\\nto accelerate the testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single stage with a multitask loss. It saves the additional'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nFig. 6. RPN in Faster R-CNN [17]. K predeﬁned anchor boxes are\\nconvoluted with each sliding window to produce ﬁxed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nexpense on storage space and improves both accuracy and\\nefﬁciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such\\nas selective search and Edgebox, to generate a candidate pool\\nof isolated region proposals. Region proposal computation\\nis also a bottleneck in improving efﬁciency. To solve this\\nproblem, Ren et al. [17], [92] introduced an additional region\\nproposal network (RPN), which acts in a nearly cost-free way\\nby sharing full-image conv features with detection network.\\nRPN is achieved with an FCN, which has the ability to\\npredict object bounds and scores at each position simultane-\\nously. Similar to [78], RPN takes an image of arbitrary size to\\ngenerate a set of rectangular object proposals. RPN operates\\non a speciﬁc conv layer with th e preceding layers shared with\\nthe object detection network.\\nThe architecture of RPN is shown in Fig. 6. The network\\nslides over the conv feature map and fully connects to an n×n\\nspatial window. A low-dimensional vector (512-dimensional\\nfor VGG16) is obtained in each s liding window and fed into\\ntwo sibling FC layers, namely, box-classiﬁcation layer (cls)\\nand box-regression layer (reg). This architecture is imple-\\nmented with an n × n conv layer followed by two sibling\\n1 × 1 conv layers. To increase nonlinearity, ReLU is applied\\nto the output of the n × n conv layer.\\nThe regressions toward true BBs are achieved by comparing\\nproposals relative to reference boxes (anchors). In the Faster\\nR-CNN, anchors of three scales and three aspect ratios are\\nadopted. The loss function is similar to (1)\\nL(p\\ni ,ti ) = 1\\nNcls\\n∑\\ni\\nLcls\\n(\\npi , p∗\\ni\\n)\\n+ λ 1\\nNreg\\n∑\\ni\\np∗\\ni Lreg\\n(\\nti ,t∗\\ni\\n)\\n(4)\\nwhere pi is the predicted probability of the ith anchor being an\\nobject. The ground truth label p∗\\ni is 1 if the anchor is positive,\\notherwise 0. ti stores four parameterized coordinates of the\\npredicted bounding box while t∗\\ni is related to the ground-truth\\nbox overlapping with a positive anchor.Lcls is a binary log loss\\nand Lreg is a smoothed L1 loss similar to (2). These two terms\\nare normalized with the minibatch size ( Ncls) and the number\\nof anchor locations ( Nreg), respectively. In the form of FCNs,\\nFaster R-CNN can be trained end-to-end by backpropagation\\nand SGD in an alternate training manner.\\nWith the proposal of Faster R-CNN, region proposal-based\\nCNN architectures for object detection can really be trained in\\nan end-to-end way. Also, a frame rate of 5 frames per second\\n(fps) on a GPU is achieved with the state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces objectlike regi ons (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a preva-\\nlent family [16], [17] of deep networks for object detection\\nis composed of two subnetworks: a shared fully convolu-\\ntional subnetwork (independe nt of RoIs) and an unshared\\nRoI-wise subnetwork. This decomposition originates from\\npioneering classiﬁcation archit ectures (e.g., AlexNet [6] and\\nVGG16 [46]) which consist of a convolutional subnetwork and\\nseveral FC layers separated by a s peciﬁc spatial pooling layer.\\nRecent state-of-the-art image c lassiﬁcation networks, such\\nas ResNets [47] and GoogLeNets [45], [93], are fully convo-\\nlutional. To adapt to these architectures, it is natural to con-\\nstruct a fully convolutional object detection network without\\nRoI-wise subnetwork. However, it turns out to be inferior\\nwith such a naive solution [47]. This inconsistency is due\\nto the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classiﬁcation. In other words, shifting an object inside\\nan image should be indiscriminative in image classiﬁcation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregionwise layers. Therefore, Dai et al. [65] proposed an\\nR-FCNs (see Fig. S2 in the supplementary material).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k\\n2 position-sensitive\\ns c o r em a p sw i t haﬁ x e dg r i do fk × k ﬁrst and a position-\\nsensitive RoI pooling layer is then appended to aggregate\\nthe responses from these score maps. Finally, in each\\nRoI, k\\n2 position-sensitive scores are averaged to produce a\\nC + 1-d vector and softmax responses across categories are\\ncomputed. Another 4 k2-d conv layer is appended to obtain\\nclass-agnostic BBs.\\nWith R-FCN, more powerful classiﬁcation networks\\ncan be adopted to accomplish object detection in a fully\\nconvolutional architecture by sharing nearly all the layers,\\nand the state-of-the-art results are obtained on both PASCAL\\nVOC and Microsoft COCO [94] data sets at a test speed\\nof 170 ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in many\\nobject detection systems to improve scale invariance [24], [64]\\n[Fig. 7(a)]. However, training time and memory consumption\\nincrease rapidly. To this end, some techniques take only\\na single input scale to represent high-level semantics and\\nincrease the robustness to scale changes [Fig. 7(b)], and image\\npyramids are built at test time which results in an inconsistency\\nbetween train/test-time infer ences [16], [17]. The in-network\\nfeature hierarchy in a deep ConvNet produces feature maps of\\ndifferent spatial resolutions while introduces large semantic\\ngaps caused by different depths [Fig. 7(c)]. To avoid using\\nlow-level features, pioneer works [71], [95] usually build the\\npyramid starting from middle layers or just sum transformed'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 7\\nFig. 7. Main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single-scale features are adopted for\\nfaster detection. (c) Alternative to the featurized image pyramid is to reuse\\nthe pyramidal feature hierarchy com puted by a ConvNet. (d) FPN integrates\\nboth (b) and (c). Blue outlines indicate feature maps and thicker outlines\\ndenote semantically stronger features.\\nfeature responses, missing the higher resolution maps of the\\nfeature hierarchy.\\nDifferent from these approaches, FPN [66] holds an archi-\\ntecture with a BU pathway, a top-down (TD) pathway and\\nseveral lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and semanti-\\ncally weak features [Fig. 7(d)]. The BU pathway, which is the\\nbasic forward backbone ConvNet, produces a feature hierarchy\\nby downsampling the corresponding feature maps with a stride\\nof 2. The layers owning the same size of output maps are\\ngrouped into the same network stage and the output of the last\\nlayer of each stage is chosen as the reference set of feature\\nmaps to build the following TD pathway.\\nTo build the TD pathway, feature maps from higher network\\nstages are upsampled at ﬁrst and then enhanced with those of\\nthe same spatial size from the BU pathway via lateral connec-\\ntions. A 1 × 1 conv layer is appended to the upsampled map\\nto reduce channel dimensions and the mergence is achieved\\nby elementwise addition. Finally, a 3 × 3 convolution is also\\nappended to each merged map to reduce the aliasing effect\\nof upsampling and the ﬁnal feature map is generated. This\\nprocess is iterated until the ﬁnest resolution map is generated.\\nAs feature pyramid can extract rich semantics from all levels\\nand be trained end to end with all scales, the state-of-the-\\nart representation can be obtained without sacriﬁcing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g., region proposal generation) and to many\\nother computer vision tasks (e.g., instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nThe multitask scheme will create spurious edge and exhibit\\nsystematic errors on overlapping instances [98]. To solve this\\nproblem, parallel to the existing branches in Faster R-CNN\\nfor classiﬁcation and bounding box regression, the Mask R-\\nCNN [67] adds a branch to predict segmentation masks in a\\npixel-to-pixel manner (Fig. 8).\\nDifferent from the other two branches that are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m × m mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nFig. 8. Mask R-CNN framework fo r instance segmentation [67].\\nrepresentation requires fewer parameters but is more accurate\\nthan that in [97]. Formally, besides the two losses in (1) for\\nclassiﬁcation and bounding box regression, an additional loss\\nfor segmentation mask branch is deﬁned to reach a multitask\\nloss. This loss is only associated with ground-truth class and\\nrelies on the classiﬁcation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classiﬁcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely, RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch\\nonly adds a small computationa l burden and its cooperation\\nwith other tasks provides complementary information for\\nobject detection. As a result, Mask R-CNN is simple to\\nimplement with promising instance segmentation and object\\ndetection results. In a word, Mask R-CNN is a ﬂexible\\nand efﬁcient framework for instance-level recognition, which\\ncan be easily generalized to other tasks (e.g., human pose\\nestimation [7], [S4]) with minimal modiﬁcation.\\n8) Multitask Learning, Multiscale Representation, and Con-\\ntextual Modeling: Although the Faster R-CNN gets promising\\nresults with several hundred proposals, it still struggles in\\nsmall-size object detection and localization, mainly due to\\nthe coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO data set which consists\\nof objects at a broad range of scal es, less prototypical images,\\nand requires more precise loca lization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmultitask learning [100], multiscale representation [95], and\\ncontext modeling [101] to combine complementary informa-\\ntion from multiple sources.\\nMultitask learning learns a useful representation for mul-\\ntiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. [100] introduced conv features trained for\\nobject segmentation and “stuff” (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet). Dai et al. [97] presented multitask network\\ncascades of three networks, namely, class-agnostic region\\nproposal generation, pixel-level instance segmentation, and\\nregional instance classiﬁcation. Li et al. [104] incorporated the'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n8 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nweakly supervised object segmentation cues and region-based\\nobject detection into a multistage architecture to fully exploit\\nthe learned segmentation features.\\nMultiscale representation combines activations from\\nmultiple layers with skip-lay er connections to provide\\nsemantic information of different spatial resolutions [66].\\nCai et al. [105] proposed the multiscale CNN (MS-CNN)\\nto ease the inconsistency between the sizes of objects\\nand receptive ﬁelds with multiple scale-independent\\noutput layers. Yang et al. [34] investigated two strategies,\\nnamely, scale-dependent pooling (SDP) and layerwise\\ncascaded rejection classiﬁers (CRCs), to exploit appropriate\\nscale-dependent conv features. Kong et al. [101] proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps fro m different resolutions into a\\nuniform space.\\nContextual modeling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. [106] proposed the SegDeepM to\\nexploit object segmentation which reduces the dependency\\non initial candidate boxes with the Markov random ﬁeld.\\nMoysset et al. [108] took advantage of four directional 2-D\\nlong short-term memories (LSTMs) [107] to convey global\\ncontext between different local regions and reduced trainable\\nparameters with local parameter sharing. Zeng et al. [109] pro-\\nposed a novel gated bidirectional-net (GBD-Net) by introduc-\\ning gated functions to control message transmission between\\ndifferent support regions.\\nThe combination incorporates different components above\\ninto the same model to improve detection performance fur-\\nther. Gidaris and Komodakis [110] proposed the multire-\\ngion CNN (MR-CNN) model to capture different aspects of\\nan object, the distinct appear ances of various object parts,\\nand semantic segmentation-aware features. To obtain con-\\ntextual and multiscale representations, Bell et al. [95] pro-\\nposed the inside–outside net (ION) by exploiting informa-\\ntion both inside and outside the RoI with spatial recurrent\\nneural networks [111] and skip pooling [101]. Zagoruyko\\net al. [112] proposed the MultiPath architecture by introducing\\nthree modiﬁcations to the Fast R-CNN, including multiscale\\nskip connections [95], a modiﬁed foveal structure [110], and\\na novel loss function summing different intersection over\\nunion (IoU) losses.\\n9) Thinking in Deep Learning-Based Object Detection:\\nApart from the above-mentioned approaches, there are still\\nmany important factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. [113] proposed an effective online mining\\nalgorithm (OHEM) for automatic selection of the hard exam-\\nples, which leads to a more eff ective and efﬁcient training.\\nInstead of concentrating on feature extraction, Ren et al.\\n[114] made a detailed analysis on object classiﬁers and found\\nthat it is of particular importance for object detection to con-\\nstruct a deep and convolutional per-region classiﬁer carefully,\\nespecially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signiﬁcant scale variation, occlusion,\\nor truncation, especially when only 2-D object detection is\\ninvolved. To address this problem, Xiang et al. [60] proposed\\na novel subcategory-aware RPN, which guides the generation\\nof region proposals with subcategory information related to\\nobject poses and jointly optimize object detection and subcat-\\negory classiﬁcation.\\nOuyang et al. [115] found that the samples from differ-\\nent classes follow a long-tailed distribution, which indicates\\nthat different classes with distinct numbers of samples have\\ndifferent degrees of impacts on feature learning. To this end,\\nobjects are ﬁrst clustered into visually similar class groups,\\nand then, a hierarchical feature learning scheme is adopted to\\nlearn deep representations for each group separately.\\nIn order to minimize the computational cost and achieve\\nthe state-of-the-art performance, with the “deep and thin”\\ndesign principle and following the pipeline of Fast R-CNN,\\nHong et al. [116] proposed the architecture of PV ANET,\\nwhich adopts some building blocks including concatenated\\nReLU [117], Inception [45], and HyperNet [101] to reduce\\nthe expense on multiscale feature extraction and trains the\\nnetwork with BN [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 fps).\\nB. Regression/Classiﬁcation-Based Framework\\nRegion proposal-based frameworks are composed of several\\ncorrelated stages, including region proposal generation, feature\\nextraction with CNN, classiﬁcation, and bounding box regres-\\nsion, which are usually trained separately. Even in the recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different component s becomes the bottleneck in the\\nreal-time application.\\nOne-step frameworks based on global regression/\\nclassiﬁcation, mapping straightly from image pixels to\\nbounding\\nbox coordinates and cl ass probabilities, can reduce\\ntime expense. We ﬁrst review some pioneer CNN models\\nand then focus on two signiﬁcant frameworks, namely,\\nYOLO [18] and SSD [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiﬁcation task.\\nSzegedy et al. [118] formulated the object detection task\\nas a DNN-based regression, generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has difﬁculty in handling\\noverlapping objects, and BBs generated by direct upsampling\\nis far from perfect.\\nPinheiro et al. [119] proposed a CNN model with two\\nbranches: one generates class agnostic segmentation masks\\nand the other predicts the likelihood of a given patch centered\\non an object. Inference is efﬁcient since class scores and\\nsegmentation can be obtained in a single model with most\\nof the CNN operations shared.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 9\\nFig. 9. Main idea of YOLO [18].\\nErhan et al. [68] and Szegedy et al. [120] proposed the\\nregression-based MultiBox to produce scored class-agnostic\\nregion proposals. A uniﬁed loss was introduced to bias both\\nlocalization and conﬁdences of multiple components to predict\\nthe coordinates of class-agnostic BBs. However, a large num-\\nber of additional parameters are introduced to the ﬁnal layer.\\nYoo et al. [69] adopted an iterative classiﬁcation approach\\nto handle object detection and proposed an impressive end-\\nto-end CNN architecture named AttentionNet. Starting from\\nthe top-left and bottom-right corners of an image, Attention-\\nNet points to a target object by generating quantized weak\\ndirections and converges to an accurate object boundary box\\nwith an ensemble of iterative predictions. However, the model\\nbecomes quite inefﬁcient when handling multiple categories\\nwith a progressive two-step procedure.\\nNajibi et al. [70] proposed a proposal-free iterative\\ngrid-based object detector (G-CNN), which models object\\ndetection as ﬁnding a path from a ﬁxed grid to boxes tightly\\nsurrounding the objects [70]. Starting with a ﬁxed multiscale\\nbounding box grid, G-CNN trains a regressor to move and\\nscale elements of the grid toward objects iteratively. However,\\nG-CNN has a difﬁculty in dealing with small or highly\\noverlapping objects.\\n2) YOLO: Redmon et al. [18] proposed a novel framework\\ncalled YOLO, which makes the use of the whole topmost\\nfeature map to predict both conﬁdences for multiple categories\\nand BBs. The basic idea of YOLO is exhibited in Fig. 9.\\nYOLO divides the input image into an S × S grid and each\\ngrid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B BBs and their\\ncorresponding conﬁdence scores. Formally, conﬁdence scores\\nare deﬁned as Pr(Object) ∗IOU\\ntruth\\npred , which indicates how likely\\nthere exist objects (Pr (Object) ≥ 0) and shows conﬁdences\\nof its prediction (IOU truth\\npred\\n). At the same time, regardless\\nof the number of boxes, C conditional class probabilities\\n(Pr(Classi |Object)) should also be predicted in each grid cell.\\nIt should be noticed that only the contribution from the grid\\ncell containing an obj ect is calculated.\\nAt test time, class-speciﬁc conﬁdence scores for each box\\nare achieved by multiplying the individual box conﬁdence\\npredictions with the conditional class probabilities as follows :\\nPr(Object) ∗ IOU\\ntruth\\npred ∗ Pr(Classi|Object)\\n= Pr(Classi) ∗ IOUtruth\\npred (5)\\nwhere the existing probability of class-speciﬁc objects in the\\nbox and the ﬁtness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized :\\nλ\\ncoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nobj\\nij [(xi −ˆxi )2 + (yi −ˆyi )2]\\n+ λcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nobj\\nij\\n[ ( √\\nwi −\\n√\\nˆwi )2 + (\\n√\\nhi −\\n√\\nˆhi\\n) 2]\\n+\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nobj\\nij\\n(Ci − ˆCi )2\\n+ λnoobj\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nnoobj\\nij\\n(Ci − ˆCi )2\\n+\\nS2\\n∑\\ni=0\\n/BD\\nobj\\ni\\n∑\\nc∈classes\\n(pi (c) −ˆpi (c))2. (6)\\nIn a certain cell i, (xi , yi ) denote the center of the box relative\\nto the bounds of the grid cell,(wi ,hi ) are the normalized width\\nand height relative to the image size, Ci represents the conﬁ-\\ndence scores, /BD\\nobj\\ni indicates the exist ence of objects, and /BD\\nobj\\nij\\ndenotes that the prediction is conducted by the jth bounding\\nbox predictor. Note that only when an object is present in\\nthat grid cell, the loss function penalizes classiﬁcation errors.\\nSimilarly, when the predictor is “responsible” for the ground\\ntruth box (i.e., the highest IoU of any predictor in that grid cell\\nis achieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 × 1 reduction layers followed by 3 × 3c o n v\\nlayers. The network can process images in real time at 45 fps\\nand a simpliﬁed version Fast YOLO can reach 155 fps\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on the background,\\nwhich makes the cooperation with Fast R-CNN become pos-\\nsible. An improved version, YOLOv2, was later proposed\\nin [72], which adopts several impressive strategies, such as\\nBN, anchor boxes, dimension cluster, and multiscale training.\\n3) SSD: YOLO has a difﬁculty in dealing with small\\nobjects in groups, which is caused by strong spatial con-\\nstraints imposed on bounding box predictions [18]. Mean-\\nwhile, YOLO struggles to generalize to objects in new/unusual\\naspect ratios/conﬁgurations an d produces relatively coarse\\nfeatures due to multiple downsampling operations.\\nAiming at these problems, Liu et al. [71] proposed an SSD,\\nwhich was inspired by the anchors adopted in MultiBox [68],\\nRPN [17], and multiscale representation [95]. Given a speciﬁc\\nfeature map, instead of ﬁxed grids adopted in YOLO, the SSD\\ntakes the advantage of a set of default anchor boxes with\\ndifferent aspect ratios and scales to discretize the output space\\nof BBs. To handle objects with various sizes, the network\\nfuses predictions from multiple feature maps with different\\nresolutions.\\nThe architecture of SSD is demonstrated in Fig. 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales\\nand aspect ratios and their associated conﬁdences. The net-\\nwork is trained with a weighted sum of localization loss'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nTABLE I\\nOVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES\\nFig. 10. Architecture of SSD 300 [71]. SSD adds se veral feature layers to the end of VGG16 backbone ne twork to predict the offsets to default anchor\\nboxes and their associated conﬁdences. Final detection r esults are obtained by conducting NMS on multiscale reﬁned BBs.\\n(e.g., Smooth L1) and conﬁdence loss (e.g., Softmax), which\\nis similar to (1). Final detection results are obtained by\\nconducting NMS on multiscale reﬁned BBs.\\nIntegrating with hard negative mining, data augmentation,\\nand a larger number of carefully chosen default anchors,\\nSSD signiﬁcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO while being three\\ntimes faster. The SSD300 (input image size is 300 × 300)\\nruns at 59 fps, which is more accurate and efﬁcient than\\nYOLO. However, SSD is not skilled at dealing with small\\nobjects, which can be relieved by adopting better feature\\nextractor backbone (e.g., ResNet101), adding deconvolution\\nlayers with skip connections to introduce additional large-scale\\ncontext [73], and designing better network structure (e.g., stem\\nblock and dense block) [74].\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark data sets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121], and Microsoft COCO [94].\\nThe evaluated approaches include R-CNN [15], SPP-\\nnet [64], Fast R-CNN [16], networks on convolutional\\nfeature maps (NOC) [114], Bayes [85], MR-CNN&\\nS-CNN [105], Faster R-CNN [17], HyperNet [101], ION [95],\\nMS-GR [104], StuffNet [100], SSD300 [71], SSD512 [71],\\nOHEM [113], SDP +CRC [34], G-CNN [70], SubCNN [60],\\nGBD-Net [109], PV ANET [116], YOLO [18], YOLOv2 [72],\\nR-FCN [65], FPN [66], Mask R-CNN [67], DSSD [73],\\nand DSOD [74]. If no speciﬁc instructions for the adopted\\nframework are provided, the utilized model is a VGG16 [46]\\npretrained on 1000-way ImageNet classiﬁcation task [39].\\nDue to the limitation of the paper length, we only provide an\\noverview, including proposal, learning method, loss function,\\nprograming language, and platform, of the prominent\\narchitectures in Table I. Deta iled experimental settings, which\\ncan be found in the original papers, are missed. In addition\\nto the comparisons of detectio n accuracy, another comparison\\nis provided to evaluate their test consumption on PASCAL\\nVOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 data sets consist of 20 categories. The evaluation terms\\nare AP in each single category and mAP across all the 20 cat-\\negories. Comparative results are exhibited in Tables II and III,\\nfrom which the following remarks can be obtained.\\n1) If incorporated with a proper way, more powerful back-\\nbone CNN models can deﬁnitely improve the object\\ndetection performance (the comparison among R-CNN\\nwith AlexNet, R-CNN with VGG16 and SPP-net with\\nZF-Net [122]).\\n2) With the introduction of the SPP layer (SPP-net), end-\\nto-end multitask architecture (FRCN), and RPN (Faster\\nR-CNN), object detection performance is improved\\ngradually and apparently.\\n3) Due to a large number of trainable parameters, in order\\nto obtain multilevel robust features, data augmentation\\nis very important for deep learning-based models (Faster\\nR-CNN with “07,” “07 + 12,” and “07 + 12 + coco”).\\n4) Apart from basic models, there are still many\\nother factors affecting object detection performance,\\nsuch as multiscale and multiregion feature extrac-\\ntion (e.g., MR-CNN), modiﬁed classiﬁcation networks\\n(e.g., NOC), additional information from other corre-\\nlated tasks (e.g., StuffNet, HyperNet), multiscale rep-\\nresentation (e.g., ION), and mining of hard negative\\nsamples (e.g., OHEM).\\n5) As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN, and ﬁne-grained features,\\nthe localization errors ar e corrected (YOLOv2).\\n6) By combining many recent tricks and modeling the\\nwhole network as a fully convolutional one, R-FCN'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 11\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 T EST SET (%)\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 T EST SET (%)\\nachieves a more obvious improvement of detection per-\\nformance over other approaches.\\n2) Microsoft COCO: Microsoft COCO is composed\\nof 300 000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requi rement on object localization,\\nthis data set is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are given in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n1) Multiscale training and test are beneﬁcial in improv-\\ning object detection perform ance, which provide addi-\\ntional information in different resolutions (R-FCN).\\nFPN and DSSD provide some better ways to build\\nfeature pyramids to achieve multiscale representation.\\nThe complementary information from other related tasks\\nis also helpful for accurate object localization (Mask\\nR-CNN with instance segmentation task).\\n2) Overall, region proposal-based methods, such as Faster\\nR-CNN and R-FCN, perform better than regression/\\nclassiﬁcation-based approaches, namely, YOLO and\\nSSD, due to the fact that quite a lot of localization\\nerrors are produced by regression/classiﬁcation-based\\napproaches.\\n3) Context modeling is helpful to locate small objects,\\nwhich provides additional information by consult-\\ning nearby objects and surroundings (GBD-Net and\\nmultipath).\\n4) Due to the existence of a large number of nonstandard\\nsmall objects, the results on this data set are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g., ResNeXt [123]) and\\nuseful strategies (e.g., mu ltitask learning [67], [124]),\\nthe performance can be improved.\\n5) The success of DSOD in training from scratch stresses\\nthe importance of the network design to release the\\nrequirements for perfect pretrained classiﬁers on relevant\\ntasks and a large number of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA\\nTitan X GPU. Except for “SS” which is processed with CPU,\\nthe other procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n1) By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test\\ntime is further reduced with the uniﬁed multitask learn-\\ning (FRCN) and removal of additional region proposal\\ngeneration stage (Faster R-CNN). It is also helpful to\\ncompress the parameters of FC layers with SVD [91]\\n(PA VNET and FRCN).\\n2) It takes additional test time to extract multiscale fea-\\ntures and contextual information (ION and MR-RCNN&\\nS-RCNN).\\n3) It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time\\nconsumption can be reduced by adding as many lay-\\ners into shared fully convolutional layers as possible\\n(FRCN).'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO T EST DEV SET (%)\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 T EST SET\\n4) Regression-based models can usually be processed\\nin real time at the cost of a drop in accuracy\\ncompared with region proposal-based models. Also,\\nregion proposal-based models can be modiﬁed into\\nreal-time systems with the introduction of other\\ntricks [116] (PV ANET), such as BN [43] and residual\\nconnections [123].\\nIV . S\\nALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous appli-\\ncations incorporate the visual saliency to improve their perfor-\\nmance, such as image cropping [125] and segmentation [126],\\nimage retrieval [57], and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely, BU [127] and TD [128]. Local\\nfeature contrast plays the central role in BU salient object\\ndetection, regardless of the semantic contents of the scene.\\nTo learn local feature contrast , various local and global fea-\\ntures are extracted from pixels, e.g., edges [129] and spa-\\ntial information [130]. However, high-level and multiscale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low-contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is\\ntask-oriented and takes prior knowledge about object cate-\\ngories to guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep Learning in Salient Object Detection\\nDue to the signiﬁcance for providing high-level and mul-\\ntiscale feature representation a nd the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133], and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Vig et al. [29] follows a completely\\nautomatic data-driven approach to perform a large-scale search\\nfor optimal features, namely, an ensemble of deep networks\\nwith different layers and parameters. To address the problem\\nof limited training data, Kummerer et al. [134] proposed the\\nDeep Gaze by transferring from the AlexNet to generate a\\nhigh-dimensional feature space and create a saliency map.\\nA similar architecture was proposed by Huang et al. [135] to\\nintegrate saliency prediction into pretrained object recognition\\nDNNs. The transfer is accomp lished by ﬁne-tuning DNNs’\\nweights with an objective function based on the saliency\\nevaluation metrics, such as similarity, KL-divergence, and\\nnormalized scanpath saliency.\\nSome works combined local and global visual\\nclues to improve salient object detection performance.\\nWang et al. [136] trained two independent deep CNNs\\n(DNN-L and DNN-G) to capture local information and global\\ncontrast and predicted saliency maps by integrating both\\nlocal estimation and global search. Cholakkal et al. [137]\\nproposed a weakly supervised saliency detection framework\\nto combine visual saliency from BU and TD saliency'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 13\\nTABLE VI\\nCOMPARISON BETWEEN STATE-OF -THE -ART METHODS\\nmaps and reﬁned the results with a multiscale superpixel-\\naveraging. Zhao et al. [138] proposed a multicontext deep\\nlearning framework, which utilizes a uniﬁed learning\\nframework to model global and local context jointly with\\nthe aid of superpixel segmentation. To predict saliency in\\nvideos, Bak et al. [139] fused two static saliency models,\\nnamely, spatial stream net and temporal stream net, into a\\ntwo-stream framework with a novel empirically grounded\\ndata augmentation technique.\\nComplementary information from semantic segmentation\\nand context modeling is beneﬁcial . To learn internal represen-\\ntations of saliency efﬁciently, He et al. [140] proposed a novel\\nsuperpixelwise CNN approach called SuperCNN, in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully CNN, Li et al. [141] proposed a\\nmultitask deep saliency model, in which intrinsic correlations\\nbetween saliency detection and semantic segmentation are set\\nup. However, due to the conv layers with large receptive\\nﬁelds and pooling layers, blurry object boundaries and coarse\\nsaliency maps are produced. Tang and Wu [142] proposed\\na novel saliency detection framework (CRPSD) [142], which\\ncombines the region-level saliency estimation and pixel-level\\nsaliency prediction together with three closely related CNNs.\\nLi and Yu [143]proposed a deep contrast network to combine\\nsegmentwise spatial pooling and pixel-level fully convolutional\\nstreams [143].\\nThe proper integration of multiscale feature maps is also\\nof signiﬁcance for improving de tection performance. Based\\non Fast R-CNN, Wang et al. [144] proposed the RegionNet\\nby performing salient object detection with end-to-end edge\\npreserving and multiscale contextual modeling. Liu et al. [28]\\nproposed a multiresolution CNN (Mr-CNN) to predict eye ﬁx-\\nations, which is achieved by learning both BU visual saliency\\nand TD visual factors from raw image data simultaneously.\\nCornia et al. [145] proposed an architecture that combines fea-\\ntures extracted at different levels of the CNN. Li and Yu [146]\\nproposed a multiscale deep CNN framework to extract three\\nscales of deep contrast featur es, namely, the mean-subtracted\\nregion, the bounding box of its immediate neighboring regions,\\nand the masked entire image, from each candidate region.\\nIt is efﬁcient and accurate to train a direct pixelwise\\nCNN architecture to predict salient objects with the aids\\nof recurrent neural networks and deconvolution networks.\\nPan et al. [147] formulated saliency prediction as a mini-\\nmization optimization on the Euclidean distance between the\\npredicted saliency map and the ground truth and proposed\\ntwo kinds of architectures: a shallow one trained from scratch\\nand a deeper one adapted fro m a deconvoluted VGG net-\\nwork. Asconvolutional–deconvolution networks are not expert\\nin recognizing objects of multiple scales, Kuen et al. [148]\\nproposed a recurrent attentional convolutional–deconvolution\\nnetwork with several spatial transformer and recurrent network\\nunits to conquer this problem. To fuse local, global, and\\ncontextual information of salient objects, Tang et al. [149]\\ndeveloped a deeply supervised recurrent CNN to perform a\\nfull image-to-image saliency detection.\\nB. Experimental Evaluation\\nFour representative data sets, including Evaluation on Com-\\nplex Scene Saliency Dataset (ECSSD) [156], HKU-IS [146],\\nPASCALS [157], and SOD [158], are used to evaluate several\\nstate-of-the-art methods. ECSSD consists of 1000 structurally\\ncomplex but semantically meaningful natural images. HKU-IS\\nis a large-scale data set containing over 4000 challenging\\nimages. Most of these images have more than one salient\\nobject and own low contrast. PASCALS is a subset chosen\\nfrom the validation set of PASCAL VOC 2010 segmentation\\ndata set and is composed of 850 natural images. The SOD data\\nset possesses 300 images containing multiple salient objects.\\nThe training and validation sets f or different data sets are kept\\nthe same as those in [152].\\nTwo standard metrics, namely, F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values precomputed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is deﬁned as follows:\\nF\\nβ = (1 + β2)Presion × Recall\\nβ2Presion + Recall (7)\\nwhere β2 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation :\\nMAE = 1\\nH × W\\nH∑\\ni=1\\nW∑\\nj=1\\n| ˆS(i, j) = ˆZ(i, j)| (8)\\nwhere ˆZ and ˆS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected nonsalient pixels [159].\\nThe following approaches are evaluated: contextual hyper-\\ngraph modeling (CHM) [150], RC [151], discriminative'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n14 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nregional feature integration (DRFI) [152], MC [138], mul-\\ntiscale deep CNN features (MDF) [146], local estimation\\nand global search (LEGS) [136], DSR [149], multi-task deep\\nneural network [141], CRPSD [142], deep contrast learn-\\ning (DCL) [143], encoded low level distance (ELD) [153],\\nnonlocal deep features (NLDF) [154], and deep supervision\\nwith short connections (DSSC) [155]. Among these meth-\\nods, CHM, RC, and DRFI are classical ones with the best\\nperformance [159], while the other methods are all associated\\nwith CNN. F-measure and MAE scores are given in Table VI.\\nFrom Table VI, we can ﬁnd that CNN-based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a more\\naccurate saliency. ELD refers to low-level handcrafted features\\nfor complementary information. LEGS adopts generic region\\nproposals to provide initial salient regions, which may be\\ninsufﬁcient for salient detection. DSR and MT act in different\\nways by introducing a recurrent network and semantic seg-\\nmentation, which provide insights for future improvements.\\nCRPSD, DCL, NLDF, and DSSC are all based on multiscale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF,\\nand DSSC perform the best on these four data sets. DSSC\\nearns the best performance by modeling scale-to-scale short\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of the CNN-based methods need to model\\nvisual saliency along region boundaries with the aid of super-\\npixel segmentation. Meanwhile, the extraction of multiscale\\ndeep CNN features is of signiﬁcance for measuring local\\nconspicuity. Finally, it is necessary to strengthen local con-\\nnections between different CNN layers as well as to utilize\\ncomplementary information from local and global context.\\nV. F\\nACE DETECTION\\nFace detection is essential to many face applications\\nand acts as an important preprocessing procedure to\\nface recognition [160]–[162], f ace synthesis [163], [164], and\\nfacial expression analysis [165]. Different from generic object\\ndetection, this task is to recognize and locate face regions\\ncovering a very large range of scales (30–300 pts versus\\n10–1000 pts). At the same time, faces have their unique object\\nstructural conﬁgurations (e.g., the distribution of different\\nface parts) and characteristics (e.g., skin color). All these\\ndifferences lead to special attention to this task. However, large\\nvisual variations of faces, such a s occlusions, pose variations,\\nand illumination changes, impose great challenges for this task\\nin real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiﬁers with Haar-like features\\nand AdaBoost, achieving good performance with real-time\\nefﬁciency. However, this detector may degrade signiﬁcantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. [24] proposed a deformable part model (DPM)\\nfor face detection. However, for these traditional face detection\\nmethods, high computational e xpenses and lar ge quantities\\nof annotations are required to achieve a reasonable result.\\nFig. 11. ROC curves of state-of-the-a rt methods on FDDB. (a) Discrete ROC\\ncurves. (b) Continuous ROC curves.\\nIn addition, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep Learning in Face Detection\\nRecently, some CNN-based face detection approaches have\\nbeen proposed [167]–[169]. As less accurate localization\\nresults from independent regressions of object coordinates,\\nYu et al. [167] proposed a novel IoU loss function for pre-\\ndicting the four bounds of box jointly. Farfade et al. [168]\\nproposed a deep dense face detector (DDFD) to conduct\\nmultiview face detection, which is able to detect faces in\\na wide range of orientations without the requirement of\\npose/landmark annotations. Yang et al. [169] proposed a novel\\ndeep learning-based face detec tion framework, which collects\\nthe responses from local facial parts (e.g., eyes, nose, and\\nmouths) to address face detection under severe occlusions\\nand unconstrained pose variations. Yang et al. [170] proposed\\na scale-friendly detection network named ScaleFace, which\\nsplits a large range of target scales into smaller subranges.\\nDifferent specialized subnetworks are constructed on these\\nsubscales and combined into a single one to conduct end-to-\\nend optimization. Hao et al. [171] designed an efﬁcient CNN\\nto predict the scale distribution histogram of the faces and took\\nthis histogram to guide the zoomed-in view and zoomed-out\\nview of the image. Since the faces are approximately in\\nuniform scale after zoom, compared with other state-of-the-art\\nbaselines, better performance is achieved with a less computa-\\ntion cost. In addition, some generic detection frameworks are\\nextended to face detection with different modiﬁcations, e.g.,\\nFaster R-CNN [30], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3-D modeling and face landmarks, in a multitask\\nlearning manner. Huang et al. [174] proposed a uniﬁed end-to-\\nend FCN framework called DenseBox to jointly conduct face\\ndetection and landmark localization. Li et al. [175] proposed\\na multitask discriminative learning framework that integrates\\na ConvNet with a ﬁxed 3-D mean face model in an end-to-end'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 15\\nmanner. In the framework, two issues are addressed to trans-\\nfer from generic object detection to face detection, namely,\\neliminating predeﬁned anchor boxes by a 3-D mean face\\nmodel and replacing RoI pooling layer with a conﬁguration\\npooling layer. Zhang et al. [176] proposed a deep cascaded\\nmultitask framework named multitask cascaded convolutional\\nnetworks (MTCNN) which exploits the inherent correlations\\nbetween face detection and alignment in the unconstrained\\nenvironment to boost up detection performance in a coarse-\\nto-ﬁne manner.\\nReducing computational expenses is of necessity in real\\napplications. To achieve real-time detection on the mobile plat-\\nform, Kalinovskii and Spitsyn [177] proposed a new solution\\nof frontal face detection based on compact CNN cascades. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify, and reﬁne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al. [32]\\nproposed a cascaded CNN denoted by supervised transformer\\nnetwork. This network takes a multitask RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang and Nevatia [8] proposed a\\nthree-stage cascade structu re based on FCNs, while in each\\nstage, a multiscale FCN is utilized to reﬁne the positions of\\npossible faces. Qin et al. [178] proposed a uniﬁed framework\\nthat achieves better results with the complementary informa-\\ntion from different jointly trained CNNs.\\nB. Experimental Evaluation\\nThe FDDB [179] data set has a total of 2845 pictures in\\nwhich 5171 faces are annotated with an elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the\\nreceiver operating characteristic (ROC) curve for the discrete\\nscores can reﬂect the dependence of the detected face fractions\\non the number of false alarms. Compared with annotations,\\nany detection with an IoU ratio exceeding 0.5 is treated as\\npositive. Each annotation is only associated with one detection.\\nThe ROC curve for the continuous scores is the reﬂection of\\nface localization quality.\\nThe evaluated models cover DDFD [168], Cascade-CNN\\n[180], aggregate channel features (ACF)-multiscale [181],\\nPico [182], Head-Hunter [183], Joint Cascade [31], SURF-\\nmultiview [184], Viola–Jones [166], NPDFace [185],\\nFaceness [169], convolutional channel features (CCF) [186],\\nMTCNN [176], Conv3-D [175], Hyperface [187],\\nUnitBox [167], locally decorrelated channel\\nfeatures (LDCF +) [S2], DeepIR [173], hybrid-resolution\\nmodel with elliptical regressor (HR-ER) [188], Face-R-\\nCNN [172], and ScaleFace [170]. ACF-multiscale, Pico,\\nHeadHunter, Joint Cascade, SURF-multiview, Viola-Jones,\\nNPDFace, and LDCF + are built on classic hand-crafted\\nfeatures while the rest methods are based on deep CNN\\nfeatures. The ROC curves are shown in Fig. 11.\\nIn Fig. 11(a), in spite of relatively competitive results pro-\\nduced by LDCF +, it can be observed that most of the classic\\nmethods perform with similar results and are outperformed by\\nCNN-based methods by a signiﬁcant margin. In Fig. 11(b),\\nit can be observed that most of the CNN-based methods earn\\nsimilar true positive rates between 60% and 70% while DeepIR\\nand HR-ER perform much better than them. Among classic\\nmethods, Joint Cascade is still competitive. As earlier works,\\nDDFD and CCF directly make use of generated feature maps\\nand obtain relatively poor results. CascadeCNN builds cas-\\ncaded CNNs to locate face regions, which is efﬁcient but inac-\\ncurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3-D, and Hyperface proves the effectiveness of multitask\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales and make a balance between accuracy and\\nefﬁciency. DeepIR and Face-R-CNN are two extensions of the\\nFaster R-CNN architecture to face detection, which validate\\nthe signiﬁcance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN-based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cas cades, adapting scale-aware\\ndetection, and learning multitask shared CNN features.\\nVI. P\\nEDESTRIAN DETECTION\\nRecently, pedestrian detec tion has been intensively\\nstudied, which has a close relationship to pedestrian\\ntracking [189], [190], person reidentiﬁcation [191], [192],\\nand robot navigation [193], [194]. Prior to the recent\\nprogress in deep CNN (DCNN)-based methods [195], [196],\\nsome researchers combined boosted decision forests\\nwith hand-crafted features to obtain pedestrian\\ndetectors [197]–[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes in typ-\\nical scenarios of pedestrian det ection (e.g., automatic driving\\nand intelligent surveillance), t he application of RoI pooling\\nlayer in generic object detection pipeline may result in “plain”\\nfeatures due to collapsing bins. In the meantime, the main\\nsource of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different conﬁgurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep Learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature-\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some studies have been conducted to analyze\\nthe reasons. Zhang et al. [203] attempted to adapt generic\\nFaster R-CNN [17] to pedestrian detection. They modiﬁed the\\ndownstream classiﬁer by adding boosted forests to shared,\\nhigh-resolution conv feature maps and taking an RPN to han-\\ndle small instances and hard neg ative examples. To deal with'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n16 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\ncomplex occlusions existing in pedestrian images, inspired\\nby DPM [24], Tian et al. [204] proposed a deep learning\\nframework called DeepParts, which makes decisions based\\non an ensemble of extensive part detectors. DeepParts has\\nadvantages in dealing with weakly labeled data, low IoU\\npositive proposals, and partial occlusion.\\nOther researchers also tried to combine complementary\\ninformation from multiple data sources. CompACT-Deep\\nadopts a complexity-aware cascade to combine hand-crafted\\nfeatures and ﬁne-tuned DCNNs [195]. Based on Faster\\nR-CNN, Liu et al. [205] proposed multispectral DNNs for\\npedestrian detection to combine complementary information\\nfrom color and thermal images. Tian et al. [206] proposed\\na task-assistant CNN to jointly learn multiple tasks with\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. [207] pro-\\nposed a DNN fusion architecture for fast and robust pedes-\\ntrian detection. Based on the candidate BBs generated with\\nSSD detectors [71], multiple binary classiﬁers are processed\\nparallelly to conduct soft-rejection-based network fusion by\\nconsulting their aggregated degree of conﬁdences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel, and a large VGG16 model [195]. DeepParts contains\\n45 ﬁne-tuned DCNN models, and a set of strategies, includ-\\ning bounding box shifting handling and part selection, are\\nrequired to arrive at the reported results [204]. Therefore,\\nthe modiﬁcation and simpliﬁcation are of signiﬁcance to\\nreduce the burden on both software and hardware to satisfy\\nreal-time detection demand. Tome et al. [59] proposed a novel\\nsolution to adapt generic object detection pipeline to pedestrian\\ndetection by optimizing most of its stages. Hu et al. [208]\\ntrained an ensemble of boosted decision models by reusing\\nthe conv feature maps, and a further improvement was gained\\nwith simple pixel labeling and additional complementary\\nhand-crafted features. Tome et al. [209] proposed a reduced\\nmemory region-based deep CNN architecture, which fuses\\nregional responses from both ACF detectors and SVM classi-\\nﬁers into R-CNN. Ribeiro et al. [33] addressed the problem of\\nhuman-aware navigation and proposed a vision-based person\\ntracking system guided by multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian data set [3]. The data set was collected from the\\nvideos of a vehicle driving through an urban environment and\\nconsists of 250 000 frames with about 2300 unique pedestrians\\nand 350 000 annotated BBs. Three kinds of labels, namely,\\n“Person (clear identiﬁcations), ” “Person? (unclear identiﬁca-\\ntions),” and “People (large group of individuals),” are assigned\\nto different BBs. The perform ance is measured with the\\nlog-average miss rate (L-AMR) which is computed evenly\\nspaced in log-space in the range 10\\n−2 to 1 by averaging miss\\nrate at the rate of nine false positives per image [3]. According\\nto the differences in the height and visible part of the BBs,\\na total of nine popular settings are adopted to evaluate different\\nproperties of these models. Details of these settings are as\\nin [3].\\nEvaluated methods include Checkerboards + [198],\\nLDCF++ [S2], SCF +AlexNet [210], SA-FastRCNN [211],\\nMS-CNN [105], DeepParts [204], CompACT-Deep [195],\\nRPN+BF [203], and F-DNN +SS [207]. The ﬁrst two\\nmethods are based on hand-crafted features while the rest\\nones rely on deep CNN features. All results are exhibited\\nin Table VII. From this table, we observe that different\\nfrom other tasks, classic handcrafted features can still earn\\ncompetitive results with boosted decision forests [203],\\nACF [197], and HOG +LUV channels [S2]. As an early\\nattempt to adapt CNN to pedestrian detection, the features\\ngenerated by SCF +AlexNet are not so discriminant and\\nproduce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely, local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due\\nto complexity, it is too time-consuming to achieve real-time\\ndetection. The multiscale representation of MS-CNN improves\\nthe accuracy of pedestrian locations. SA-FastRCNN extends\\nFast R-CNN to automatically detect pedestrians according\\nto their different scales, which has trouble when there are\\npartial occlusions. RPN+BF combines the detectors produced\\nby Faster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN +SS, which is composed\\nof multiple parallel classiﬁers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN, and MS-CNN.\\nIn short, CNN-based methods can provide more accu-\\nrate candidate boxes and multilevel semantic information for\\nidentifying and loca ting pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classiﬁers, extracting multiscale and part-based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. P\\nROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor the future work.\\nThe ﬁrst one is small object detection such as occurring\\nin COCO data set and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n1) Multitask Joint Optim ization and Multimodal\\nInformation Fusion: Due to the correlations between\\ndifferent tasks within and outside object detection,\\nmultitask joint optimization has already been studied\\nby many researchers [16], [17]. However, apart from\\nthe tasks mentioned in Section III-A8, it is desirable to\\nthink over the characteristics of different subtasks of\\nobject detection (e.g., superpixel semantic segmentation\\nin salient object detection) and extend multitask\\noptimization to other applications such as instance\\nsegmentation [66], multiobject tracking [202], and\\nmultiperson pose estimation [S4]. In addition, given'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 17\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF STATE-OF -THE -ART MODELS ON CALTECH PEDESTRIAN DATA SET.\\nALL NUMBERS ARE REPORTED IN L-AMR\\na speciﬁc application, the information from different\\nmodalities, such as text [212] , thermal data [205], and\\nimages [65], can be fused together to achieve a more\\ndiscriminant network.\\n2) Scale Adaption: Objects usually exist in different scales,\\nwhich are more apparent in face detection and pedes-\\ntrian detection. To increase the robustness to scale\\nchanges, it is demanded to train scale-invariant, mul-\\ntiscale or scale-adaptive det ectors. For scale-invariant\\ndetectors, more powerful backbone architectures (e.g.,\\nResNext [123]), negative sample mining [113], reverse\\nconnection [213], and subcategory modeling [60] are all\\nbeneﬁcial. For multiscale detectors, both the FPN [66]\\nthat produces multiscale feature maps and the generative\\nadversarial network [214] that narrows representation\\ndifferences between small objects and the large ones\\nwith a low-cost architecture provide insights into gen-\\nerating meaningful feature p yramid. For scale-adaptive\\ndetectors, it is useful to combine knowledge graph [215],\\nattentional mechanism [216], cascade network [180],\\nand scale distribution estimation [171] to detect objects\\nadaptively.\\n3) Spatial Correlations and Contextual Modeling: Spatial\\ndistribution plays an important role in object detec-\\ntion. Therefore, region proposal generation and grid\\nregression are taken to obtain probable object loca-\\ntions. However, the correlations between multiple pro-\\nposals and object categorie s are ignored. In addition,\\nthe global structure information is abandoned by the\\nposition-sensitive score maps in R-FCN. To solve these\\nproblems, we can refer to diverse subset selection [217]\\nand sequential reasoning tasks [218] for possible solu-\\ntions. It is also meaningful to mask salient parts and\\ncouple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor\\nand accomplish real-time object detection, with the emergence\\nof the large-scale image and video data. The following three\\naspects can be taken into account.\\n1) Cascade Network: In a cascade network, a cas-\\ncade of detectors is built in different stages or\\nlayers [180], [220]. Easily distinguishable examples are\\nrejected at shallow layers so that features and classiﬁers\\nat later stages can handle more difﬁcult samples with\\nthe aid of the decisions from previous stages. However,\\ncurrent cascades are built in a greedy manner, where\\nprevious stages in cascade are ﬁxed when training a new\\nstage. Therefore, the optim izations of different CNNs\\nare isolated, which stresses the necessity of end-to-end\\noptimization for CNN cascade. At the same time, it is\\nalso a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n2) Unsupervised and Weakly Supervised Learning: It is\\nvery time-consuming to manually draw large quantities\\nof BBs. To release this burden, semantic prior [55],\\nunsupervised object discovery [221], multiple instance\\nlearning [222], and DNN prediction [47] can be inte-\\ngrated to make the best use of image-level supervision\\nto assign object category tags to corresponding object\\nregions and reﬁne object boundaries. Furthermore,\\nweakly annotations (e.g., center-click annotations [223])\\nare also helpful for achieving high-quality detectors with\\nmodest annotation efforts, especially aided by the mobile\\nplatform.\\n3) Network Optimization: Given speciﬁc applications and\\nplatforms, it is signiﬁcant to make a balance among\\nspeed, memory, and accuracy by selecting an optimal\\ndetection architecture [116], [224]. However, despite\\nthat detection accuracy is reduced, it is more mean-\\ningful to learn compact models with a fewer number\\nof parameters [209]. This situation can be relieved by\\nintroducing better pretraining schemes [225], knowledge\\ndistillation [226], and hint learning [227]. DSOD also\\nprovides a promising guideline to train from scratch\\nto bridge the gap between different image sources and\\ntasks [74].\\nThe third one is to extend typical methods for\\n2-D object detection to adapt 3-D object detection\\nand video object detection, with the requirements from\\nautonomous driving, intelligent t ransportation, and intelligent\\nsurveillance.\\n1) 3-D Object Detection: With the applications of 3-D\\nsensors (e.g., Light Detection and Ranging and cam-\\nera), additional depth information can be utilized to\\nbetter understand the images in 2-D and extend the\\nimage-level knowledge to the real world. However, sel-\\ndom of these 3-D-aware techniques aim to place correct\\n3-D BBs around detected objects. To achieve better\\nbounding results, multiview representation [181] and\\n3-D proposal network [228] may provide some guide-\\nlines to encode depth information with the aid of inertial\\nsensors (accelerometer and gyrometer) [229].'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n18 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\n2) Video Object Detection: Temporal information across\\ndifferent frames plays an important role in under-\\nstanding the behaviors of different objects. However,\\nthe accuracy suffers from degenerated object appear-\\nances (e.g., motion blur and video defocus) in videos\\nand the network is usually not trained end to end. To this\\nend, spatiotemporal tubelets [230], optical ﬂow [199],\\nand LSTM [107] should be considered to fundamentally\\nmodel object associations between consecutive frames.\\nVIII. C\\nONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation, and background\\nswitches, deep learning-based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning-based object detection frameworks\\nthat handle different subproblems, such as occlusion, clutter,\\nand low resolution, with different degrees of modiﬁcations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely, salient object\\ndetection, face detection, and pedestrian detection, are also\\nbrieﬂy reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object detec-\\ntion landscape. This review is also meaningful for the develop-\\nments in neural networks and related learning systems, which\\nprovides valuable insights and guidelines for future progress.\\nR\\nEFERENCES\\n[1] P. F. Felzenszwalb et al. , “Object detection with discriminatively\\ntrained part-based models,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 32, no. 9, pp. 1627–1645, Sep. 2010.\\n[2] K.-K. Sung and T. Poggio, “Example-based learning for view-based\\nhuman face detection,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39–51, Jan. 1998.\\n[3] C. Wojek et al. , “Pedestrian detection: An evaluation of the state\\nof the art,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 34, no. 4,\\npp. 743–761, Apr. 2012.\\n[4] H. Kobatake and Y . Yoshinaga, “Detection of spicules on mammogram\\nbased on skeleton analysis,” I E E ET r a n s .M e d .I m a g ., vol. 15, no. 3,\\npp. 235–245, Jun. 1996.\\n[5] Y . Jia et al., “Caffe: Convolutional architecture for fast feature embed-\\nding,” in Proc. ACM MM, 2014, pp. 675–678.\\n[6] A. Krizhevsky et al., “ImageNet classiﬁcation with deep convolutional\\nneural networks,” in Proc. NIPS, 2012, pp. 1097–1105.\\n[7] Z. Cao et al. , “Realtime multi-person 2D pose estimation using part\\nafﬁnity ﬁelds,” in Proc. CVPR, 2017, pp. 1302–1310.\\n[8] Z. Yang and R. Nevatia, “A multi-scale cascade fully convolutional\\nnetwork face detector,” in Proc. ICPR, 2016, pp. 633–638.\\n[9] C. Chen et al., “DeepDriving: Learning affordance for direct perception\\nin autonomous driving,” in Proc. ICCV, 2015, pp. 2722–2730.\\n[10] X. Chen et al. , “Multi-view 3D object detection network for\\nautonomous driving,” in Proc. CVPR, 2017, pp. 6526–6534.\\n[11] A. Dundar et al., “Embedded streaming deep neural networks acceler-\\nator with applications,” IEEE Trans. Neural Netw. Learn. Syst., vol. 28,\\nno. 7, pp. 1572–1583, Jul. 2017.\\n[12] R. J. Cintra et al., “Low-complexity approximate convolutional neural\\nnetworks,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 29, no. 12,\\npp. 5981–5992, 2018.\\n[13] S. H. Khan et al., “Cost-sensitive learning of deep feature representa-\\ntions from imbalanced data,” IEEE Trans. Neural Netw. Learn. Syst. ,\\nvol. 29, no. 8, pp. 3573–3587, Aug. 2018.\\n[14] A. Stuhlsatz et al. , “Feature extraction with deep neural networks by\\na generalized discriminant analysis,” IEEE Trans. Neural Netw. Learn.\\nSyst., vol. 23, no. 4, pp. 596–608, Apr. 2012.\\n[15] R. Girshick et al. , “Rich feature hierarchies for accurate object\\ndetection and semantic segmentation,” in Proc. CVPR, 2014,\\npp. 580–587.\\n[16] R. Girshick, “Fast R-CNN,” in Proc. ICCV, 2015, pp. 1440–1448.\\n[17] S. Ren et al., “Faster R-CNN: Towards real-time object detection with\\nregion proposal networks,” in Proc. NIPS, 2015, pp. 91–99.\\n[18] J. Redmon et al. , “You only look once: Uniﬁed, real-time object\\ndetection,” in Proc. CVPR, 2016, pp. 779–788.\\n[19] D. G. Lowe, “Distinctive image features from scale-invariant key-\\npoints,” Int. J. Comput. Vis. , vol. 60, no. 2, pp. 91–110, 2004.\\n[20] N. Dalal and B. Triggs, “Histogr ams of oriented gradients for human\\ndetection,” in Proc. CVPR, 2005, pp. 886–893.\\n[21] R. Lienhart and J. Maydt, “An extended set of Haar-like features for\\nrapid object detection,” in Proc. ICIP, 2002, p. 1.\\n[22] C. Cortes and V . Vapnik, “Support vector machine,” Mach. Learn. ,\\nvol. 20, no. 3, pp. 273–297, 1995.\\n[23] Y . Freund and R. E. Schapire, “A decision-theoretic generalization of\\non-line learning and an application to boosting,” J. Comput. Syst. Sci. ,\\nvol. 55, no. 1, pp. 119–139, 1997.\\n[24] P. F. Felzenszwalb et al. , “Object detection with discriminatively\\ntrained part-based models,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 32, no. 9, pp. 1627–1645, Sep. 2010.\\n[25] M. Everingham et al., “The pascal visual object classes (VOC) chal-\\nlenge,” Int. J. Comput. Vis. , vol. 88, no. 2, pp. 303–338, 2008.\\n[26] G. E. Hinton and R. R. Salakhutdi nov, “Reducing the dimensionality of\\ndata with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,\\n2006.\\n[27] Y . LeCun et al. , “Deep learning,” Nature, vol. 521, pp. 436–444,\\nMay 2015.\\n[28] N. Liu et al. , “Predicting eye ﬁxations using convolutional neural\\nnetworks,” in Proc. CVPR, 2015, pp. 362–370.\\n[29] E. Vig et al. , “Large-scale optimization of hierarchical features\\nfor saliency prediction in natural images,” in Proc. CVPR, 2014,\\npp. 2798–2805.\\n[30] H. Jiang and E. Learned-Miller, “Face detection with the faster\\nR-CNN,” in Proc. FG, 2017, pp. 650–657.\\n[31] D. Chen et al., “Joint cascade face detection and alignment,” in Proc.\\nECCV, 2014, pp. 109–122.\\n[32] D. Chen et al. , “Supervised transformer network for efﬁcient face\\ndetection,” in Proc. ECCV, 2016, pp. 122–138.\\n[33] A. Mateus et al. . (2016). “Efﬁcient and robust pedestrian detection\\nusing deep learning for human-aware navigation.” [Online]. Available:\\nhttps://arxiv.org/abs/1607.04441\\n[34] F. Yang et al. , “Exploit all the layers: Fast and accurate CNN object\\ndetector with scale dependent pooling and cascaded rejection classi-\\nﬁers,” in Proc. CVPR, 2016, pp. 2129–2137.\\n[35] P. N. Druzhkov and V . D. Kustikova, “A survey of deep learning meth-\\nods and software tools for image classiﬁcation and object detection,”\\nPattern Recognit. Image Anal. ,\\n vol. 26, no. 1, pp. 9–15, 2016.\\n[36] W. Pitts and W. S. McCulloch, “How we know universals the perception\\nof auditory and visual forms,” Bull. Math. Biophys. , vol. 9, no. 3,\\npp. 127–147, 1947.\\n[37] D. E. Rumelhart et al., “Learning representations by back-propagating\\nerrors,” Nature, vol. 323, pp. 533–536, Oct. 1986.\\n[38] G. Hinton et al., “Deep neural networks for acoustic modeling in speech\\nrecognition: The shared views of four research groups,” IEEE Signal\\nProcess. Mag., vol. 29, no. 6, pp. 82–97, Nov. 2012.\\n[39] J. Deng et al., “ImageNet: A large-scale hierarchical image database,”\\nin Proc. CVPR, 2009, pp. 248–255.\\n[40] L. Deng et al. , “Binary coding of speech spectrograms using a deep\\nauto-encoder,” in Proc. INTERSPEECH, 2010, pp. 1692–1695.\\n[41] G. Dahl et al., “Phone recognition with the mean -covariance restricted\\nBoltzmann machine,” in Proc. NIPS, 2010, pp. 469–477.\\n[42] G. E. Hinton et al. . (2012). “Improving neural networks by pre-\\nventing co-adaptation of feature detectors.” [Online]. Available:\\nhttps://arxiv.org/abs/1207.0580\\n[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,” in Proc. ICML,\\n2015, pp. 448–456.\\n[44] P. Sermanet et al.. (2013). “OverFeat: Integrated recognition, localiza-\\ntion and detection using convolutional networks.” [Online]. Available:\\nhttps://arxiv.org/abs/1312.6229\\n[45] C. Szegedy et al., “Going deeper with convolutions,” in Proc. CVPR,\\n2015, pp. 1–9.\\n[46] K. Simonyan and A. Zisserman. (2014).“Very deep convolutional\\nnetworks for large-scale image recognition.” [Online]. Available:\\nhttps://arxiv.org/abs/1409.1556\\n[47] K. He et al., “Deep residual learning for image recognition,” in Proc.\\nCVPR, 2016, pp. 770–778.\\n[48] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\\nBoltzmann machines,” in Proc. ICML, 2010, pp. 807–814.\\n[49] M. Oquab et al. , “Weakly supervised object recognition with\\nconvolutional neural networks,” in Proc. NIPS, 2014, pp. 1–10.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 19\\n[50] M. Oquab et al. , “Learning and transferring mid-level image\\nrepresentations using convolutional neural networks,” in Proc. CVPR,\\n2014, pp. 1717–1724.\\n[51] F. M. Wadley, “Probit analysis: A statistical treatment of the sigmoid\\nresponse curve,” Ann. Entomol. Soc. Amer., vol. 67, no. 4, pp. 549–553,\\n1947.\\n[52] K. Kavukcuoglu et al. , “Learning invariant features through\\ntopographic ﬁlter maps,” in Proc. CVPR, 2009, pp. 1605–1612.\\n[53] K. Kavukcuoglu et al., “Learning convolutional feature hierarchies for\\nvisual recognition,” in Proc. NIPS, 2010, pp. 1090–1098.\\n[54] M. D. Zeiler et al., “Deconvolutional networks,” in Proc. CVPR, 2010,\\npp. 2528–2535.\\n[55] H. Noh et al. , “Learning deconvolution network for semantic\\nsegmentation,” in Proc. ICCV, 2015, pp. 1520–1528.\\n[56] Z.-Q. Zhao et al., “Plant leaf identiﬁcation via a growing convolution\\nneural network with progressive sample learning,” in Proc. ACCV ,\\n2014, pp. 348–361.\\n[57] A. Babenko et al., “Neural codes for image retrieval,” in Proc. ECCV,\\n2014, pp. 584–599.\\n[58] J. Wan et al. , “Deep learning for content-based image retrieval:\\nA comprehensive study,” in ACM MM, 2014, pp. 157–166.\\n[59] D. Tomè et al. , “Deep convolutional neural networks for pedestrian\\ndetection,” Signal Process., Image Commun. , vol. 47, pp. 482–489,\\nSep. 2016.\\n[60] Y . Xiang et al., “Subcategory-aware convolutional neural networks for\\nobject proposals and detection,” in Proc. WACV, 2017, pp. 924–933.\\n[61] Z.-Q. Zhao et al. , “Pedestrian detection based on fast R-CNN and\\nbatch normalization,” in Proc. ICIC, 2017, pp. 735–746.\\n[62] J. Ngiam et al. , “Multimodal deep learning,” in Proc. ICML , 2011,\\npp. 689–696.\\n[63] Z. Wu et al. , “Modeling spatial-temporal clues in a hybrid deep\\nlearning framework for video classiﬁcation,” in Proc. ACM MM, 2015,\\npp. 461–470.\\n[64] K. He et al., “Spatial pyramid pooling in deep convolutional networks\\nfor visual recognition,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 37, no. 9, pp. 1904–1916, Sep. 2015.\\n[65] J. Dai et al. , “R-FCN: Object detection via region-based fully\\nconvolutional networks,” in Proc. NIPS, 2016, pp. 379–387.\\n[66] T.-Y . Lin et al. , “Feature pyramid networks for object detection,” in\\nProc. CVPR, 2017, pp. 936–944.\\n[67] K. He et al., “Mask R-CNN,” in Proc. ICCV, 2017, pp. 2980–2988.\\n[68] D. Erhan et al., “Scalable object detection using deep neural networks,”\\nin Proc. CVPR, 2014, pp. 2155–2162.\\n[69] D. Yoo et al., “AttentionNet: Aggregating weak directions for accurate\\nobject detection,” in Proc. CVPR, 2015, pp. 2659–2667.\\n[70] M. Najibi et al., “G-CNN: An iterative grid based object detector,” in\\nProc. CVPR, 2016, pp. 2369–2377.\\n[71] W. Liu et al., “SSD: Single shot multibox detector,” in Proc. ECCV,\\n2016, pp. 21–37.\\n[72] J. Redmon and A. Farhadi. (2016). “YOLO9000: Better, faster,\\nstronger.” [Online]. Available: https://arxiv.org/abs/1612.08242\\n[73] C.-Y . Fu et al.. (2017). “DSSD : Deconvolutional single shot detector.”\\n[Online]. Available: https://arxiv.org/abs/1701.06659\\n[74] Z. Shen et al. , “DSOD: Learning deeply supervised object detectors\\nfrom scratch,” in Proc. ICCV, 2017, p. 7.\\n[75] G. E. Hinton et al. , “Transforming auto-encoders,” in Proc. ICANN,\\n2011, pp. 44–51.\\n[76] G. W. Taylor et al., “Learning invariance through imitation,” in Proc.\\nCVPR, 2011, pp. 2729–2736.\\n[77] X. Ren and D. Ramanan, “Histograms of sparse codes for object\\ndetection,” in Proc. CVPR, 2013, pp. 3246–3253.\\n[78] J. R. R. Uijlings et al. , “Selective search for object recognition,” Int.\\nJ. Comput. Vis., vol. 104, no. 2, pp. 154–171, Apr. 2013.\\n[79] P. Sermanet et al., “Pedestrian detection with unsupervised multi-stage\\nfeature learning,” in Proc. CVPR, 2013, pp. 3626–3633.\\n[80] P. Krähenbühl and V . Koltun, “Geodesic object proposals,” in Proc.\\nECCV, 2014, pp. 725–739.\\n[81] P. Arbeláez et al. , “Multiscale combinatorial grouping,” in Proc.\\nCVPR, 2014, pp. 328–335.\\n[82] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals\\nfrom edges,” in Proc. ECCV, 2014, pp. 391–405.\\n[83] W. Kuo et al. , “Deepbox: Learning objectness with convolutional\\nnetworks,” in Proc. ICCV, 2015, pp. 2479–2487.\\n[84] P. O. Pinheiro et al. , “Learning to reﬁne object segments,” in Proc.\\nECCV, 2016, pp. 75–91.\\n[85] Y . Zhang et al., “Improving object detection with deep convolutional\\nnetworks via Bayesian optimizatio n and structured prediction,” in\\nProc. CVPR, 2015, pp. 249–258.\\n[86] S. Gupta et al., “Learning rich features from RGB-D images for object\\ndetection and segmentation,” in Proc. ECCV, 2014, pp. 345–360.\\n[87] W. Ouyang et al., “DeepID-Net: Deformable deep convolutional neural\\nnetworks for object detection,” in Proc. CVPR, 2015, pp. 2403–2412.\\n[88] K. Lenc and A. Vedaldi. (2015). “R-CNN minus R.” [Online].\\nAvailable: https://arxiv.org/abs/1506.06981\\n[89] S. Lazebnik et al. , “Beyond bags of features: Spatial pyramid\\nmatching for recognizing natural scene categories,” in Proc. CVPR,\\n2006, pp. 2169–2178.\\n[90] F. Perronnin et al., “Improving the Fisher kernel for large-scale image\\nclassiﬁcation,” in Proc. ECCV, 2010, pp. 143–156.\\n[91] J. Xue et al., “Restructuring of deep neural network acoustic models\\nwith singular value decomposition,” in Proc. INTERSPEECH , 2013,\\npp. 2365–2369.\\n[92] S. Ren et al., “Faster R-CNN: Towards real-time object detection with\\nregion proposal networks,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 39, no. 6, pp. 1137–1149, Jun. 2017.\\n[93] C. Szegedy et al., “Rethinking the inception architecture for computer\\nvision,” in Proc. CVPR, 2016, pp. 2818–2826.\\n[94] T.-Y . Lin et al. , “Microsoft COCO: Common objects in context,” in\\nProc. ECCV, 2014, pp. 740–755.\\n[95] S. Bell et al. , “Inside-outside net: Detecting objects in context with\\nskip pooling and recurrent neural networks,” in Proc. CVPR, 2016,\\npp. 2874–2883.\\n[96] A. Arnab and P. H. S. Torr, “Pixe lwise instance segmentation with a\\ndynamically instantiated network,” in Proc. CVPR, 2017, pp. 879–888.\\n[97] J. Dai et al. , “Instance-aware semantic segmentation via multi-task\\nnetwork cascades,” in Proc. CVPR, 2016, pp. 3150–3158.\\n[98] Y . Li et al. , “Fully convolutional instance-aware semantic\\nsegmentation,” in Proc. CVPR, 2017, pp. 4438–4446.\\n[99] M. Jaderberg et al. , “Spatial transformer networks,” in Proc. Adv.\\nNeural Inf. Process. Syst. , 2015, pp. 2017–2025.\\n[100] S. Brahmbhatt et al. , “StuffNet: Using ‘Stuff’ to improve object\\ndetection,” in Proc. WACV, 2017, pp. 934–943.\\n[101] T. Kong et al., “HyperNet: Towards accurate region proposal generation\\nand joint object detection,” in Proc. CVPR, 2016, pp. 845–853.\\n[102] A. Pentina et al., “Curriculum learning of multiple tasks,” in Proc.\\nCVPR, 2015, pp. 5492–5500.\\n[103] J. Yim et al. , “Rotating your face using multi-task deep neural\\nnetwork,” in Proc. CVPR, 2015, pp. 676–684.\\n[104] J. Li et al.. (2016). “Multi-stage object detection with group recursive\\nlearning.” [Online]. Available: https://arxiv.org/abs/1608.05159\\n[105] Z. Cai et al., “A uniﬁed multi-scale deep convolutional neural network\\nfor fast object detection,” in Proc. ECCV, 2016, pp. 354–370.\\n[106] Y . Zhu et al. , “segDeepM: Exploiting segmentation and context in\\ndeep neural networks for object detection,” in Proc. CVPR, 2015,\\npp. 4703–4711.\\n[107] W. Byeon et al. , “Scene labeling with LSTM recurrent neural\\nnetworks,” in Proc. CVPR, 2015, pp. 3547–3555.\\n[108] B. Moysset et al. . (2016). “Learning to detect and localize\\nmany objects from few examples.” [Online]. Available:\\nhttps://arxiv.org/abs/1611.05664\\n[109] X. Zeng et al. , “Gated bi-directional CNN for object detection,” in\\nProc. ECCV, 2016, pp. 354–369.\\n[110] S. Gidaris and N. Komodakis, “O bject detection via a multi-region\\nand semantic segmentation-aware CNN model,” in Proc. CVPR, 2015,\\npp. 1134–1142.\\n[111] M. Schuster and K. K. Paliwal, “ Bidirectional recurrent neural\\nnetworks,” IEEE Trans. Signal Process. , vol. 45, no. 11,\\npp. 2673–2681, Nov. 1997.\\n[112] S. Zagoruyko et al. (2016). “A multiPath network for object detection.”\\n[Online]. Available: https://arxiv.org/abs/1604.02135\\n[113] A. Shrivastava et al. , “Training region-based object detectors\\nwith online hard example mining,” in Proc. CVPR, 2016,\\npp. 761–769.\\n[114] S. Ren et al. , “Object detection networks on convolutional feature\\nmaps,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39, no. 7,\\npp. 1476–1481, Jul. 2017.\\n[115] W. Ouyang et al., “Factors in ﬁnetuning deep model for object detection\\nwith long-tail distribution,” in Proc. CVPR, 2016, pp. 864–873.\\n[116] S. Hong et al. . (2016). “PV ANet: Lightweight deep neural\\nnetworks for real-time object detection.” [Online]. Available:\\nhttps://arxiv.org/abs/1611.08588\\n[117] W. Shang et al. , “Understanding and improvi ng convolutional neural\\nnetworks via concatenated rectiﬁed linear units,” in Proc. ICML, 2016,\\npp. 1–9.\\n[118] C. Szegedy et al. , “Deep neural networks for object detection,” in\\nProc. NIPS, 2013, pp. 1–9.\\n[119] P. O. Pinheiro et al., “Learning to segment object candidates,” in Pr\\n oc.\\nNIPS, 2015, pp. 1990–1998.\\n[120] C. Szegedy et al. . (2014). “Scalable, high-quality object detection.”\\n[Online]. Available: https://arxiv.org/abs/1412.1441'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n20 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\n[121] M. Everingham et al. . (2011). The PASCAL Visual Object\\nClasses Challenge 2012 (VOC2012) Results (2012). [Online].\\nAvailable: http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html\\n[122] M. D. Zeiler and R. Fergus, “ Visualizing and understanding\\nconvolutional networks,” in Proc. ECCV, 2014, pp. 818–833.\\n[123] S. Xie et al. , “Aggregated residual transformations for deep neural\\nnetworks,” in Proc. CVPR, 2017, pp. 5987–5995.\\n[124] \"J. Dai et al. (2017). “Deformable convolutional networks.” [Online].\\nAvailable: https://arxiv.org/abs/1703.06211\\n[125] C. Rother et al. , “AutoCollage,” ACM Trans. Graph. , vol. 25, no. 3,\\npp. 847–852, 2006.\\n[126] C. Jung and C. Kim, “A uniﬁed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,” IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272–1283, Mar. 2012.\\n[127] W.-C. Tu et al. , “Real-time salient object detection with a minimum\\nspanning tree,” in Proc. CVPR, 2016, pp. 2334–2342.\\n[128] J. Yang and M.-H. Yang, “Top-down visual saliency via joint CRF and\\ndictionary learning,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39,\\nno. 3, pp. 576–588, Mar. 2017.\\n[129] P. L. Rosin, “A simple method for detecting salient regions,” Pattern\\nRecognit., vol. 42, no. 11, pp. 2363–2371, Nov. 2009.\\n[130] T. Liu et al., “Learning to detect a salient object,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 33, no. 2, pp. 353–367, Feb. 2011.\\n[131] J. Long et al. , “Fully convolutional networks for semantic\\nsegmentation,” in Proc. CVPR, 2015, pp. 3431–3440.\\n[132] D. Gao et al. , “Discriminant saliency, the detection of suspicious\\ncoincidences, and applications to visual recognition,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 31, no. 6, pp. 989–1005, Jun. 2009.\\n[133] S. Xie and Z. Tu, “Holistica lly-nested edge detection,” in Proc. ICCV,\\n2015, pp. 1395–1403.\\n[134] M. Kümmerer et al. . (2014). “Deep gaze I: Boosting saliency\\nprediction with feature maps trained on ImageNet.” [Online]. Available:\\nhttps://arxiv.org/abs/1411.1045\\n[135] X. Huang et al. , “SALICON: Reducing the semantic gap in saliency\\nprediction by adapting deep neural networks,” in Proc. ICCV , 2015,\\npp. 262–270.\\n[136] L. Wang et al. , “Deep networks for saliency detection via local\\nestimation and global search,” in Proc. CVPR, 2015, pp. 3183–3192.\\n[137] H. Cholakkal et al. . (2016). “Backtracking spatial pyramid pooling\\n(SPP)-based image classiﬁer for weakly supervised top-down salient\\nobject detection.” [Online]. Available: https://arxiv.org/abs/1611.05345\\n[138] R. Zhao et al. , “Saliency detection by multi-context deep learning,”\\nin Proc. CVPR, 2015, pp. 1265–1274.\\n[139] C. Bak et\\n al.. (2016). “Spatio-temporal saliency networks\\nfor dynamic saliency prediction.” [Online]. Available:\\nhttps://arxiv.org/abs/1607.04730\\n[140] S. He et al. , “SuperCNN: A superpixelwi se convolutional neural\\nnetwork for salient object detection,” Int. J. Comput. Vis. , vol. 115,\\nno. 3, pp. 330–344, 2015.\\n[141] X. Li et al., “DeepSaliency: Multi-task deep neural network model for\\nsalient object detection,” IEEE Trans. Image Process., vol. 25, no. 8,\\npp. 3919–3930, Aug. 2016.\\n[142] Y . Tang and X. Wu, “Saliency det ection via combining region-level\\nand pixel-level predictions with CNNs,” in Proc. ECCV , 2016,\\npp. 809–825.\\n[143] G. Li and Y . Yu, “Deep contrast lear ning for salient object detection,”\\nin Proc. CVPR, 2016, pp. 478–487.\\n[144] X. Wang et al. . (2016). “Edge preserving and multi-scale contextual\\nneural network for salient object detection.” [Online]. Available:\\nhttps://arxiv.org/abs/1608.08029\\n[145] M. Cornia et al., “A deep multi-level network for saliency prediction,”\\nin Proc. ICPR, 2016, pp. 3488–3493.\\n[146] G. Li and Y . Yu, “Visual saliency detection based on multiscale\\ndeep CNN features,” IEEE Trans. Image Process., vol. 25, no. 11,\\npp. 5012–5024, Nov. 2016.\\n[147] J. Pan et al. , “Shallow and deep convolutional networks for saliency\\nprediction,” in Proc. CVPR, 2016, pp. 598–606.\\n[148] J. Kuen et al., “Recurrent attentional networks for saliency detection,”\\nin Proc. CVPR, 2016, pp. 3668–3677.\\n[149] Y . Tang et al. , “Deeply-supervised recurrent convolutional neural\\nnetwork for saliency detection,” inProc. ACM MM, 2016, pp. 397–401.\\n[150] X. Li et al. , “Contextual hypergraph modeling for salient object\\ndetection,” in Proc. ICCV, 2013, pp. 3328–3335.\\n[151] M.-M. Cheng et al., “Global contrast based salient region detection,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 37, no. 3, pp. 569–582,\\nMar. 2015.\\n[152] H. Jiang et al. , “Salient object detection: A discriminative\\nregional feature integration approach,” in Proc. CVPR, 2013,\\npp. 2083–2090.\\n[153] G. Lee et al. , “Deep saliency with encoded low level distance map\\nand high level features,” in Proc. CVPR, 2016, pp. 660–668.\\n[154] Z. Luo et al. , “Non-local deep features for salient object detection,”\\nin Proc. CVPR, 2017, pp. 6593–6601.\\n[155] Q. Hou et al. . (2016). “Deeply supervised salient object\\ndetection with short connections.” [Online]. Available:\\nhttps://arxiv.org/abs/1611.04849\\n[156] Q. Yan et al.,\\n“Hierarchical saliency detection,” in Proc. CVPR, 2013,\\npp. 1155–1162.\\n[157] Y . Li et al. , “The secrets of salient object segmentation,” in Proc.\\nCVPR, 2014, pp. 280–287.\\n[158] V . Movahedi and J. H. Elder, “Design and perceptual validation\\nof performance measures for salient object segmentation,” in Proc.\\nCVPRW, 2010, pp. 49–56.\\n[159] A. Borji et al. , “Salient object detection: A bench-\\nmark,” IEEE Trans. Image Process. , vol. 24, no. 12,\\npp. 5706–5722, Dec. 2015.\\n[160] C. Peng et al. , “Graphical representation for heterogeneous face\\nrecognition,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39, no. 2,\\npp. 301–312, Feb. 2017.\\n[161] C. Peng et al. , “Face recognition from multiple stylistic sketches:\\nScenarios, datasets, and evaluation,” in Proc. ECCV, 2016, pp. 3–18.\\n[162] X. Gao et al., “Face sketch–photo synthesis and retrieval using sparse\\nrepresentation,” IEEE Trans. Circuits Syst. Video Technol. , vol. 22,\\nno. 8, pp. 1213–1226, Aug. 2012.\\n[163] N. Wang et al. , “A comprehensive survey to face hallucination,” Int.\\nJ. Comput. Vis., vol. 106, no. 1, pp. 9–30, 2014.\\n[164] C. Peng et al. , “Multiple representations-based face sketch–photo\\nsynthesis,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 27, no. 11,\\npp. 2201–2215, Nov. 2016.\\n[165] A. Majumder et al. , “Automatic facial expression recognition system\\nusing deep network-based data fusion,” IEEE Trans. Cybern., vol. 48,\\nno. 1, pp. 103–114, Jan. 2018.\\n[166] P. Viola and M. J. Jones, “Robust real-time face detection,” Int. J.\\nComput. Vis., vol. 57, no. 2, pp. 137–154, 2004.\\n[167] J. Yu et al. , “Unitbox: An advanced object detection network,” in\\nProc. ACM MM, 2016, pp. 516–520.\\n[168] S. S. Farfade et al., “Multi-view face detection using deep convolutional\\nneural networks,” in Proc. ICMR, 2015, pp. 643–650.\\n[169] S. Yang et al., “From facial parts responses to face detection: A deep\\nlearning approach,” in Proc. ICCV, 2015, pp. 3676–3684.\\n[170] S. Yang et al. , “Face detection through scale-friendly deep\\nconvolutional networks,” in Proc. CVPR, 2017, pp. 1–12.\\n[171] Z. Hao et al. , “Scale-aware face detection,” in Proc. CVPR, 2017,\\npp. 1913–1922.\\n[172] H. Wang et al. . (2017). “Face R-CNN.” [Online]. Available:\\nhttps://arxiv.org/abs/1706.01061\\n[173] X. Sun et al. . (2017). “Face detection using deep learning:\\nAn improved faster RCNN approach.” [Online]. Available:\\nhttps://arxiv.org/abs/1701.08289\\n[174] L. Huang et al. . (2015). “DenseBox: Unifying landmark\\nlocalization with end to end object detection.” [Online]. Available:\\nhttps://arxiv.org/abs/1509.04874\\n[175] Y . Li et al., “face detection with end-to-end integration of a ConvNet\\nand a 3D model,” in Proc. ECCV, 2016, pp. 420–436.\\n[176] K. Zhang et al. , “Joint face detection and alignment using multitask\\ncascaded convolutional networks,” IEEE Signal Process. Lett., vol. 23,\\nno. 10, pp. 1499–1503, Oct. 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, “Compact convolutional neural\\nnetwork cascade for face detection,” in Proc. CEUR Workshop, 2016,\\npp. 375–387.\\n[178] H. Qin et al., “Joint training of cascaded CNN for face detection,” in\\nProc. CVPR, 2016, pp. 3456–3465.\\n[179] V . Jain and E. Learned-Miller, “FDDB: A benchmark for face detection\\nin unconstrained settings,” Univ. Massachusetts, Amherst, Amherst,\\nMA, USA, Tech. Rep. UM-CS-2010-009, 2010.\\n[180] H. Li et al. , “A convolutional neural network cascade for face\\ndetection,” in Proc. CVPR, 2015, pp. 5325–5334.\\n[181] B. Yang et al. , “Aggregate channel features for multi-view face\\ndetection,” in Proc. IJCB, 2014, pp. 1–8.\\n[182] N. Markuš et al. . (2013). “Object detection with pixel intensity\\ncomparisons organized in decisi on trees.” [Online]. Available:\\nhttps://arxiv.org/abs/1305.4537\\n[183] M. Mathias et al. , “Face detection without bells and whistles,” in\\nProc. ECCV, 2014.\\n[184] J. Li and Y . Zhang, “Learning surf cascade for fast and accurate object\\ndetection,” in Proc. CVPR, 2013.\\n[185] S. Liao et al. , “A fast and accurate unconstrained face detector,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. 2,\\npp. 211–223, Feb. 2016.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 21\\n[186] B. Yang et al., “Convolutional channel features,” in Proc. ICCV, 2015,\\npp. 82–90.\\n[187] R. Ranjan et al. . (2016). “HyperFace: A deep multi-task\\nlearning framework for face det ection, landmark localization,\\npose estimation, and gender recognition.” [Online]. Available:\\nhttps://arxiv.org/abs/1603.01249\\n[188] P. Hu and D. Ramanan, “Finding tiny faces,” in Proc. CVPR, 2017,\\npp. 1522–1530.\\n[189] Z. Jiang and D. Q. Huynh, “Multiple pedestrian tracking from\\nmonocular videos in an interacting multiple model framework,” IEEE\\nTrans. Image Process., vol. 27, no. 3, pp. 1361–1375, Mar. 2018.\\n[190] D. M. Gavrila and S. Munder, “Multi-cue pedestrian detection and\\ntracking from a moving vehicle,” Int. J. Comput. Vis. , vol. 73, no. 1,\\npp. 41–59, Jun. 2007.\\n[191] S. Xu et al. , “Jointly attentive spatial-temporal pooling networks\\nfor video-based person re-identiﬁcation,” in Proc. ICCV , 2017,\\npp. 4743–4752.\\n[192] Z. Liu et al. , “Stepwise metric promotion for unsupervised video\\nperson re-identiﬁcation,” in Proc. ICCV, 2017, pp. 2448–2457.\\n[193] A. Khan et al. , “Cooperative robots to observe moving targets:\\nReview,”IEEE Trans. Cybern., vol. 48, no. 1, pp. 187–198, Jan. 2018.\\n[194] A. Geiger et al. , “Vision meets robotics: The KITTI dataset,” Int. J.\\nRobot. Res., vol. 32, no. 11, pp. 1231–1237, 2013.\\n[195] Z. Cai et al., “Learning complexity-aware cascades for deep pedestrian\\ndetection,” in Proc. ICCV, 2015, pp. 3361–3369.\\n[196] Y . Tian et al. , “Deep learning strong parts f or pedestrian detection,”\\nin Proc. CVPR, 2015, pp. 1904–1912.\\n[197] P. Dollár et al. , “Fast feature pyramids for object detection,” IEEE\\nTrans. Pattern Anal. Mach. Intell. , vol. 36, no. 8, pp. 1532–1545,\\nAug. 2014.\\n[198] S. Zhang et al. , “Filtered channel features for pedestrian detection,”\\nin Proc. CVPR, 2015, pp. 1751–1760.\\n[199] S. Paisitkriangkrai et al. , “Pedestrian detection with spatially pooled\\nfeatures and structured ensemble learning,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 38, no. 6, pp. 1243–1257, Jun. 2016.\\n[200] L. Lin et al. , “Discriminatively trained And-Or graph models for\\nobject shape detection,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 37, no. 5, pp. 959–972, May 2015.\\n[201] M. Mathias et al. , “Handling occlusions with Franken-classiﬁers,” in\\nProc. ICCV, 2013, pp. 1505–1512.\\n[202] S. Tang et al. , “Detection and tracking of occluded people,” Int. J.\\nComput.\\nVis., vol. 110, no. 1, pp. 58–69, 2014.\\n[203] L. Zhang et al., “Is faster R-CNN doing well for pedestrian detection?”\\nin Proc. ECCV, 2016, pp. 443–457.\\n[204] Y . Tian et al. , “Deep learning strong parts f or pedestrian detection,”\\nin Proc. ICCV, 2015.\\n[205] J. Liu et al.. (2016). “Multispectral deep neural networks for pedestrian\\ndetection.” [Online]. Available: https://arxiv.org/abs/1611.02644\\n[206] Y . Tian et al. , “Pedestrian detection aided by deep learning semantic\\ntasks,” in Proc. CVPR, 2015, pp. 5079–5087.\\n[207] X. Du et al. , “Fused DNN: A deep neural network fusion approach\\nto fast and robust pedestrian detection,” in Proc. WACV, 2017,\\npp. 953–961.\\n[208] Q. Hu et al. , “Pushing the limits of deep CNNs for pedestrian\\ndetection,” IEEE Trans. Circuits Syst. Video Technol. , vol. 28, no. 6,\\npp. 1358–1368, Jun. 2018.\\n[209] D. Tomé et al. , “Reduced memory region based deep convolutional\\nneural network detection,” in Proc. ICCE , Berlin, Germany, 2016,\\npp. 15–19.\\n[210] J. Hosang et al., “Taking a deeper look at pedestrians,” in Proc. CVPR,\\n2015, pp. 4073–4082.\\n[211] J. Li et al.. (2015). “Scale-aware fast R-CNN for pedestrian detection.”\\n[Online]. Available: https://arxiv.org/abs/1510.08160\\n[212] Y . Gao et al. , “Visual-textual joint relevance learning for tag-based\\nsocial image search,” IEEE Trans. Image Process., vol. 22, no. 1,\\npp. 363–376, Jan. 2013.\\n[213] T. Kong et al. , “RON: Reverse connection with objectness prior\\nnetworks for object detection,” in Proc. CVPR, 2017, pp. 5244–5252.\\n[214] I. J. Goodfellow et al. , “Generative adversarial nets,” in Proc. NIPS,\\n2014, pp. 2672–2680.\\n[215] Y . Fang et al. , “Object detection meets knowledge graphs,” in Proc.\\nIJCAI, 2017, pp. 1661–1667.\\n[216] S. Welleck et al. , “Saliency-based sequential image attention with\\nmultiset prediction,” in Proc. NIPS, 2017, pp. 5173–5183.\\n[217] S. Azadi et al., “Learning detection with diverse proposals,” in Proc.\\nCVPR, 2017, pp. 7369–7377.\\n[218] S. Sukhbaatar et al. , “End-to-end memory networks,” in Proc. NIPS,\\n2015, pp. 2440–2448.\\n[219]\\nP. Dabkowski and Y . Gal, “Real time image saliency for black box\\nclassiﬁers,” in Proc. NIPS, 2017, pp. 6967–6976.\\n[220] B. Yang et al., “CRAFT objects from images,” in Proc. CVPR, 2016,\\npp. 6043–6051.\\n[221] I. Croitoru et al. , “Unsupervised learning from video to detect fore-\\nground objects in single images,” in Proc. ICCV, 2017, pp. 4345–4353.\\n[222] C. Wang et al. , “Weakly supervised object localization with latent\\ncategory learning,” in Proc. ECCV, 2014, pp. 431–445.\\n[223] D. P. Papadopoulos et al., “Training object class detectors with click\\nsupervision,” in Proc. CVPR, 2017, pp. 180–189.\\n[224] J. Huang et al. , “Speed/accuracy trade-offs for modern convolutional\\nobject detectors,” in Proc. CVPR, 2017, pp. 3296–3297.\\n[225] Q. Li et al., “Mimicking very efﬁcient network for object detection,”\\nin Proc. CVPR, 2017, pp. 7341–7349.\\n[226] G. Hinton et al. , “Distilling the knowledge in a neural network,”\\nComput. Sci., vol. 14, no. 7, pp. 38–39, 2015.\\n[227] A. Romero et al., “FitNets: Hints for thin deep nets,” in Proc. ICLR,\\n2015, pp. 1–13.\\n[228] X. Chen et al. , “3D object proposals for accurate object class\\ndetection,” in Proc. NIPS, 2015, pp. 424–432.\\n[229] J. Dong et al. , “Visual-inertial-semantic scene representation for 3D\\nobject detection,” in Proc. CVPR, 2017, pp. 960–970.\\n[230] K. Kang et al. , “Object detection in videos with tubelet proposal\\nnetworks,” in Proc. CVPR, 2017, pp. 889–897.\\nZhong-Qiu Zhao (M’10) received the Ph.D. degree\\nin pattern recognition and intelligent system from\\nthe University of Science and Technology of China,\\nHefei, China, in 2007.\\nFrom 2008 to 2009, he held a post-doctoral\\nposition in image processing with the CNRS\\nUMR6168 Lab Sciences de Information et des\\nSystèmes, La Garde, France. From 2013 to 2014,\\nhe was a Research Fellow in image processing with\\nthe Department of Computer Science, Hong Kong\\nBaptist University, Hong Kong. He is currently a\\nProfessor with the Hefei University of Technology, Hefei. His current research\\ninterests include pattern recognition, image processing, and computer vision.\\nPeng Zheng received the bachelor’s degree from\\nthe Hefei University of Technology, Hefei, China,\\nin 2010, where he is currently pursuing the Ph.D.\\ndegree.\\nHis current research in terests include pattern\\nrecognition, image processing, and computer vision.\\nShou-Tao Xu is currently pursuing the master’s\\ndegree with the Hefei University of Technology,\\nHefei, China.\\nHis current research in terests include pattern\\nrecognition, image processing, deep learning, and\\ncomputer vision.\\nXindong Wu (F’11) received the Ph.D. degree in\\nartiﬁcial intelligence from The University of Edin-\\nburgh, Edinburgh, U.K.\\nHe is currently an Alfred and Helen Lamson\\nEndowed Professor of computer science with the\\nUniversity of Louisiana at Lafayette, Lafayette, LA,\\nUSA. His current research interests include data\\nmining, knowledge-based systems, and Web infor-\\nmation exploration.\\nDr. Wu is a Fellow of the AAAS. He is the\\nSteering Committee Chair of the IEEE International\\nConference on Data Mining. He served as the Editor-in-Chief for the IEEE\\nTRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING (IEEE Com-\\nputer Society) between 2005 and 2008.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "055056cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )   \n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c7166d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 63 documents into 315 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='The Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='P Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='for both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='comments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The\\ndata transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciﬁc training strate gies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='example generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-\\ning initial retrieval-focused pretraining followed by ful l-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more div erse data is crucial for\\nadvancing retrieval model performance, and that leveragin g LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Our model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-\\ntor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhan ced agent systems\\n∗ https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on ﬁne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiﬁcant performance improvements[ 4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ”signal states” repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe ﬁnal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='dient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance\\nof up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='enhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neﬃcient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiﬁcation. Our framework comprises two core components : 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc require ments of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extractio n from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s charact eristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and g eneralization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the ﬁrst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='capability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pa ir classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—suc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]—combined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model’s dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='followed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='we also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='for improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Embedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='to identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constrainin g tokens’ ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='categories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted in to a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ”Question/Query/User” portion is converted into que ries, while the\\n”Answer/Response/Assistant” portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='mantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical va lue of 1, while\\n”no”/”false” are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diﬀeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by us ing the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diﬀerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to\\nto enhance model training eﬃciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='additional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = − 1\\nn\\n∑\\ni\\nlog esim(qi,d +\\ni )/τ\\nesim(qi,d +\\ni )/τ + ∑\\nj esim(qi,d −\\nj )/τ + ∑\\nj̸=i esim(qi,q j )/τ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n∑\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) − sim(xi, x j)\\nτ )) (2)\\n3.3.3 CLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiﬁcation datasets with limited categories may lead to false neg atives from items\\nof diﬀerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='sample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = − 1\\nn\\n∑\\ni\\nlog esim(ti,t +\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/τ +\\n∑\\nn\\nMASK(ti, t −\\ni,n ) ·esim(ti,t −\\ni,n )/τ +\\n∑\\nj̸=i\\nMASK(ti, t j ) ·esim(ti,t j )/τ +\\n∑\\nj̸=i\\n∑\\nn\\nMASK(ti, t −\\nj,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='portance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃcu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='which represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must\\naccurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantic ally equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='semantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcat ions yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='enable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lα\\ni∑ m\\nj=1 lα\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed’s[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusive ly uses retrieval-\\noriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='datasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for η,\\nwhile non-retrieval data constitutes 1 − η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scalin g factor α , a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Sret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ ηRET ·lα\\ni\\nSret\\nif di ∈ RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='passage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface’s sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Classiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-\\nproximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='conﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations f or all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bﬂoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='boards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='both task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding\\nsecured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/c lassiﬁcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1\\nSeed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='NV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99\\nxiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Qwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transform ation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we d eveloped a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='strategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eﬀective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’9 4: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine le arning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Rangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-s hot learners.”\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”F ine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th Int ernational ACM\\nSIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Raghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large languag e models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jaspe r and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv :2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embedd ings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ ıc Magne, and Nils Reimers . ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embed ding: Gen-\\neral text embedding with more and better negative samples.” arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–id f measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='mation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGI R’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with tran sfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='multi-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Through Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embeddin g.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer norma lization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ”Glu variants improve transformer.” arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Wang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapor e. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Generative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345–2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unn atural in-\\nstructions: Tuning language models with (almost) no human labor.” ar Xiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Schiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='models, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis\\nplatform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='linguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation .” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ing comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and\\nMichael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558–3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='explainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association fo r Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriﬁcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='should not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='reason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='manuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Laureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The Faerie Queene’, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='signiﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork it wasn’t an epic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conﬂicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='wary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer’s\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1\\nObject Detection With Deep Learning: A Review\\nZhong-Qiu Zhao , Member, IEEE, Peng Zheng, Shou-Tao Xu, and Xindong Wu , Fellow, IEEE\\nAbstract— Due to object detection’s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by construct-\\ning complex ensembles that combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='classiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently in\\nnetwork architecture, training strategy, and optimization func-\\ntion. In this paper, we provide a review of deep learning-based\\nobject detection frameworks. Our review begins with a brief\\nintroduction on the history of deep learning and its representative\\ntool, namely, the convolutional neural network. Then, we focus\\non typical generic object detection architectures along with some\\nmodiﬁcations and useful tricks to improve detection performance\\nfurther. As distinct speciﬁc detection tasks exhibit different\\ncharacteristics, we also brieﬂy survey several speciﬁc tasks,\\nincluding salient object detection, face detection, and pedestrian\\ndetection. Experimental analyses are also provided to compare'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='characteristics, we also brieﬂy survey several speciﬁc tasks,\\nincluding salient object detection, face detection, and pedestrian\\ndetection. Experimental analyses are also provided to compare\\nvarious methods and draw some meaningful conclusions. Finally,\\nseveral promising directions and tasks are provided to serve as\\nguidelines for future work in both object detection and relevant\\nneural network-based learning systems.\\nIndex Terms— Deep learning, neural network, object detection.\\nI. I NTRODUCTION\\nT\\nO GAIN a complete image understanding, we should\\nnot only concentrate on classifying different images but\\nalso try to precisely estimat e the concepts and locations\\nof objects contained in each image. This task is referred\\nas object detection [1], [S1], which usually consists of dif-\\nferent subtasks such as face detection [2], [S2], pedestrian\\ndetection [3], [S2], and skeleton detection [4], [S3]. As one of\\nthe fundamental computer vision problems, object detection'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ferent subtasks such as face detection [2], [S2], pedestrian\\ndetection [3], [S2], and skeleton detection [4], [S3]. As one of\\nthe fundamental computer vision problems, object detection\\nis able to provide valuable information for semantic under-\\nstanding of images and videos and is related to many applica-\\ntions, including image classiﬁcation [5], [6], human behavior\\nanalysis [7], [S4], face recognition [8], [S5], and autonomous\\ndriving [9], [10]. Meanwhile, inheriting from neural networks\\nManuscript received September 8, 2017; revised March 3, 2018 and\\nJuly 12, 2018; accepted October 15, 2018. This work was supported in part\\nby the National Natural Scienc e Foundation of China under Grant 61672203,\\nGrant 61375047, and Grant 91746209, in part by the National Key Research\\nand Development Program of China under Grant 2016YFB1000901, and in\\npart by the Anhui Natural Science F unds for Distinguished Young Scholar\\nunder Grant 170808J08. (Corresponding author: Zhong-Qiu Zhao.)'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='part by the Anhui Natural Science F unds for Distinguished Young Scholar\\nunder Grant 170808J08. (Corresponding author: Zhong-Qiu Zhao.)\\nZ.-Q. Zhao, P. Zheng, and S.-T. Xu are with the College of Computer\\nScience and Information Engineering, Hefei University of Technology, Hefei\\n230009, China (e-mail: zhongqiuzhao@gmail.com).\\nX. Wu is with the School of Computing and Informatics, University of\\nLouisiana at Lafayette, Lafayette, LA 70504 USA.\\nThis paper has supplementary downloadable material available at\\nhttp://ieeexplore.ieee.org, provided by the authors.\\nColor versions of one or more of the ﬁgures in this paper are available\\nonline at http://ieeexplore.ieee.org.\\nDigital Object Identiﬁer 10.1109/TNNLS.2018.2876865\\nand related learning systems, the progress in these ﬁelds\\nwill develop neural network algorithms and will also have\\ngreat impacts on object detection techniques that can be\\nconsidered as learning systems [11]–[14], [S6]. However, due'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='will develop neural network algorithms and will also have\\ngreat impacts on object detection techniques that can be\\nconsidered as learning systems [11]–[14], [S6]. However, due\\nto large variations in viewpoints, poses, occlusions, and light-\\ning conditions, it is difﬁcult to perfectly accomplish object\\ndetection with an additional object localization task. Therefore,\\nmuch attention has been attr acted to this ﬁeld in recent\\nyears [15]–[18].\\nThe problem deﬁnition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object be longs to (object classiﬁca-\\ntion). Therefore, the pipeline of traditional object detection\\nmodels can be mainly divided into three stages: informative\\nregion selection, feature extraction, and classiﬁcation.\\nA. Informative Region Selection\\nAs different objects may appear in any positions of the\\nimage and have different aspect ratios or sizes, it is a natural'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='A. Informative Region Selection\\nAs different objects may appear in any positions of the\\nimage and have different aspect ratios or sizes, it is a natural\\nchoice to scan the whole image with a multiscale sliding\\nwindow. Although this exhaustive strategy can ﬁnd out all\\npossible positions of the objects, its shortcomings are also\\nobvious. Due to a large number of candidate windows, it is\\ncomputationally expensive and produces too many redundant\\nwindows. However, if only a ﬁxed number of sliding window\\ntemplates is applied, unsatisfactory regions may be produced.\\nB. Feature Extraction\\nTo recognize different objects, we need to extract visual\\nfeatures that can provide a semantic and robust represen-\\ntation. Scale-invariant feature transform [19], histograms of\\noriented gradients (HOG) [20], and Haar-like [21] features are\\nthe representative ones. This is due to the fact that these\\nfeatures can produce representations associated with complex'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='oriented gradients (HOG) [20], and Haar-like [21] features are\\nthe representative ones. This is due to the fact that these\\nfeatures can produce representations associated with complex\\ncells in human brain [19]. However, due to the diversity of\\nappearances, illumination conditions, and backgrounds, it is\\ndifﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nC. Classiﬁcation\\nBesides, a classiﬁer is needed to distinguish a target object\\nfrom all the other categories and to make the representations\\nmore hierarchical, semantic, and informative for visual recog-\\nnition. Usually, the supported vector machine (SVM) [22],\\nAdaBoost [23], and deformable part-based model (DPM) [24]\\nare good choices. Among these classiﬁers, the DPM is a\\nﬂexible model by combining object parts with deformation\\ncost to handle severe deformations. In DPM, with the aid\\nof a graphical model, carefully designed low-level features'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ﬂexible model by combining object parts with deformation\\ncost to handle severe deformations. In DPM, with the aid\\nof a graphical model, carefully designed low-level features\\nand kinematically inspired part decompositions are combined.\\n2162-237X © 2019 IEEE. Personal u se is perm itted, but republication/redistribution requires IEEE permission.\\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nFig. 1. Application dom ains of object detection.\\nDiscriminative learning of graphical models allows for build-\\ning high-precision part-based models for a variety of object\\nclasses.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state-of-the-art results have\\nbeen obtained on PASCAL visual object classes (VOC) object\\ndetection competition [25] and real-time embedded systems\\nhave been obtained with a low burden on hardware. However,\\nsmall gains are obtained during 2010–2012 by only building\\nensemble systems and employing minor variants of successful\\nmethods [15]. This fact is due to the following reasons: 1) the\\ngeneration of candidate bounding boxes (BBs) with a sliding\\nwindow strategy is redundant, inefﬁcient, and inaccurate and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='methods [15]. This fact is due to the following reasons: 1) the\\ngeneration of candidate bounding boxes (BBs) with a sliding\\nwindow strategy is redundant, inefﬁcient, and inaccurate and\\n2) the semantic gap cannot be bridged by the combination\\nof manually engineered low-level descriptors and discrimina-\\ntively trained shallow models.\\nThanks to the emergency of deep neural networks\\n(DNNs) [6], [26], [S7], a more signiﬁcant gain is obtained\\nwith the introduction of regions with convolutional neural\\nnetwork (CNN) features (R-CNN) [15]. DNNs, or the most\\nrepresentative CNNs, act in a quite different way from tra-\\nditional approaches. They have d eeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso, the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [27].\\nSince the proposal of R-CNN, a great deal of improved'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='learn informative object representations without the need to\\ndesign features manually [27].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including fast R-CNN that\\njointly optimizes classiﬁcation and bounding box regres-\\nsion tasks [16], faster R-CNN that takes an additional sub-\\nnetwork to generate region proposals [17], and you only\\nlook once (YOLO) that accomplishes object detection via a\\nﬁxed-grid regression [18]. All of them bring different degrees\\nof detection performance improvements over the primary\\nR-CNN and make real-time and accurate object detection more\\nachievable.\\nIn this paper, a systematic review is provided to\\nsummarize representative models and their different char-\\nacteristics in several application domains, including generic\\nobject detection [15]–[17], salient object detection [28], [29],\\nface detection [30]–[32], and pedestrian detection [33], [34].\\nTheir relationships are depicted in Fig. 1. Based on basic'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object detection [15]–[17], salient object detection [28], [29],\\nface detection [30]–[32], and pedestrian detection [33], [34].\\nTheir relationships are depicted in Fig. 1. Based on basic\\nCNN architectures, the generic object detection is achieved\\nwith bounding box regression, while salient object detec-\\ntion is accomplished with local contrast enhancement and\\npixel-level segmentation. Face detection and pedestrian detec-\\ntion are closely related to ge neric object detection and\\nmainly accomplished with multiscale adaption and multi-\\nfeature fusion/boosting forest, respectively. The dotted lines\\nindicate that the corresponding domains are associated with\\neach other under certain conditions. It should be noticed\\nthat the covered domains are diversiﬁed. Pedestrian and face\\nimages have regular structures, while general objects and scene\\nimages have more complex variations in geometric structures\\nand layouts. Therefore, different deep models are required by\\nvarious images.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='images have more complex variations in geometric structures\\nand layouts. Therefore, different deep models are required by\\nvarious images.\\nThere has been a relevant pion eer effort [35] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classiﬁcation and object detection but\\npays little attention on detailing speciﬁc algorithms. Different\\nfrom it, our work not only reviews deep learning-based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section II,\\na brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section III. Then, reviews\\nof CNN applied in several speciﬁc tasks, including salient\\nobject detection, face detection, and pedestrian detection, are'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tion architectures are presented in Section III. Then, reviews\\nof CNN applied in several speciﬁc tasks, including salient\\nobject detection, face detection, and pedestrian detection, are\\nexhibited in Section IV–VI, respectively. Several promising\\nfuture directions are proposed in Section VII. At last, some\\nconcluding remarks are presented in Section VIII.\\nII. B\\nRIEF OVERVIEW OF DEEP LEARNING\\nPrior to an overview on deep learning-based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. History: Birth, Decline, and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date\\nback to the 1940s [36], and the original intention was to\\nsimulate the human brain system to solve general learning\\nproblems in a principled way. It was popular in the 1980s and\\n1990s with the proposal of the back-propagation algorithm'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='simulate the human brain system to solve general learning\\nproblems in a principled way. It was popular in the 1980s and\\n1990s with the proposal of the back-propagation algorithm\\nby Rumelhart et al. [37]. However, due to the overﬁtting of\\ntraining, lack of large-scale training data, limited computation\\npower, and insigniﬁcance in performance compared with other\\nmachine learning tools, neural networks fell out of fashion in\\nthe early 2000s.\\nDeep learning has become popular since 2006 [26], [S7],\\nwith a breakthrough in speech recognition [38]. The recovery\\nof deep learning can be attributed to the following factors.\\n1) The emergence of large-scale annotated training data,\\nsuch as ImageNet [39], to fully exhibit its very large\\nlearning capacity.\\n2) Fast development of high-performance parallel comput-\\ning systems, such as GPU clusters.\\n3) Signiﬁcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ing systems, such as GPU clusters.\\n3) Signiﬁcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npretraining guided by autoencoder [40] or restricted\\nBoltzmann machine [41], a good initialization is pro-\\nvided. With dropout and data augmentation, the over-\\nﬁtting problem in training has been relieved [6], [42].'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 3\\nWith batch normalization (BN), the training of very\\nDNNs becomes quite efﬁcient [43]. Meanwhile, various\\nnetwork structures, such as AlexNet [6], Overfeat [44],\\nGoogLeNet [45], Visual Geometry Group (VGG) [46],\\nand Residual Net (ResNet) [47], have been extensively\\nstudied to improve the performance.\\nWhat prompts deep learning to have a huge impact on\\nthe entire academic commun ity? It may owe to the con-\\ntribution of Hinton’s group, whose continuous efforts have\\ndemonstrated that deep learning would bring a revolutionary\\nbreakthrough on grand challenges rather than just obvious\\nimprovements on small data sets. Their success results from\\ntraining a large CNN on 1.2 million labeled images together\\nwith a few techniques [6] [e.g., rectiﬁed linear unit (ReLU)'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='improvements on small data sets. Their success results from\\ntraining a large CNN on 1.2 million labeled images together\\nwith a few techniques [6] [e.g., rectiﬁed linear unit (ReLU)\\noperation [48] and “dropout” regularization].\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [27].\\nA typical CNN architecture, which is referred to as VGG16,\\ncan be found in Fig. S1 in the supplementary material. Each\\nlayer of CNN is known as a feature map. The feature map\\nof the input layer is a 3-D matrix of pixel intensities for\\ndifferent color channels (e.g., RGB). The feature map of\\nany internal layer is an induced multichannel image, whose\\n“pixel” can be viewed as a speciﬁc feature. Every neu-\\nron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive ﬁeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as ﬁltering and pooling. Filtering (convolution)'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='from the previous layer (receptive ﬁeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as ﬁltering and pooling. Filtering (convolution)\\noperation convolutes a ﬁlter matrix (learned weights) with\\nthe values of a receptive ﬁeld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain ﬁnal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling, and local contrast normalization [52],\\nsummarizes the responses of a receptive ﬁeld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an ini-\\ntial feature hierarchy is constructed, which can be ﬁne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the ﬁnal layer with different activation functions [6]\\nis added to get a speciﬁc conditional probability for each'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='layers to adapt to different visual tasks. According to the tasks\\ninvolved, the ﬁnal layer with different activation functions [6]\\nis added to get a speciﬁc conditional probability for each\\noutput neuron. The whole network can be optimized on an\\nobjective function (e.g., mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 co nvolutional (conv) layers,\\n3 FC layers, 3 max-pooling layers, and a softmax classiﬁcation\\nlayer. The conv feature maps are produced by convoluting\\n3*3 ﬁlter windows, and feature map resolutions are reduced\\nwith 2 stride max-pooling layers. An arbitrary test image of the\\nsame size as training samples can be processed with the trained\\nnetwork. Rescaling or cropping operations may be needed if\\ndifferent sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarized as follows.\\n1) Hierarchical feature rep resentation, which is the'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='different sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarized as follows.\\n1) Hierarchical feature rep resentation, which is the\\nmultilevel representations from pixel to high-level\\nsemantic features learned by a hierarchical multistage\\nFig. 2. Two types of frameworks: region proposal based and\\nregression/classiﬁcation based. SPP: spatial pyramid pooling [64], FRCN:\\nfaster R-CNN [16], RPN: region pr oposal network [17], FCN: fully con-\\nvolutional network [65], BN: batch nor malization [43], and Deconv layers:\\ndeconvolution layers [54] .\\nstructure [15], [53], can be learned from data automati-\\ncally and hidden factors of input data can be disentan-\\ngled through multilevel nonlinear mappings.\\n2) Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expres-\\nsive capability.\\n3) The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g., fast'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='architecture provides an exponentially increased expres-\\nsive capability.\\n3) The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g., fast\\nR-CNN combines classiﬁcation and bounding box\\nregression into a multitask learning manner).\\n4) Beneﬁtting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can\\nbe recast as high-dimensiona l data transform problems\\nand solved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research ﬁelds, such as image superresolu-\\ntion reconstruction [54], [55], image classiﬁcation [5], [56],\\nimage retrieval [57], [58], face recognition [8], [S5], pedes-\\ntrian detection [59]–[61], and video analysis [62], [63].\\nIII. G\\nENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image and labeling them with\\nrectangular BBs to show the conﬁdences of existence. The'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image and labeling them with\\nrectangular BBs to show the conﬁdences of existence. The\\nframeworks of generic object detection methods can mainly\\nbe categorized into two types (see Fig. 2). One follows the tra-\\nditional object detection pipeline, generating region proposals\\nat ﬁrst and then classifying each proposal into different object\\ncategories. The other regards object detection as a regression\\nor classiﬁcation problem, adopting a uniﬁed framework to\\nachieve ﬁnal results (categories and locations) directly. The\\nregion proposal-based methods mainly include R-CNN [15],\\nspatial pyramid pooling (SPP)-net [64], Fast R-CNN [16],\\nFaster R-CNN [17], region-based fully convolutional network\\n(R-FCN) [65], feature pyramid networks (FPN) [66], and\\nMask R-CNN [67], some of which are correlated with each\\nother (e.g., SPP-net modiﬁes R-CNN with an SPP layer).'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='(R-FCN) [65], feature pyramid networks (FPN) [66], and\\nMask R-CNN [67], some of which are correlated with each\\nother (e.g., SPP-net modiﬁes R-CNN with an SPP layer).\\nThe regression /classiﬁcation-based methods mainly include\\nMultiBox [68], AttentionNet [69], G-CNN [70], YOLO [18],\\nSingle Shot MultiBox Detector (SSD) [71], YOLOv2 [72],\\ndeconvolutional single shot detector (DSSD) [73], and deeply\\nsupervised object detectors (DSOD) [74]. The correlations\\nbetween these two pipelines are bridged by the anchors\\nintroduced in Faster R-CNN. Details of these methods are as\\nfollows.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n4 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nFig. 3. Flowchart of R-CNN [15], which consists of three stages: 1) extracts\\nBU region proposals, 2) computes features for each proposal using a CNN,\\nand then 3) classiﬁes each region with class-speciﬁc linear SVMs.\\nA. Region Proposal-Based Framework\\nThe region proposal-based framework, a two-step process,\\nmatches the attentional mechanism of the human brain to\\nsome extent, which gives a coarse scan of the whole scenario\\nﬁrst and then focuses on regions of interest (RoIs). Among\\nthe prerelated works [44], [75], [76], the most representative\\none is Overfeat [44]. This model inserts CNN into the sliding\\nwindow method, which predicts BBs directly from locations\\nof the topmost feature map after obtaining the conﬁdences of\\nunderlying object categories.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='window method, which predicts BBs directly from locations\\nof the topmost feature map after obtaining the conﬁdences of\\nunderlying object categories.\\n1) R-CNN: It is of signiﬁcance to improve the quality\\nof candidate BBs and to take a deep architecture to extract\\nhigh-level features. To solve these problems, R-CNN was\\nproposed by Girshick et al. [15] and obtained a mean average\\nprecision (mAP) of 53 .3% with more than 30% improvement\\nover the previous best result (DPM histograms of sparse\\ncodes [77]) on PASCAL VOC 2012. Fig. 3 shows the ﬂow-\\nchart of R-CNN, which can be divided into three stages as\\nfollows.\\na) Region Proposal Generation: The R-CNN adopts\\nselective search [78] to generate about 2000 region proposals\\nfor each image. The selective search method relies on simple\\nbottom-up (BU) grouping and saliency cues to provide more\\naccurate candidate boxes of arbitrary sizes quickly and to\\nreduce the searching space in object detection [24], [39].'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='bottom-up (BU) grouping and saliency cues to provide more\\naccurate candidate boxes of arbitrary sizes quickly and to\\nreduce the searching space in object detection [24], [39].\\nb) CNN-Based Deep Feature Extraction: In this stage,\\neach region proposal is warped or cropped into a ﬁxed\\nresolution, and the CNN module in [6] is utilized to extract\\na 4096-dimensional feature as the ﬁnal representation. Due\\nto large learning capacity, dominant expressive power, and\\nhierarchical structure of CNNs, a high-level, semantic, and\\nrobust feature representation for each region proposal can be\\nobtained.\\nc) Classiﬁcation and Localization: With pretrained\\ncategory-speciﬁc linear SVMs for multiple classes, different\\nregion proposals are scored on a set of positive regions and\\nbackground (negative) regions. The scored regions are then\\nadjusted with bounding box regression and ﬁltered with a\\ngreedy nonmaximum suppression (NMS) to produce ﬁnal BBs\\nfor preserved object locations.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='adjusted with bounding box regression and ﬁltered with a\\ngreedy nonmaximum suppression (NMS) to produce ﬁnal BBs\\nfor preserved object locations.\\nWhen there are scarce or insufﬁcient labeled data,\\npretraining is usually conducted. Instead of unsupervised\\npretraining [79], R-CNN ﬁrst conducts supervised pretraining\\non ImageNet Large-Scale Visual Recognition Competition,\\na very large auxiliary data set, and then takes a domain-speciﬁc\\nﬁne-tuning. This scheme has been adopted by most of the\\nsubsequent approaches [16], [17].\\nIn spite of its improvements over traditional methods and\\nsigniﬁcance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n1) Due to the existence of FC layers, the CNN requires a\\nﬁxed size (e.g., 227 × 227) input image, which directly\\nleads to the recomputation of the whole CNN for each\\nevaluated region, taking a great deal of time in the testing\\nperiod.\\n2) Training of R-CNN is a multistage pipeline. At ﬁrst,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='leads to the recomputation of the whole CNN for each\\nevaluated region, taking a great deal of time in the testing\\nperiod.\\n2) Training of R-CNN is a multistage pipeline. At ﬁrst,\\na convolutional network (ConvNet) on object proposals\\nis ﬁne-tuned. Then, the softmax classiﬁer learned by\\nﬁne-tuning is replaced by SVMs to ﬁt in with ConvNet\\nfeatures. Finally, bounding-box regressors are trained.\\n3) Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on\\nthe disk. It will take a long time to process a relatively\\nsmall training set with very deep networks, such as\\nVGG16. At the same time, the storage memory required\\nby these features should also be a matter of concern.\\n4) Although selective search can generate region propos-\\nals with relatively high recalls, the obtained region\\nproposals are still redundant and this procedure is\\ntime-consuming (around 2 s to extract 2000 region\\nproposals).\\nTo solve these problems, many methods have been'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='proposals are still redundant and this procedure is\\ntime-consuming (around 2 s to extract 2000 region\\nproposals).\\nTo solve these problems, many methods have been\\nproposed. Geodesic object proposals [80] takes a much faster\\ngeodesic-based segmentation to replace traditional graph\\ncuts. Mutiscale combinatorial grouping [81] searches different\\nscales of the image for multiple hierarchical segmentations and\\ncombinatorially groups different regions to produce proposals.\\nInstead of extracting visually distinct segments, the edge boxes\\nmethod [82] adopts the idea that objects are more likely to\\nexist in BBs with fewer contours straggling their boundaries.\\nAlso, some studies tried to r erank or reﬁne preextracted\\nregion proposals to remove unn ecessary ones and obtained a\\nlimited number of valuable ones, such as DeepBox [83] and\\nSharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='limited number of valuable ones, such as DeepBox [83] and\\nSharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na Bayesian optimization-based search algorithm to guide\\nthe regressions of different BBs sequentially and trained\\nclass-speciﬁc CNN classiﬁers with a structured loss to penal-\\nize the localization inaccuracy explicitly. Gupta et al. [86]\\nimproved object detection for RGB-D images with seman-\\ntically rich image and depth features and learned a new\\ngeocentric embedding for dep th images to encode each pixel.\\nThe combination of object detectors and superpixel classi-\\nﬁcation framework gains a promising result on the seman-\\ntic scene segmentation task. Ouyang et al. [87] proposed a\\ndeformable deep CNN (DeepID-Net) that introduces a novel\\ndeformation constrained pooling (def-pooling) layer to impose\\ngeometric penalty on the deformation of various object parts'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='deformable deep CNN (DeepID-Net) that introduces a novel\\ndeformation constrained pooling (def-pooling) layer to impose\\ngeometric penalty on the deformation of various object parts\\nand makes an ensemble of models with different settings.\\nLenc and Vedaldi [88] provided an analysis on the role of\\nproposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth BBs with K -means clustering.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 5\\nFig. 4. Architecture of SPP-net for object detection [64].\\nHowever, more candidate boxes are required to achieve com-\\nparable results to those of R-CNN.\\n2) SPP-Net: FC layers must take a ﬁxed-size input. That\\nis why R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. [64] took the theory of\\nspatial pyramid matching (SPM) [89], [90] into consideration\\nand proposed a novel CNN architecture named SPP-net. SPM\\ntakes several ﬁner to coarser scales to partition the image into'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='spatial pyramid matching (SPM) [89], [90] into consideration\\nand proposed a novel CNN architecture named SPP-net. SPM\\ntakes several ﬁner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Fig. 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the ﬁfth conv layer (conv5) to project region\\nproposals of arbitrary sizes to ﬁxed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses but also have relationships with their spatial\\npositions [64]. The layer after the ﬁnal conv layer is referred to\\nas the SPP layer. If the number of feature maps in conv5 is 256,\\ntaking a three-level pyramid, the ﬁnal feature vector for each\\nregion proposal obtained after the SPP layer has a dimension\\nof 256 × (1\\n2 + 22 + 42) = 5376.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='taking a three-level pyramid, the ﬁnal feature vector for each\\nregion proposal obtained after the SPP layer has a dimension\\nof 256 × (1\\n2 + 22 + 42) = 5376.\\nSPP-net not only gains better results with a correct estima-\\ntion of different region proposals in their corresponding scales\\nbut also improves detection efﬁciency in the testing period\\nwith the sharing of computation cost before SPP layer among\\ndifferent proposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and efﬁciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost the\\nsame multistage pipeline as R-CNN, including feature extrac-\\ntion, network ﬁne-tuning, SVM training, and bounding-box\\nregressor ﬁtting. Therefore, an additional expense on storage\\nspace is still required. In addition, the conv layers preceding\\nthe SPP layer cannot be updated with the ﬁne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='space is still required. In addition, the conv layers preceding\\nthe SPP layer cannot be updated with the ﬁne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multitask loss on classiﬁca tion and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Fig. 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a ﬁxed-length feature\\nvector is extracted from each region proposal with an RoI\\nFig. 5. Architecture of Fast R-CNN [16].\\npooling layer. The RoI pooling l ayer is a special case of the\\nSPP layer, which has only one pyramid level. Each feature\\nvector is then fed into a sequence of FC layers before ﬁnally\\nbranching into two sibling output layers. One output layer is\\nresponsible for producing softmax probab ilities for all C + 1'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='vector is then fed into a sequence of FC layers before ﬁnally\\nbranching into two sibling output layers. One output layer is\\nresponsible for producing softmax probab ilities for all C + 1\\ncategories ( C object classes plus one “background” class)\\nand the other output layer encodes reﬁned bounding-box\\npositions with four real-valued numbers. All parameters in\\nthese procedures (except the generation of region proposals)\\nare optimized via a multitask loss in an end-to-end way.\\nThe multitasks loss L is deﬁned in the following to jointly\\ntrain classiﬁcation and bounding-box regression:\\nL(p,u,t\\nu ,v) = Lcls(p,u) + λ[u ≥ 1]Lloc(tu ,v) (1)\\nwhere Lcls(p,u) =− log pu calculates the log loss for ground\\ntruth class u,a n d pu is driven from the discrete probability\\ndistribution p = (p0,··· , pC ) over the C +1 outputs from the\\nlast FC layer. Lloc(tu ,v) is deﬁned over the predicted offsets\\ntu = (tu\\nx ,tu\\ny ,tu\\nw,tu\\nh ) and ground-truth bounding-box regression'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='distribution p = (p0,··· , pC ) over the C +1 outputs from the\\nlast FC layer. Lloc(tu ,v) is deﬁned over the predicted offsets\\ntu = (tu\\nx ,tu\\ny ,tu\\nw,tu\\nh ) and ground-truth bounding-box regression\\ntargets v = (vx ,v y ,v w,v h ),w h e r ex, y,w, and h denote\\nthe two coordinates of the box center, width, and height,\\nrespectively. Each t\\nu adopts the parameter settings in [15] to\\nspecify an object proposal with a log-space height/width shift\\nand scale-invariant translati on. The Iverson bracket indicator\\nfunction [u ≥ 1] is employed to omit all background RoIs.\\nTo provide more robustness against outliers and eliminate the\\nsensitivity in exploding gradients, a smooth L\\n1 loss is adopted\\nto ﬁt bounding-box regressors as follows:\\nLloc(tu,v) =\\n∑\\ni∈x,y,w,h\\nsmoothL1\\n(tu\\ni − vi\\n) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if |x| < 1\\n|x|−0 .5o t h e r w i s e. (3)\\nTo accelerate the pipeline of Fast R-CNN, another two\\ntricks are of necessity. On the one hand, if training sam-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='i − vi\\n) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if |x| < 1\\n|x|−0 .5o t h e r w i s e. (3)\\nTo accelerate the pipeline of Fast R-CNN, another two\\ntricks are of necessity. On the one hand, if training sam-\\nples (i.e., RoIs) come from different images, backpropagation\\nthrough the SPP layer becomes highly inefﬁcient. Fast R-CNN\\nsamples minibatches hi erarchically, namely, N images sam-\\npled randomly at ﬁrst and then R/N RoIs sampled in each\\nimage, where R represents the number of RoIs. Critically,\\ncomputation and memory are shared by RoIs from the same\\nimage in the forward and backward pass. On the other hand,\\nmuch time is spent in computing the FC layers during the\\nforward pass [16]. The truncated singular value decomposition\\n(SVD) [91] can be utilized to compress large FC layers and\\nto accelerate the testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to accelerate the testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single stage with a multitask loss. It saves the additional'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nFig. 6. RPN in Faster R-CNN [17]. K predeﬁned anchor boxes are\\nconvoluted with each sliding window to produce ﬁxed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nexpense on storage space and improves both accuracy and\\nefﬁciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such\\nas selective search and Edgebox, to generate a candidate pool\\nof isolated region proposals. Region proposal computation\\nis also a bottleneck in improving efﬁciency. To solve this\\nproblem, Ren et al. [17], [92] introduced an additional region\\nproposal network (RPN), which acts in a nearly cost-free way'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='is also a bottleneck in improving efﬁciency. To solve this\\nproblem, Ren et al. [17], [92] introduced an additional region\\nproposal network (RPN), which acts in a nearly cost-free way\\nby sharing full-image conv features with detection network.\\nRPN is achieved with an FCN, which has the ability to\\npredict object bounds and scores at each position simultane-\\nously. Similar to [78], RPN takes an image of arbitrary size to\\ngenerate a set of rectangular object proposals. RPN operates\\non a speciﬁc conv layer with th e preceding layers shared with\\nthe object detection network.\\nThe architecture of RPN is shown in Fig. 6. The network\\nslides over the conv feature map and fully connects to an n×n\\nspatial window. A low-dimensional vector (512-dimensional\\nfor VGG16) is obtained in each s liding window and fed into\\ntwo sibling FC layers, namely, box-classiﬁcation layer (cls)\\nand box-regression layer (reg). This architecture is imple-\\nmented with an n × n conv layer followed by two sibling'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='two sibling FC layers, namely, box-classiﬁcation layer (cls)\\nand box-regression layer (reg). This architecture is imple-\\nmented with an n × n conv layer followed by two sibling\\n1 × 1 conv layers. To increase nonlinearity, ReLU is applied\\nto the output of the n × n conv layer.\\nThe regressions toward true BBs are achieved by comparing\\nproposals relative to reference boxes (anchors). In the Faster\\nR-CNN, anchors of three scales and three aspect ratios are\\nadopted. The loss function is similar to (1)\\nL(p\\ni ,ti ) = 1\\nNcls\\n∑\\ni\\nLcls\\n(\\npi , p∗\\ni\\n)\\n+ λ 1\\nNreg\\n∑\\ni\\np∗\\ni Lreg\\n(\\nti ,t∗\\ni\\n)\\n(4)\\nwhere pi is the predicted probability of the ith anchor being an\\nobject. The ground truth label p∗\\ni is 1 if the anchor is positive,\\notherwise 0. ti stores four parameterized coordinates of the\\npredicted bounding box while t∗\\ni is related to the ground-truth\\nbox overlapping with a positive anchor.Lcls is a binary log loss\\nand Lreg is a smoothed L1 loss similar to (2). These two terms'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='predicted bounding box while t∗\\ni is related to the ground-truth\\nbox overlapping with a positive anchor.Lcls is a binary log loss\\nand Lreg is a smoothed L1 loss similar to (2). These two terms\\nare normalized with the minibatch size ( Ncls) and the number\\nof anchor locations ( Nreg), respectively. In the form of FCNs,\\nFaster R-CNN can be trained end-to-end by backpropagation\\nand SGD in an alternate training manner.\\nWith the proposal of Faster R-CNN, region proposal-based\\nCNN architectures for object detection can really be trained in\\nan end-to-end way. Also, a frame rate of 5 frames per second\\n(fps) on a GPU is achieved with the state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces objectlike regi ons (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a preva-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='instead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a preva-\\nlent family [16], [17] of deep networks for object detection\\nis composed of two subnetworks: a shared fully convolu-\\ntional subnetwork (independe nt of RoIs) and an unshared\\nRoI-wise subnetwork. This decomposition originates from\\npioneering classiﬁcation archit ectures (e.g., AlexNet [6] and\\nVGG16 [46]) which consist of a convolutional subnetwork and\\nseveral FC layers separated by a s peciﬁc spatial pooling layer.\\nRecent state-of-the-art image c lassiﬁcation networks, such\\nas ResNets [47] and GoogLeNets [45], [93], are fully convo-\\nlutional. To adapt to these architectures, it is natural to con-\\nstruct a fully convolutional object detection network without\\nRoI-wise subnetwork. However, it turns out to be inferior\\nwith such a naive solution [47]. This inconsistency is due\\nto the dilemma of respecting translation variance in object'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RoI-wise subnetwork. However, it turns out to be inferior\\nwith such a naive solution [47]. This inconsistency is due\\nto the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classiﬁcation. In other words, shifting an object inside\\nan image should be indiscriminative in image classiﬁcation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregionwise layers. Therefore, Dai et al. [65] proposed an\\nR-FCNs (see Fig. S2 in the supplementary material).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k\\n2 position-sensitive\\ns c o r em a p sw i t haﬁ x e dg r i do fk × k ﬁrst and a position-\\nsensitive RoI pooling layer is then appended to aggregate\\nthe responses from these score maps. Finally, in each'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='s c o r em a p sw i t haﬁ x e dg r i do fk × k ﬁrst and a position-\\nsensitive RoI pooling layer is then appended to aggregate\\nthe responses from these score maps. Finally, in each\\nRoI, k\\n2 position-sensitive scores are averaged to produce a\\nC + 1-d vector and softmax responses across categories are\\ncomputed. Another 4 k2-d conv layer is appended to obtain\\nclass-agnostic BBs.\\nWith R-FCN, more powerful classiﬁcation networks\\ncan be adopted to accomplish object detection in a fully\\nconvolutional architecture by sharing nearly all the layers,\\nand the state-of-the-art results are obtained on both PASCAL\\nVOC and Microsoft COCO [94] data sets at a test speed\\nof 170 ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in many\\nobject detection systems to improve scale invariance [24], [64]\\n[Fig. 7(a)]. However, training time and memory consumption\\nincrease rapidly. To this end, some techniques take only'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object detection systems to improve scale invariance [24], [64]\\n[Fig. 7(a)]. However, training time and memory consumption\\nincrease rapidly. To this end, some techniques take only\\na single input scale to represent high-level semantics and\\nincrease the robustness to scale changes [Fig. 7(b)], and image\\npyramids are built at test time which results in an inconsistency\\nbetween train/test-time infer ences [16], [17]. The in-network\\nfeature hierarchy in a deep ConvNet produces feature maps of\\ndifferent spatial resolutions while introduces large semantic\\ngaps caused by different depths [Fig. 7(c)]. To avoid using\\nlow-level features, pioneer works [71], [95] usually build the\\npyramid starting from middle layers or just sum transformed'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 7\\nFig. 7. Main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single-scale features are adopted for\\nfaster detection. (c) Alternative to the featurized image pyramid is to reuse\\nthe pyramidal feature hierarchy com puted by a ConvNet. (d) FPN integrates\\nboth (b) and (c). Blue outlines indicate feature maps and thicker outlines\\ndenote semantically stronger features.\\nfeature responses, missing the higher resolution maps of the\\nfeature hierarchy.\\nDifferent from these approaches, FPN [66] holds an archi-\\ntecture with a BU pathway, a top-down (TD) pathway and\\nseveral lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and semanti-\\ncally weak features [Fig. 7(d)]. The BU pathway, which is the'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='several lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and semanti-\\ncally weak features [Fig. 7(d)]. The BU pathway, which is the\\nbasic forward backbone ConvNet, produces a feature hierarchy\\nby downsampling the corresponding feature maps with a stride\\nof 2. The layers owning the same size of output maps are\\ngrouped into the same network stage and the output of the last\\nlayer of each stage is chosen as the reference set of feature\\nmaps to build the following TD pathway.\\nTo build the TD pathway, feature maps from higher network\\nstages are upsampled at ﬁrst and then enhanced with those of\\nthe same spatial size from the BU pathway via lateral connec-\\ntions. A 1 × 1 conv layer is appended to the upsampled map\\nto reduce channel dimensions and the mergence is achieved\\nby elementwise addition. Finally, a 3 × 3 convolution is also\\nappended to each merged map to reduce the aliasing effect'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to reduce channel dimensions and the mergence is achieved\\nby elementwise addition. Finally, a 3 × 3 convolution is also\\nappended to each merged map to reduce the aliasing effect\\nof upsampling and the ﬁnal feature map is generated. This\\nprocess is iterated until the ﬁnest resolution map is generated.\\nAs feature pyramid can extract rich semantics from all levels\\nand be trained end to end with all scales, the state-of-the-\\nart representation can be obtained without sacriﬁcing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g., region proposal generation) and to many\\nother computer vision tasks (e.g., instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ing task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nThe multitask scheme will create spurious edge and exhibit\\nsystematic errors on overlapping instances [98]. To solve this\\nproblem, parallel to the existing branches in Faster R-CNN\\nfor classiﬁcation and bounding box regression, the Mask R-\\nCNN [67] adds a branch to predict segmentation masks in a\\npixel-to-pixel manner (Fig. 8).\\nDifferent from the other two branches that are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m × m mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nFig. 8. Mask R-CNN framework fo r instance segmentation [67].\\nrepresentation requires fewer parameters but is more accurate\\nthan that in [97]. Formally, besides the two losses in (1) for'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Fig. 8. Mask R-CNN framework fo r instance segmentation [67].\\nrepresentation requires fewer parameters but is more accurate\\nthan that in [97]. Formally, besides the two losses in (1) for\\nclassiﬁcation and bounding box regression, an additional loss\\nfor segmentation mask branch is deﬁned to reach a multitask\\nloss. This loss is only associated with ground-truth class and\\nrelies on the classiﬁcation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classiﬁcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely, RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='adopts a simple and quantization-free layer, namely, RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch\\nonly adds a small computationa l burden and its cooperation\\nwith other tasks provides complementary information for\\nobject detection. As a result, Mask R-CNN is simple to\\nimplement with promising instance segmentation and object\\ndetection results. In a word, Mask R-CNN is a ﬂexible\\nand efﬁcient framework for instance-level recognition, which\\ncan be easily generalized to other tasks (e.g., human pose\\nestimation [7], [S4]) with minimal modiﬁcation.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and efﬁcient framework for instance-level recognition, which\\ncan be easily generalized to other tasks (e.g., human pose\\nestimation [7], [S4]) with minimal modiﬁcation.\\n8) Multitask Learning, Multiscale Representation, and Con-\\ntextual Modeling: Although the Faster R-CNN gets promising\\nresults with several hundred proposals, it still struggles in\\nsmall-size object detection and localization, mainly due to\\nthe coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO data set which consists\\nof objects at a broad range of scal es, less prototypical images,\\nand requires more precise loca lization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmultitask learning [100], multiscale representation [95], and\\ncontext modeling [101] to combine complementary informa-\\ntion from multiple sources.\\nMultitask learning learns a useful representation for mul-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='context modeling [101] to combine complementary informa-\\ntion from multiple sources.\\nMultitask learning learns a useful representation for mul-\\ntiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. [100] introduced conv features trained for\\nobject segmentation and “stuff” (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet). Dai et al. [97] presented multitask network\\ncascades of three networks, namely, class-agnostic region\\nproposal generation, pixel-level instance segmentation, and\\nregional instance classiﬁcation. Li et al. [104] incorporated the'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n8 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nweakly supervised object segmentation cues and region-based\\nobject detection into a multistage architecture to fully exploit\\nthe learned segmentation features.\\nMultiscale representation combines activations from\\nmultiple layers with skip-lay er connections to provide\\nsemantic information of different spatial resolutions [66].\\nCai et al. [105] proposed the multiscale CNN (MS-CNN)\\nto ease the inconsistency between the sizes of objects\\nand receptive ﬁelds with multiple scale-independent\\noutput layers. Yang et al. [34] investigated two strategies,\\nnamely, scale-dependent pooling (SDP) and layerwise\\ncascaded rejection classiﬁers (CRCs), to exploit appropriate\\nscale-dependent conv features. Kong et al. [101] proposed\\nthe HyperNet to calculate the shared features between RPN'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='cascaded rejection classiﬁers (CRCs), to exploit appropriate\\nscale-dependent conv features. Kong et al. [101] proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps fro m different resolutions into a\\nuniform space.\\nContextual modeling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. [106] proposed the SegDeepM to\\nexploit object segmentation which reduces the dependency\\non initial candidate boxes with the Markov random ﬁeld.\\nMoysset et al. [108] took advantage of four directional 2-D\\nlong short-term memories (LSTMs) [107] to convey global\\ncontext between different local regions and reduced trainable\\nparameters with local parameter sharing. Zeng et al. [109] pro-\\nposed a novel gated bidirectional-net (GBD-Net) by introduc-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='context between different local regions and reduced trainable\\nparameters with local parameter sharing. Zeng et al. [109] pro-\\nposed a novel gated bidirectional-net (GBD-Net) by introduc-\\ning gated functions to control message transmission between\\ndifferent support regions.\\nThe combination incorporates different components above\\ninto the same model to improve detection performance fur-\\nther. Gidaris and Komodakis [110] proposed the multire-\\ngion CNN (MR-CNN) model to capture different aspects of\\nan object, the distinct appear ances of various object parts,\\nand semantic segmentation-aware features. To obtain con-\\ntextual and multiscale representations, Bell et al. [95] pro-\\nposed the inside–outside net (ION) by exploiting informa-\\ntion both inside and outside the RoI with spatial recurrent\\nneural networks [111] and skip pooling [101]. Zagoruyko\\net al. [112] proposed the MultiPath architecture by introducing\\nthree modiﬁcations to the Fast R-CNN, including multiscale'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='neural networks [111] and skip pooling [101]. Zagoruyko\\net al. [112] proposed the MultiPath architecture by introducing\\nthree modiﬁcations to the Fast R-CNN, including multiscale\\nskip connections [95], a modiﬁed foveal structure [110], and\\na novel loss function summing different intersection over\\nunion (IoU) losses.\\n9) Thinking in Deep Learning-Based Object Detection:\\nApart from the above-mentioned approaches, there are still\\nmany important factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. [113] proposed an effective online mining\\nalgorithm (OHEM) for automatic selection of the hard exam-\\nples, which leads to a more eff ective and efﬁcient training.\\nInstead of concentrating on feature extraction, Ren et al.\\n[114] made a detailed analysis on object classiﬁers and found\\nthat it is of particular importance for object detection to con-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Instead of concentrating on feature extraction, Ren et al.\\n[114] made a detailed analysis on object classiﬁers and found\\nthat it is of particular importance for object detection to con-\\nstruct a deep and convolutional per-region classiﬁer carefully,\\nespecially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signiﬁcant scale variation, occlusion,\\nor truncation, especially when only 2-D object detection is\\ninvolved. To address this problem, Xiang et al. [60] proposed\\na novel subcategory-aware RPN, which guides the generation\\nof region proposals with subcategory information related to\\nobject poses and jointly optimize object detection and subcat-\\negory classiﬁcation.\\nOuyang et al. [115] found that the samples from differ-\\nent classes follow a long-tailed distribution, which indicates\\nthat different classes with distinct numbers of samples have\\ndifferent degrees of impacts on feature learning. To this end,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ent classes follow a long-tailed distribution, which indicates\\nthat different classes with distinct numbers of samples have\\ndifferent degrees of impacts on feature learning. To this end,\\nobjects are ﬁrst clustered into visually similar class groups,\\nand then, a hierarchical feature learning scheme is adopted to\\nlearn deep representations for each group separately.\\nIn order to minimize the computational cost and achieve\\nthe state-of-the-art performance, with the “deep and thin”\\ndesign principle and following the pipeline of Fast R-CNN,\\nHong et al. [116] proposed the architecture of PV ANET,\\nwhich adopts some building blocks including concatenated\\nReLU [117], Inception [45], and HyperNet [101] to reduce\\nthe expense on multiscale feature extraction and trains the\\nnetwork with BN [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 fps).'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='rate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 fps).\\nB. Regression/Classiﬁcation-Based Framework\\nRegion proposal-based frameworks are composed of several\\ncorrelated stages, including region proposal generation, feature\\nextraction with CNN, classiﬁcation, and bounding box regres-\\nsion, which are usually trained separately. Even in the recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different component s becomes the bottleneck in the\\nreal-time application.\\nOne-step frameworks based on global regression/\\nclassiﬁcation, mapping straightly from image pixels to\\nbounding\\nbox coordinates and cl ass probabilities, can reduce\\ntime expense. We ﬁrst review some pioneer CNN models\\nand then focus on two signiﬁcant frameworks, namely,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='bounding\\nbox coordinates and cl ass probabilities, can reduce\\ntime expense. We ﬁrst review some pioneer CNN models\\nand then focus on two signiﬁcant frameworks, namely,\\nYOLO [18] and SSD [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiﬁcation task.\\nSzegedy et al. [118] formulated the object detection task\\nas a DNN-based regression, generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has difﬁculty in handling\\noverlapping objects, and BBs generated by direct upsampling\\nis far from perfect.\\nPinheiro et al. [119] proposed a CNN model with two\\nbranches: one generates class agnostic segmentation masks\\nand the other predicts the likelihood of a given patch centered\\non an object. Inference is efﬁcient since class scores and\\nsegmentation can be obtained in a single model with most\\nof the CNN operations shared.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 9\\nFig. 9. Main idea of YOLO [18].\\nErhan et al. [68] and Szegedy et al. [120] proposed the\\nregression-based MultiBox to produce scored class-agnostic\\nregion proposals. A uniﬁed loss was introduced to bias both\\nlocalization and conﬁdences of multiple components to predict\\nthe coordinates of class-agnostic BBs. However, a large num-\\nber of additional parameters are introduced to the ﬁnal layer.\\nYoo et al. [69] adopted an iterative classiﬁcation approach\\nto handle object detection and proposed an impressive end-\\nto-end CNN architecture named AttentionNet. Starting from\\nthe top-left and bottom-right corners of an image, Attention-\\nNet points to a target object by generating quantized weak\\ndirections and converges to an accurate object boundary box'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the top-left and bottom-right corners of an image, Attention-\\nNet points to a target object by generating quantized weak\\ndirections and converges to an accurate object boundary box\\nwith an ensemble of iterative predictions. However, the model\\nbecomes quite inefﬁcient when handling multiple categories\\nwith a progressive two-step procedure.\\nNajibi et al. [70] proposed a proposal-free iterative\\ngrid-based object detector (G-CNN), which models object\\ndetection as ﬁnding a path from a ﬁxed grid to boxes tightly\\nsurrounding the objects [70]. Starting with a ﬁxed multiscale\\nbounding box grid, G-CNN trains a regressor to move and\\nscale elements of the grid toward objects iteratively. However,\\nG-CNN has a difﬁculty in dealing with small or highly\\noverlapping objects.\\n2) YOLO: Redmon et al. [18] proposed a novel framework\\ncalled YOLO, which makes the use of the whole topmost\\nfeature map to predict both conﬁdences for multiple categories\\nand BBs. The basic idea of YOLO is exhibited in Fig. 9.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='called YOLO, which makes the use of the whole topmost\\nfeature map to predict both conﬁdences for multiple categories\\nand BBs. The basic idea of YOLO is exhibited in Fig. 9.\\nYOLO divides the input image into an S × S grid and each\\ngrid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B BBs and their\\ncorresponding conﬁdence scores. Formally, conﬁdence scores\\nare deﬁned as Pr(Object) ∗IOU\\ntruth\\npred , which indicates how likely\\nthere exist objects (Pr (Object) ≥ 0) and shows conﬁdences\\nof its prediction (IOU truth\\npred\\n). At the same time, regardless\\nof the number of boxes, C conditional class probabilities\\n(Pr(Classi |Object)) should also be predicted in each grid cell.\\nIt should be noticed that only the contribution from the grid\\ncell containing an obj ect is calculated.\\nAt test time, class-speciﬁc conﬁdence scores for each box\\nare achieved by multiplying the individual box conﬁdence'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='cell containing an obj ect is calculated.\\nAt test time, class-speciﬁc conﬁdence scores for each box\\nare achieved by multiplying the individual box conﬁdence\\npredictions with the conditional class probabilities as follows :\\nPr(Object) ∗ IOU\\ntruth\\npred ∗ Pr(Classi|Object)\\n= Pr(Classi) ∗ IOUtruth\\npred (5)\\nwhere the existing probability of class-speciﬁc objects in the\\nbox and the ﬁtness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized :\\nλ\\ncoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nobj\\nij [(xi −ˆxi )2 + (yi −ˆyi )2]\\n+ λcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nobj\\nij\\n[ ( √\\nwi −\\n√\\nˆwi )2 + (\\n√\\nhi −\\n√\\nˆhi\\n) 2]\\n+\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nobj\\nij\\n(Ci − ˆCi )2\\n+ λnoobj\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nnoobj\\nij\\n(Ci − ˆCi )2\\n+\\nS2\\n∑\\ni=0\\n/BD\\nobj\\ni\\n∑\\nc∈classes\\n(pi (c) −ˆpi (c))2. (6)\\nIn a certain cell i, (xi , yi ) denote the center of the box relative\\nto the bounds of the grid cell,(wi ,hi ) are the normalized width'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='+\\nS2\\n∑\\ni=0\\n/BD\\nobj\\ni\\n∑\\nc∈classes\\n(pi (c) −ˆpi (c))2. (6)\\nIn a certain cell i, (xi , yi ) denote the center of the box relative\\nto the bounds of the grid cell,(wi ,hi ) are the normalized width\\nand height relative to the image size, Ci represents the conﬁ-\\ndence scores, /BD\\nobj\\ni indicates the exist ence of objects, and /BD\\nobj\\nij\\ndenotes that the prediction is conducted by the jth bounding\\nbox predictor. Note that only when an object is present in\\nthat grid cell, the loss function penalizes classiﬁcation errors.\\nSimilarly, when the predictor is “responsible” for the ground\\ntruth box (i.e., the highest IoU of any predictor in that grid cell\\nis achieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 × 1 reduction layers followed by 3 × 3c o n v\\nlayers. The network can process images in real time at 45 fps\\nand a simpliﬁed version Fast YOLO can reach 155 fps'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='modules with 1 × 1 reduction layers followed by 3 × 3c o n v\\nlayers. The network can process images in real time at 45 fps\\nand a simpliﬁed version Fast YOLO can reach 155 fps\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on the background,\\nwhich makes the cooperation with Fast R-CNN become pos-\\nsible. An improved version, YOLOv2, was later proposed\\nin [72], which adopts several impressive strategies, such as\\nBN, anchor boxes, dimension cluster, and multiscale training.\\n3) SSD: YOLO has a difﬁculty in dealing with small\\nobjects in groups, which is caused by strong spatial con-\\nstraints imposed on bounding box predictions [18]. Mean-\\nwhile, YOLO struggles to generalize to objects in new/unusual\\naspect ratios/conﬁgurations an d produces relatively coarse\\nfeatures due to multiple downsampling operations.\\nAiming at these problems, Liu et al. [71] proposed an SSD,\\nwhich was inspired by the anchors adopted in MultiBox [68],'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='features due to multiple downsampling operations.\\nAiming at these problems, Liu et al. [71] proposed an SSD,\\nwhich was inspired by the anchors adopted in MultiBox [68],\\nRPN [17], and multiscale representation [95]. Given a speciﬁc\\nfeature map, instead of ﬁxed grids adopted in YOLO, the SSD\\ntakes the advantage of a set of default anchor boxes with\\ndifferent aspect ratios and scales to discretize the output space\\nof BBs. To handle objects with various sizes, the network\\nfuses predictions from multiple feature maps with different\\nresolutions.\\nThe architecture of SSD is demonstrated in Fig. 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales\\nand aspect ratios and their associated conﬁdences. The net-\\nwork is trained with a weighted sum of localization loss'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nTABLE I\\nOVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES\\nFig. 10. Architecture of SSD 300 [71]. SSD adds se veral feature layers to the end of VGG16 backbone ne twork to predict the offsets to default anchor\\nboxes and their associated conﬁdences. Final detection r esults are obtained by conducting NMS on multiscale reﬁned BBs.\\n(e.g., Smooth L1) and conﬁdence loss (e.g., Softmax), which\\nis similar to (1). Final detection results are obtained by\\nconducting NMS on multiscale reﬁned BBs.\\nIntegrating with hard negative mining, data augmentation,\\nand a larger number of carefully chosen default anchors,\\nSSD signiﬁcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO while being three\\ntimes faster. The SSD300 (input image size is 300 × 300)'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SSD signiﬁcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO while being three\\ntimes faster. The SSD300 (input image size is 300 × 300)\\nruns at 59 fps, which is more accurate and efﬁcient than\\nYOLO. However, SSD is not skilled at dealing with small\\nobjects, which can be relieved by adopting better feature\\nextractor backbone (e.g., ResNet101), adding deconvolution\\nlayers with skip connections to introduce additional large-scale\\ncontext [73], and designing better network structure (e.g., stem\\nblock and dense block) [74].\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark data sets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121], and Microsoft COCO [94].\\nThe evaluated approaches include R-CNN [15], SPP-\\nnet [64], Fast R-CNN [16], networks on convolutional\\nfeature maps (NOC) [114], Bayes [85], MR-CNN&\\nS-CNN [105], Faster R-CNN [17], HyperNet [101], ION [95],\\nMS-GR [104], StuffNet [100], SSD300 [71], SSD512 [71],'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='feature maps (NOC) [114], Bayes [85], MR-CNN&\\nS-CNN [105], Faster R-CNN [17], HyperNet [101], ION [95],\\nMS-GR [104], StuffNet [100], SSD300 [71], SSD512 [71],\\nOHEM [113], SDP +CRC [34], G-CNN [70], SubCNN [60],\\nGBD-Net [109], PV ANET [116], YOLO [18], YOLOv2 [72],\\nR-FCN [65], FPN [66], Mask R-CNN [67], DSSD [73],\\nand DSOD [74]. If no speciﬁc instructions for the adopted\\nframework are provided, the utilized model is a VGG16 [46]\\npretrained on 1000-way ImageNet classiﬁcation task [39].\\nDue to the limitation of the paper length, we only provide an\\noverview, including proposal, learning method, loss function,\\nprograming language, and platform, of the prominent\\narchitectures in Table I. Deta iled experimental settings, which\\ncan be found in the original papers, are missed. In addition\\nto the comparisons of detectio n accuracy, another comparison\\nis provided to evaluate their test consumption on PASCAL\\nVOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to the comparisons of detectio n accuracy, another comparison\\nis provided to evaluate their test consumption on PASCAL\\nVOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 data sets consist of 20 categories. The evaluation terms\\nare AP in each single category and mAP across all the 20 cat-\\negories. Comparative results are exhibited in Tables II and III,\\nfrom which the following remarks can be obtained.\\n1) If incorporated with a proper way, more powerful back-\\nbone CNN models can deﬁnitely improve the object\\ndetection performance (the comparison among R-CNN\\nwith AlexNet, R-CNN with VGG16 and SPP-net with\\nZF-Net [122]).\\n2) With the introduction of the SPP layer (SPP-net), end-\\nto-end multitask architecture (FRCN), and RPN (Faster\\nR-CNN), object detection performance is improved\\ngradually and apparently.\\n3) Due to a large number of trainable parameters, in order\\nto obtain multilevel robust features, data augmentation\\nis very important for deep learning-based models (Faster'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='gradually and apparently.\\n3) Due to a large number of trainable parameters, in order\\nto obtain multilevel robust features, data augmentation\\nis very important for deep learning-based models (Faster\\nR-CNN with “07,” “07 + 12,” and “07 + 12 + coco”).\\n4) Apart from basic models, there are still many\\nother factors affecting object detection performance,\\nsuch as multiscale and multiregion feature extrac-\\ntion (e.g., MR-CNN), modiﬁed classiﬁcation networks\\n(e.g., NOC), additional information from other corre-\\nlated tasks (e.g., StuffNet, HyperNet), multiscale rep-\\nresentation (e.g., ION), and mining of hard negative\\nsamples (e.g., OHEM).\\n5) As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN, and ﬁne-grained features,\\nthe localization errors ar e corrected (YOLOv2).'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN, and ﬁne-grained features,\\nthe localization errors ar e corrected (YOLOv2).\\n6) By combining many recent tricks and modeling the\\nwhole network as a fully convolutional one, R-FCN'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 11\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 T EST SET (%)\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 T EST SET (%)\\nachieves a more obvious improvement of detection per-\\nformance over other approaches.\\n2) Microsoft COCO: Microsoft COCO is composed\\nof 300 000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requi rement on object localization,\\nthis data set is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are given in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='different degrees of IoUs and on different object sizes. The\\nresults are given in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n1) Multiscale training and test are beneﬁcial in improv-\\ning object detection perform ance, which provide addi-\\ntional information in different resolutions (R-FCN).\\nFPN and DSSD provide some better ways to build\\nfeature pyramids to achieve multiscale representation.\\nThe complementary information from other related tasks\\nis also helpful for accurate object localization (Mask\\nR-CNN with instance segmentation task).\\n2) Overall, region proposal-based methods, such as Faster\\nR-CNN and R-FCN, perform better than regression/\\nclassiﬁcation-based approaches, namely, YOLO and\\nSSD, due to the fact that quite a lot of localization\\nerrors are produced by regression/classiﬁcation-based\\napproaches.\\n3) Context modeling is helpful to locate small objects,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SSD, due to the fact that quite a lot of localization\\nerrors are produced by regression/classiﬁcation-based\\napproaches.\\n3) Context modeling is helpful to locate small objects,\\nwhich provides additional information by consult-\\ning nearby objects and surroundings (GBD-Net and\\nmultipath).\\n4) Due to the existence of a large number of nonstandard\\nsmall objects, the results on this data set are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g., ResNeXt [123]) and\\nuseful strategies (e.g., mu ltitask learning [67], [124]),\\nthe performance can be improved.\\n5) The success of DSOD in training from scratch stresses\\nthe importance of the network design to release the\\nrequirements for perfect pretrained classiﬁers on relevant\\ntasks and a large number of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA\\nTitan X GPU. Except for “SS” which is processed with CPU,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA\\nTitan X GPU. Except for “SS” which is processed with CPU,\\nthe other procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n1) By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test\\ntime is further reduced with the uniﬁed multitask learn-\\ning (FRCN) and removal of additional region proposal\\ngeneration stage (Faster R-CNN). It is also helpful to\\ncompress the parameters of FC layers with SVD [91]\\n(PA VNET and FRCN).\\n2) It takes additional test time to extract multiscale fea-\\ntures and contextual information (ION and MR-RCNN&\\nS-RCNN).\\n3) It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time\\nconsumption can be reduced by adding as many lay-\\ners into shared fully convolutional layers as possible\\n(FRCN).'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO T EST DEV SET (%)\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 T EST SET\\n4) Regression-based models can usually be processed\\nin real time at the cost of a drop in accuracy\\ncompared with region proposal-based models. Also,\\nregion proposal-based models can be modiﬁed into\\nreal-time systems with the introduction of other\\ntricks [116] (PV ANET), such as BN [43] and residual\\nconnections [123].\\nIV . S\\nALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous appli-\\ncations incorporate the visual saliency to improve their perfor-\\nmance, such as image cropping [125] and segmentation [126],'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='most dominant object regions in an image. Numerous appli-\\ncations incorporate the visual saliency to improve their perfor-\\nmance, such as image cropping [125] and segmentation [126],\\nimage retrieval [57], and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely, BU [127] and TD [128]. Local\\nfeature contrast plays the central role in BU salient object\\ndetection, regardless of the semantic contents of the scene.\\nTo learn local feature contrast , various local and global fea-\\ntures are extracted from pixels, e.g., edges [129] and spa-\\ntial information [130]. However, high-level and multiscale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low-contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is\\ntask-oriented and takes prior knowledge about object cate-\\ngories to guide the generation of salient maps. Taking semantic'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='salient objects are obtained. TD salient object detection is\\ntask-oriented and takes prior knowledge about object cate-\\ngories to guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep Learning in Salient Object Detection\\nDue to the signiﬁcance for providing high-level and mul-\\ntiscale feature representation a nd the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133], and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Vig et al. [29] follows a completely\\nautomatic data-driven approach to perform a large-scale search'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='salient object detection.\\nThe early work by Vig et al. [29] follows a completely\\nautomatic data-driven approach to perform a large-scale search\\nfor optimal features, namely, an ensemble of deep networks\\nwith different layers and parameters. To address the problem\\nof limited training data, Kummerer et al. [134] proposed the\\nDeep Gaze by transferring from the AlexNet to generate a\\nhigh-dimensional feature space and create a saliency map.\\nA similar architecture was proposed by Huang et al. [135] to\\nintegrate saliency prediction into pretrained object recognition\\nDNNs. The transfer is accomp lished by ﬁne-tuning DNNs’\\nweights with an objective function based on the saliency\\nevaluation metrics, such as similarity, KL-divergence, and\\nnormalized scanpath saliency.\\nSome works combined local and global visual\\nclues to improve salient object detection performance.\\nWang et al. [136] trained two independent deep CNNs\\n(DNN-L and DNN-G) to capture local information and global'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='clues to improve salient object detection performance.\\nWang et al. [136] trained two independent deep CNNs\\n(DNN-L and DNN-G) to capture local information and global\\ncontrast and predicted saliency maps by integrating both\\nlocal estimation and global search. Cholakkal et al. [137]\\nproposed a weakly supervised saliency detection framework\\nto combine visual saliency from BU and TD saliency'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 13\\nTABLE VI\\nCOMPARISON BETWEEN STATE-OF -THE -ART METHODS\\nmaps and reﬁned the results with a multiscale superpixel-\\naveraging. Zhao et al. [138] proposed a multicontext deep\\nlearning framework, which utilizes a uniﬁed learning\\nframework to model global and local context jointly with\\nthe aid of superpixel segmentation. To predict saliency in\\nvideos, Bak et al. [139] fused two static saliency models,\\nnamely, spatial stream net and temporal stream net, into a\\ntwo-stream framework with a novel empirically grounded\\ndata augmentation technique.\\nComplementary information from semantic segmentation\\nand context modeling is beneﬁcial . To learn internal represen-\\ntations of saliency efﬁciently, He et al. [140] proposed a novel\\nsuperpixelwise CNN approach called SuperCNN, in which'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and context modeling is beneﬁcial . To learn internal represen-\\ntations of saliency efﬁciently, He et al. [140] proposed a novel\\nsuperpixelwise CNN approach called SuperCNN, in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully CNN, Li et al. [141] proposed a\\nmultitask deep saliency model, in which intrinsic correlations\\nbetween saliency detection and semantic segmentation are set\\nup. However, due to the conv layers with large receptive\\nﬁelds and pooling layers, blurry object boundaries and coarse\\nsaliency maps are produced. Tang and Wu [142] proposed\\na novel saliency detection framework (CRPSD) [142], which\\ncombines the region-level saliency estimation and pixel-level\\nsaliency prediction together with three closely related CNNs.\\nLi and Yu [143]proposed a deep contrast network to combine\\nsegmentwise spatial pooling and pixel-level fully convolutional\\nstreams [143].\\nThe proper integration of multiscale feature maps is also'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Li and Yu [143]proposed a deep contrast network to combine\\nsegmentwise spatial pooling and pixel-level fully convolutional\\nstreams [143].\\nThe proper integration of multiscale feature maps is also\\nof signiﬁcance for improving de tection performance. Based\\non Fast R-CNN, Wang et al. [144] proposed the RegionNet\\nby performing salient object detection with end-to-end edge\\npreserving and multiscale contextual modeling. Liu et al. [28]\\nproposed a multiresolution CNN (Mr-CNN) to predict eye ﬁx-\\nations, which is achieved by learning both BU visual saliency\\nand TD visual factors from raw image data simultaneously.\\nCornia et al. [145] proposed an architecture that combines fea-\\ntures extracted at different levels of the CNN. Li and Yu [146]\\nproposed a multiscale deep CNN framework to extract three\\nscales of deep contrast featur es, namely, the mean-subtracted\\nregion, the bounding box of its immediate neighboring regions,\\nand the masked entire image, from each candidate region.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scales of deep contrast featur es, namely, the mean-subtracted\\nregion, the bounding box of its immediate neighboring regions,\\nand the masked entire image, from each candidate region.\\nIt is efﬁcient and accurate to train a direct pixelwise\\nCNN architecture to predict salient objects with the aids\\nof recurrent neural networks and deconvolution networks.\\nPan et al. [147] formulated saliency prediction as a mini-\\nmization optimization on the Euclidean distance between the\\npredicted saliency map and the ground truth and proposed\\ntwo kinds of architectures: a shallow one trained from scratch\\nand a deeper one adapted fro m a deconvoluted VGG net-\\nwork. Asconvolutional–deconvolution networks are not expert\\nin recognizing objects of multiple scales, Kuen et al. [148]\\nproposed a recurrent attentional convolutional–deconvolution\\nnetwork with several spatial transformer and recurrent network\\nunits to conquer this problem. To fuse local, global, and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='proposed a recurrent attentional convolutional–deconvolution\\nnetwork with several spatial transformer and recurrent network\\nunits to conquer this problem. To fuse local, global, and\\ncontextual information of salient objects, Tang et al. [149]\\ndeveloped a deeply supervised recurrent CNN to perform a\\nfull image-to-image saliency detection.\\nB. Experimental Evaluation\\nFour representative data sets, including Evaluation on Com-\\nplex Scene Saliency Dataset (ECSSD) [156], HKU-IS [146],\\nPASCALS [157], and SOD [158], are used to evaluate several\\nstate-of-the-art methods. ECSSD consists of 1000 structurally\\ncomplex but semantically meaningful natural images. HKU-IS\\nis a large-scale data set containing over 4000 challenging\\nimages. Most of these images have more than one salient\\nobject and own low contrast. PASCALS is a subset chosen\\nfrom the validation set of PASCAL VOC 2010 segmentation\\ndata set and is composed of 850 natural images. The SOD data'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object and own low contrast. PASCALS is a subset chosen\\nfrom the validation set of PASCAL VOC 2010 segmentation\\ndata set and is composed of 850 natural images. The SOD data\\nset possesses 300 images containing multiple salient objects.\\nThe training and validation sets f or different data sets are kept\\nthe same as those in [152].\\nTwo standard metrics, namely, F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values precomputed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is deﬁned as follows:\\nF\\nβ = (1 + β2)Presion × Recall\\nβ2Presion + Recall (7)\\nwhere β2 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation :\\nMAE = 1\\nH × W\\nH∑\\ni=1\\nW∑\\nj=1\\n| ˆS(i, j) = ˆZ(i, j)| (8)\\nwhere ˆZ and ˆS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MAE = 1\\nH × W\\nH∑\\ni=1\\nW∑\\nj=1\\n| ˆS(i, j) = ˆZ(i, j)| (8)\\nwhere ˆZ and ˆS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected nonsalient pixels [159].\\nThe following approaches are evaluated: contextual hyper-\\ngraph modeling (CHM) [150], RC [151], discriminative'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n14 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nregional feature integration (DRFI) [152], MC [138], mul-\\ntiscale deep CNN features (MDF) [146], local estimation\\nand global search (LEGS) [136], DSR [149], multi-task deep\\nneural network [141], CRPSD [142], deep contrast learn-\\ning (DCL) [143], encoded low level distance (ELD) [153],\\nnonlocal deep features (NLDF) [154], and deep supervision\\nwith short connections (DSSC) [155]. Among these meth-\\nods, CHM, RC, and DRFI are classical ones with the best\\nperformance [159], while the other methods are all associated\\nwith CNN. F-measure and MAE scores are given in Table VI.\\nFrom Table VI, we can ﬁnd that CNN-based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a more'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='From Table VI, we can ﬁnd that CNN-based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a more\\naccurate saliency. ELD refers to low-level handcrafted features\\nfor complementary information. LEGS adopts generic region\\nproposals to provide initial salient regions, which may be\\ninsufﬁcient for salient detection. DSR and MT act in different\\nways by introducing a recurrent network and semantic seg-\\nmentation, which provide insights for future improvements.\\nCRPSD, DCL, NLDF, and DSSC are all based on multiscale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF,\\nand DSSC perform the best on these four data sets. DSSC\\nearns the best performance by modeling scale-to-scale short\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of the CNN-based methods need to model'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='earns the best performance by modeling scale-to-scale short\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of the CNN-based methods need to model\\nvisual saliency along region boundaries with the aid of super-\\npixel segmentation. Meanwhile, the extraction of multiscale\\ndeep CNN features is of signiﬁcance for measuring local\\nconspicuity. Finally, it is necessary to strengthen local con-\\nnections between different CNN layers as well as to utilize\\ncomplementary information from local and global context.\\nV. F\\nACE DETECTION\\nFace detection is essential to many face applications\\nand acts as an important preprocessing procedure to\\nface recognition [160]–[162], f ace synthesis [163], [164], and\\nfacial expression analysis [165]. Different from generic object\\ndetection, this task is to recognize and locate face regions\\ncovering a very large range of scales (30–300 pts versus\\n10–1000 pts). At the same time, faces have their unique object'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection, this task is to recognize and locate face regions\\ncovering a very large range of scales (30–300 pts versus\\n10–1000 pts). At the same time, faces have their unique object\\nstructural conﬁgurations (e.g., the distribution of different\\nface parts) and characteristics (e.g., skin color). All these\\ndifferences lead to special attention to this task. However, large\\nvisual variations of faces, such a s occlusions, pose variations,\\nand illumination changes, impose great challenges for this task\\nin real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiﬁers with Haar-like features\\nand AdaBoost, achieving good performance with real-time\\nefﬁciency. However, this detector may degrade signiﬁcantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. [24] proposed a deformable part model (DPM)\\nfor face detection. However, for these traditional face detection'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='human faces. Different from this cascade structure, Felzen-\\nszwalb et al. [24] proposed a deformable part model (DPM)\\nfor face detection. However, for these traditional face detection\\nmethods, high computational e xpenses and lar ge quantities\\nof annotations are required to achieve a reasonable result.\\nFig. 11. ROC curves of state-of-the-a rt methods on FDDB. (a) Discrete ROC\\ncurves. (b) Continuous ROC curves.\\nIn addition, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep Learning in Face Detection\\nRecently, some CNN-based face detection approaches have\\nbeen proposed [167]–[169]. As less accurate localization\\nresults from independent regressions of object coordinates,\\nYu et al. [167] proposed a novel IoU loss function for pre-\\ndicting the four bounds of box jointly. Farfade et al. [168]\\nproposed a deep dense face detector (DDFD) to conduct\\nmultiview face detection, which is able to detect faces in'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='dicting the four bounds of box jointly. Farfade et al. [168]\\nproposed a deep dense face detector (DDFD) to conduct\\nmultiview face detection, which is able to detect faces in\\na wide range of orientations without the requirement of\\npose/landmark annotations. Yang et al. [169] proposed a novel\\ndeep learning-based face detec tion framework, which collects\\nthe responses from local facial parts (e.g., eyes, nose, and\\nmouths) to address face detection under severe occlusions\\nand unconstrained pose variations. Yang et al. [170] proposed\\na scale-friendly detection network named ScaleFace, which\\nsplits a large range of target scales into smaller subranges.\\nDifferent specialized subnetworks are constructed on these\\nsubscales and combined into a single one to conduct end-to-\\nend optimization. Hao et al. [171] designed an efﬁcient CNN\\nto predict the scale distribution histogram of the faces and took\\nthis histogram to guide the zoomed-in view and zoomed-out'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='end optimization. Hao et al. [171] designed an efﬁcient CNN\\nto predict the scale distribution histogram of the faces and took\\nthis histogram to guide the zoomed-in view and zoomed-out\\nview of the image. Since the faces are approximately in\\nuniform scale after zoom, compared with other state-of-the-art\\nbaselines, better performance is achieved with a less computa-\\ntion cost. In addition, some generic detection frameworks are\\nextended to face detection with different modiﬁcations, e.g.,\\nFaster R-CNN [30], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3-D modeling and face landmarks, in a multitask\\nlearning manner. Huang et al. [174] proposed a uniﬁed end-to-\\nend FCN framework called DenseBox to jointly conduct face\\ndetection and landmark localization. Li et al. [175] proposed\\na multitask discriminative learning framework that integrates\\na ConvNet with a ﬁxed 3-D mean face model in an end-to-end'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 15\\nmanner. In the framework, two issues are addressed to trans-\\nfer from generic object detection to face detection, namely,\\neliminating predeﬁned anchor boxes by a 3-D mean face\\nmodel and replacing RoI pooling layer with a conﬁguration\\npooling layer. Zhang et al. [176] proposed a deep cascaded\\nmultitask framework named multitask cascaded convolutional\\nnetworks (MTCNN) which exploits the inherent correlations\\nbetween face detection and alignment in the unconstrained\\nenvironment to boost up detection performance in a coarse-\\nto-ﬁne manner.\\nReducing computational expenses is of necessity in real\\napplications. To achieve real-time detection on the mobile plat-\\nform, Kalinovskii and Spitsyn [177] proposed a new solution\\nof frontal face detection based on compact CNN cascades. This'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='applications. To achieve real-time detection on the mobile plat-\\nform, Kalinovskii and Spitsyn [177] proposed a new solution\\nof frontal face detection based on compact CNN cascades. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify, and reﬁne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al. [32]\\nproposed a cascaded CNN denoted by supervised transformer\\nnetwork. This network takes a multitask RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang and Nevatia [8] proposed a\\nthree-stage cascade structu re based on FCNs, while in each\\nstage, a multiscale FCN is utilized to reﬁne the positions of\\npossible faces. Qin et al. [178] proposed a uniﬁed framework\\nthat achieves better results with the complementary informa-\\ntion from different jointly trained CNNs.\\nB. Experimental Evaluation'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='possible faces. Qin et al. [178] proposed a uniﬁed framework\\nthat achieves better results with the complementary informa-\\ntion from different jointly trained CNNs.\\nB. Experimental Evaluation\\nThe FDDB [179] data set has a total of 2845 pictures in\\nwhich 5171 faces are annotated with an elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the\\nreceiver operating characteristic (ROC) curve for the discrete\\nscores can reﬂect the dependence of the detected face fractions\\non the number of false alarms. Compared with annotations,\\nany detection with an IoU ratio exceeding 0.5 is treated as\\npositive. Each annotation is only associated with one detection.\\nThe ROC curve for the continuous scores is the reﬂection of\\nface localization quality.\\nThe evaluated models cover DDFD [168], Cascade-CNN\\n[180], aggregate channel features (ACF)-multiscale [181],\\nPico [182], Head-Hunter [183], Joint Cascade [31], SURF-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='face localization quality.\\nThe evaluated models cover DDFD [168], Cascade-CNN\\n[180], aggregate channel features (ACF)-multiscale [181],\\nPico [182], Head-Hunter [183], Joint Cascade [31], SURF-\\nmultiview [184], Viola–Jones [166], NPDFace [185],\\nFaceness [169], convolutional channel features (CCF) [186],\\nMTCNN [176], Conv3-D [175], Hyperface [187],\\nUnitBox [167], locally decorrelated channel\\nfeatures (LDCF +) [S2], DeepIR [173], hybrid-resolution\\nmodel with elliptical regressor (HR-ER) [188], Face-R-\\nCNN [172], and ScaleFace [170]. ACF-multiscale, Pico,\\nHeadHunter, Joint Cascade, SURF-multiview, Viola-Jones,\\nNPDFace, and LDCF + are built on classic hand-crafted\\nfeatures while the rest methods are based on deep CNN\\nfeatures. The ROC curves are shown in Fig. 11.\\nIn Fig. 11(a), in spite of relatively competitive results pro-\\nduced by LDCF +, it can be observed that most of the classic\\nmethods perform with similar results and are outperformed by'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In Fig. 11(a), in spite of relatively competitive results pro-\\nduced by LDCF +, it can be observed that most of the classic\\nmethods perform with similar results and are outperformed by\\nCNN-based methods by a signiﬁcant margin. In Fig. 11(b),\\nit can be observed that most of the CNN-based methods earn\\nsimilar true positive rates between 60% and 70% while DeepIR\\nand HR-ER perform much better than them. Among classic\\nmethods, Joint Cascade is still competitive. As earlier works,\\nDDFD and CCF directly make use of generated feature maps\\nand obtain relatively poor results. CascadeCNN builds cas-\\ncaded CNNs to locate face regions, which is efﬁcient but inac-\\ncurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3-D, and Hyperface proves the effectiveness of multitask\\nlearning. HR-ER and ScaleFace adaptively detect faces of'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='time-consuming. The outstanding performance of MTCNN,\\nConv3-D, and Hyperface proves the effectiveness of multitask\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales and make a balance between accuracy and\\nefﬁciency. DeepIR and Face-R-CNN are two extensions of the\\nFaster R-CNN architecture to face detection, which validate\\nthe signiﬁcance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN-based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cas cades, adapting scale-aware\\ndetection, and learning multitask shared CNN features.\\nVI. P\\nEDESTRIAN DETECTION\\nRecently, pedestrian detec tion has been intensively\\nstudied, which has a close relationship to pedestrian'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection, and learning multitask shared CNN features.\\nVI. P\\nEDESTRIAN DETECTION\\nRecently, pedestrian detec tion has been intensively\\nstudied, which has a close relationship to pedestrian\\ntracking [189], [190], person reidentiﬁcation [191], [192],\\nand robot navigation [193], [194]. Prior to the recent\\nprogress in deep CNN (DCNN)-based methods [195], [196],\\nsome researchers combined boosted decision forests\\nwith hand-crafted features to obtain pedestrian\\ndetectors [197]–[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes in typ-\\nical scenarios of pedestrian det ection (e.g., automatic driving\\nand intelligent surveillance), t he application of RoI pooling\\nlayer in generic object detection pipeline may result in “plain”\\nfeatures due to collapsing bins. In the meantime, the main'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and intelligent surveillance), t he application of RoI pooling\\nlayer in generic object detection pipeline may result in “plain”\\nfeatures due to collapsing bins. In the meantime, the main\\nsource of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different conﬁgurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep Learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature-\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some studies have been conducted to analyze\\nthe reasons. Zhang et al. [203] attempted to adapt generic'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='information and occlusion handling are incorporated [202].\\nThereby, some studies have been conducted to analyze\\nthe reasons. Zhang et al. [203] attempted to adapt generic\\nFaster R-CNN [17] to pedestrian detection. They modiﬁed the\\ndownstream classiﬁer by adding boosted forests to shared,\\nhigh-resolution conv feature maps and taking an RPN to han-\\ndle small instances and hard neg ative examples. To deal with'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n16 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\ncomplex occlusions existing in pedestrian images, inspired\\nby DPM [24], Tian et al. [204] proposed a deep learning\\nframework called DeepParts, which makes decisions based\\non an ensemble of extensive part detectors. DeepParts has\\nadvantages in dealing with weakly labeled data, low IoU\\npositive proposals, and partial occlusion.\\nOther researchers also tried to combine complementary\\ninformation from multiple data sources. CompACT-Deep\\nadopts a complexity-aware cascade to combine hand-crafted\\nfeatures and ﬁne-tuned DCNNs [195]. Based on Faster\\nR-CNN, Liu et al. [205] proposed multispectral DNNs for\\npedestrian detection to combine complementary information\\nfrom color and thermal images. Tian et al. [206] proposed\\na task-assistant CNN to jointly learn multiple tasks with'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='pedestrian detection to combine complementary information\\nfrom color and thermal images. Tian et al. [206] proposed\\na task-assistant CNN to jointly learn multiple tasks with\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. [207] pro-\\nposed a DNN fusion architecture for fast and robust pedes-\\ntrian detection. Based on the candidate BBs generated with\\nSSD detectors [71], multiple binary classiﬁers are processed\\nparallelly to conduct soft-rejection-based network fusion by\\nconsulting their aggregated degree of conﬁdences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel, and a large VGG16 model [195]. DeepParts contains\\n45 ﬁne-tuned DCNN models, and a set of strategies, includ-\\ning bounding box shifting handling and part selection, are\\nrequired to arrive at the reported results [204]. Therefore,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='45 ﬁne-tuned DCNN models, and a set of strategies, includ-\\ning bounding box shifting handling and part selection, are\\nrequired to arrive at the reported results [204]. Therefore,\\nthe modiﬁcation and simpliﬁcation are of signiﬁcance to\\nreduce the burden on both software and hardware to satisfy\\nreal-time detection demand. Tome et al. [59] proposed a novel\\nsolution to adapt generic object detection pipeline to pedestrian\\ndetection by optimizing most of its stages. Hu et al. [208]\\ntrained an ensemble of boosted decision models by reusing\\nthe conv feature maps, and a further improvement was gained\\nwith simple pixel labeling and additional complementary\\nhand-crafted features. Tome et al. [209] proposed a reduced\\nmemory region-based deep CNN architecture, which fuses\\nregional responses from both ACF detectors and SVM classi-\\nﬁers into R-CNN. Ribeiro et al. [33] addressed the problem of\\nhuman-aware navigation and proposed a vision-based person'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='regional responses from both ACF detectors and SVM classi-\\nﬁers into R-CNN. Ribeiro et al. [33] addressed the problem of\\nhuman-aware navigation and proposed a vision-based person\\ntracking system guided by multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian data set [3]. The data set was collected from the\\nvideos of a vehicle driving through an urban environment and\\nconsists of 250 000 frames with about 2300 unique pedestrians\\nand 350 000 annotated BBs. Three kinds of labels, namely,\\n“Person (clear identiﬁcations), ” “Person? (unclear identiﬁca-\\ntions),” and “People (large group of individuals),” are assigned\\nto different BBs. The perform ance is measured with the\\nlog-average miss rate (L-AMR) which is computed evenly\\nspaced in log-space in the range 10\\n−2 to 1 by averaging miss\\nrate at the rate of nine false positives per image [3]. According\\nto the differences in the height and visible part of the BBs,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='spaced in log-space in the range 10\\n−2 to 1 by averaging miss\\nrate at the rate of nine false positives per image [3]. According\\nto the differences in the height and visible part of the BBs,\\na total of nine popular settings are adopted to evaluate different\\nproperties of these models. Details of these settings are as\\nin [3].\\nEvaluated methods include Checkerboards + [198],\\nLDCF++ [S2], SCF +AlexNet [210], SA-FastRCNN [211],\\nMS-CNN [105], DeepParts [204], CompACT-Deep [195],\\nRPN+BF [203], and F-DNN +SS [207]. The ﬁrst two\\nmethods are based on hand-crafted features while the rest\\nones rely on deep CNN features. All results are exhibited\\nin Table VII. From this table, we observe that different\\nfrom other tasks, classic handcrafted features can still earn\\ncompetitive results with boosted decision forests [203],\\nACF [197], and HOG +LUV channels [S2]. As an early\\nattempt to adapt CNN to pedestrian detection, the features\\ngenerated by SCF +AlexNet are not so discriminant and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ACF [197], and HOG +LUV channels [S2]. As an early\\nattempt to adapt CNN to pedestrian detection, the features\\ngenerated by SCF +AlexNet are not so discriminant and\\nproduce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely, local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due\\nto complexity, it is too time-consuming to achieve real-time\\ndetection. The multiscale representation of MS-CNN improves\\nthe accuracy of pedestrian locations. SA-FastRCNN extends\\nFast R-CNN to automatically detect pedestrians according\\nto their different scales, which has trouble when there are\\npartial occlusions. RPN+BF combines the detectors produced\\nby Faster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN +SS, which is composed\\nof multiple parallel classiﬁers with soft rejections, performs'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='by Faster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN +SS, which is composed\\nof multiple parallel classiﬁers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN, and MS-CNN.\\nIn short, CNN-based methods can provide more accu-\\nrate candidate boxes and multilevel semantic information for\\nidentifying and loca ting pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classiﬁers, extracting multiscale and part-based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. P\\nROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor the future work.\\nThe ﬁrst one is small object detection such as occurring'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor the future work.\\nThe ﬁrst one is small object detection such as occurring\\nin COCO data set and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n1) Multitask Joint Optim ization and Multimodal\\nInformation Fusion: Due to the correlations between\\ndifferent tasks within and outside object detection,\\nmultitask joint optimization has already been studied\\nby many researchers [16], [17]. However, apart from\\nthe tasks mentioned in Section III-A8, it is desirable to\\nthink over the characteristics of different subtasks of\\nobject detection (e.g., superpixel semantic segmentation\\nin salient object detection) and extend multitask\\noptimization to other applications such as instance\\nsegmentation [66], multiobject tracking [202], and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in salient object detection) and extend multitask\\noptimization to other applications such as instance\\nsegmentation [66], multiobject tracking [202], and\\nmultiperson pose estimation [S4]. In addition, given'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 17\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF STATE-OF -THE -ART MODELS ON CALTECH PEDESTRIAN DATA SET.\\nALL NUMBERS ARE REPORTED IN L-AMR\\na speciﬁc application, the information from different\\nmodalities, such as text [212] , thermal data [205], and\\nimages [65], can be fused together to achieve a more\\ndiscriminant network.\\n2) Scale Adaption: Objects usually exist in different scales,\\nwhich are more apparent in face detection and pedes-\\ntrian detection. To increase the robustness to scale\\nchanges, it is demanded to train scale-invariant, mul-\\ntiscale or scale-adaptive det ectors. For scale-invariant\\ndetectors, more powerful backbone architectures (e.g.,\\nResNext [123]), negative sample mining [113], reverse\\nconnection [213], and subcategory modeling [60] are all'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detectors, more powerful backbone architectures (e.g.,\\nResNext [123]), negative sample mining [113], reverse\\nconnection [213], and subcategory modeling [60] are all\\nbeneﬁcial. For multiscale detectors, both the FPN [66]\\nthat produces multiscale feature maps and the generative\\nadversarial network [214] that narrows representation\\ndifferences between small objects and the large ones\\nwith a low-cost architecture provide insights into gen-\\nerating meaningful feature p yramid. For scale-adaptive\\ndetectors, it is useful to combine knowledge graph [215],\\nattentional mechanism [216], cascade network [180],\\nand scale distribution estimation [171] to detect objects\\nadaptively.\\n3) Spatial Correlations and Contextual Modeling: Spatial\\ndistribution plays an important role in object detec-\\ntion. Therefore, region proposal generation and grid\\nregression are taken to obtain probable object loca-\\ntions. However, the correlations between multiple pro-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tion. Therefore, region proposal generation and grid\\nregression are taken to obtain probable object loca-\\ntions. However, the correlations between multiple pro-\\nposals and object categorie s are ignored. In addition,\\nthe global structure information is abandoned by the\\nposition-sensitive score maps in R-FCN. To solve these\\nproblems, we can refer to diverse subset selection [217]\\nand sequential reasoning tasks [218] for possible solu-\\ntions. It is also meaningful to mask salient parts and\\ncouple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor\\nand accomplish real-time object detection, with the emergence\\nof the large-scale image and video data. The following three\\naspects can be taken into account.\\n1) Cascade Network: In a cascade network, a cas-\\ncade of detectors is built in different stages or\\nlayers [180], [220]. Easily distinguishable examples are\\nrejected at shallow layers so that features and classiﬁers'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='cade of detectors is built in different stages or\\nlayers [180], [220]. Easily distinguishable examples are\\nrejected at shallow layers so that features and classiﬁers\\nat later stages can handle more difﬁcult samples with\\nthe aid of the decisions from previous stages. However,\\ncurrent cascades are built in a greedy manner, where\\nprevious stages in cascade are ﬁxed when training a new\\nstage. Therefore, the optim izations of different CNNs\\nare isolated, which stresses the necessity of end-to-end\\noptimization for CNN cascade. At the same time, it is\\nalso a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n2) Unsupervised and Weakly Supervised Learning: It is\\nvery time-consuming to manually draw large quantities\\nof BBs. To release this burden, semantic prior [55],\\nunsupervised object discovery [221], multiple instance\\nlearning [222], and DNN prediction [47] can be inte-\\ngrated to make the best use of image-level supervision'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='unsupervised object discovery [221], multiple instance\\nlearning [222], and DNN prediction [47] can be inte-\\ngrated to make the best use of image-level supervision\\nto assign object category tags to corresponding object\\nregions and reﬁne object boundaries. Furthermore,\\nweakly annotations (e.g., center-click annotations [223])\\nare also helpful for achieving high-quality detectors with\\nmodest annotation efforts, especially aided by the mobile\\nplatform.\\n3) Network Optimization: Given speciﬁc applications and\\nplatforms, it is signiﬁcant to make a balance among\\nspeed, memory, and accuracy by selecting an optimal\\ndetection architecture [116], [224]. However, despite\\nthat detection accuracy is reduced, it is more mean-\\ningful to learn compact models with a fewer number\\nof parameters [209]. This situation can be relieved by\\nintroducing better pretraining schemes [225], knowledge\\ndistillation [226], and hint learning [227]. DSOD also\\nprovides a promising guideline to train from scratch'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='introducing better pretraining schemes [225], knowledge\\ndistillation [226], and hint learning [227]. DSOD also\\nprovides a promising guideline to train from scratch\\nto bridge the gap between different image sources and\\ntasks [74].\\nThe third one is to extend typical methods for\\n2-D object detection to adapt 3-D object detection\\nand video object detection, with the requirements from\\nautonomous driving, intelligent t ransportation, and intelligent\\nsurveillance.\\n1) 3-D Object Detection: With the applications of 3-D\\nsensors (e.g., Light Detection and Ranging and cam-\\nera), additional depth information can be utilized to\\nbetter understand the images in 2-D and extend the\\nimage-level knowledge to the real world. However, sel-\\ndom of these 3-D-aware techniques aim to place correct\\n3-D BBs around detected objects. To achieve better\\nbounding results, multiview representation [181] and\\n3-D proposal network [228] may provide some guide-\\nlines to encode depth information with the aid of inertial'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='bounding results, multiview representation [181] and\\n3-D proposal network [228] may provide some guide-\\nlines to encode depth information with the aid of inertial\\nsensors (accelerometer and gyrometer) [229].'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n18 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\n2) Video Object Detection: Temporal information across\\ndifferent frames plays an important role in under-\\nstanding the behaviors of different objects. However,\\nthe accuracy suffers from degenerated object appear-\\nances (e.g., motion blur and video defocus) in videos\\nand the network is usually not trained end to end. To this\\nend, spatiotemporal tubelets [230], optical ﬂow [199],\\nand LSTM [107] should be considered to fundamentally\\nmodel object associations between consecutive frames.\\nVIII. C\\nONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation, and background\\nswitches, deep learning-based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='dealing with occlusion, scale transformation, and background\\nswitches, deep learning-based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning-based object detection frameworks\\nthat handle different subproblems, such as occlusion, clutter,\\nand low resolution, with different degrees of modiﬁcations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely, salient object\\ndetection, face detection, and pedestrian detection, are also\\nbrieﬂy reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object detec-\\ntion landscape. This review is also meaningful for the develop-\\nments in neural networks and related learning systems, which\\nprovides valuable insights and guidelines for future progress.\\nR\\nEFERENCES'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tion landscape. This review is also meaningful for the develop-\\nments in neural networks and related learning systems, which\\nprovides valuable insights and guidelines for future progress.\\nR\\nEFERENCES\\n[1] P. F. Felzenszwalb et al. , “Object detection with discriminatively\\ntrained part-based models,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 32, no. 9, pp. 1627–1645, Sep. 2010.\\n[2] K.-K. Sung and T. Poggio, “Example-based learning for view-based\\nhuman face detection,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39–51, Jan. 1998.\\n[3] C. Wojek et al. , “Pedestrian detection: An evaluation of the state\\nof the art,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 34, no. 4,\\npp. 743–761, Apr. 2012.\\n[4] H. Kobatake and Y . Yoshinaga, “Detection of spicules on mammogram\\nbased on skeleton analysis,” I E E ET r a n s .M e d .I m a g ., vol. 15, no. 3,\\npp. 235–245, Jun. 1996.\\n[5] Y . Jia et al., “Caffe: Convolutional architecture for fast feature embed-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='based on skeleton analysis,” I E E ET r a n s .M e d .I m a g ., vol. 15, no. 3,\\npp. 235–245, Jun. 1996.\\n[5] Y . Jia et al., “Caffe: Convolutional architecture for fast feature embed-\\nding,” in Proc. ACM MM, 2014, pp. 675–678.\\n[6] A. Krizhevsky et al., “ImageNet classiﬁcation with deep convolutional\\nneural networks,” in Proc. NIPS, 2012, pp. 1097–1105.\\n[7] Z. Cao et al. , “Realtime multi-person 2D pose estimation using part\\nafﬁnity ﬁelds,” in Proc. CVPR, 2017, pp. 1302–1310.\\n[8] Z. Yang and R. Nevatia, “A multi-scale cascade fully convolutional\\nnetwork face detector,” in Proc. ICPR, 2016, pp. 633–638.\\n[9] C. Chen et al., “DeepDriving: Learning affordance for direct perception\\nin autonomous driving,” in Proc. ICCV, 2015, pp. 2722–2730.\\n[10] X. Chen et al. , “Multi-view 3D object detection network for\\nautonomous driving,” in Proc. CVPR, 2017, pp. 6526–6534.\\n[11] A. Dundar et al., “Embedded streaming deep neural networks acceler-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[10] X. Chen et al. , “Multi-view 3D object detection network for\\nautonomous driving,” in Proc. CVPR, 2017, pp. 6526–6534.\\n[11] A. Dundar et al., “Embedded streaming deep neural networks acceler-\\nator with applications,” IEEE Trans. Neural Netw. Learn. Syst., vol. 28,\\nno. 7, pp. 1572–1583, Jul. 2017.\\n[12] R. J. Cintra et al., “Low-complexity approximate convolutional neural\\nnetworks,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 29, no. 12,\\npp. 5981–5992, 2018.\\n[13] S. H. Khan et al., “Cost-sensitive learning of deep feature representa-\\ntions from imbalanced data,” IEEE Trans. Neural Netw. Learn. Syst. ,\\nvol. 29, no. 8, pp. 3573–3587, Aug. 2018.\\n[14] A. Stuhlsatz et al. , “Feature extraction with deep neural networks by\\na generalized discriminant analysis,” IEEE Trans. Neural Netw. Learn.\\nSyst., vol. 23, no. 4, pp. 596–608, Apr. 2012.\\n[15] R. Girshick et al. , “Rich feature hierarchies for accurate object\\ndetection and semantic segmentation,” in Proc. CVPR, 2014,\\npp. 580–587.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Syst., vol. 23, no. 4, pp. 596–608, Apr. 2012.\\n[15] R. Girshick et al. , “Rich feature hierarchies for accurate object\\ndetection and semantic segmentation,” in Proc. CVPR, 2014,\\npp. 580–587.\\n[16] R. Girshick, “Fast R-CNN,” in Proc. ICCV, 2015, pp. 1440–1448.\\n[17] S. Ren et al., “Faster R-CNN: Towards real-time object detection with\\nregion proposal networks,” in Proc. NIPS, 2015, pp. 91–99.\\n[18] J. Redmon et al. , “You only look once: Uniﬁed, real-time object\\ndetection,” in Proc. CVPR, 2016, pp. 779–788.\\n[19] D. G. Lowe, “Distinctive image features from scale-invariant key-\\npoints,” Int. J. Comput. Vis. , vol. 60, no. 2, pp. 91–110, 2004.\\n[20] N. Dalal and B. Triggs, “Histogr ams of oriented gradients for human\\ndetection,” in Proc. CVPR, 2005, pp. 886–893.\\n[21] R. Lienhart and J. Maydt, “An extended set of Haar-like features for\\nrapid object detection,” in Proc. ICIP, 2002, p. 1.\\n[22] C. Cortes and V . Vapnik, “Support vector machine,” Mach. Learn. ,\\nvol. 20, no. 3, pp. 273–297, 1995.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='rapid object detection,” in Proc. ICIP, 2002, p. 1.\\n[22] C. Cortes and V . Vapnik, “Support vector machine,” Mach. Learn. ,\\nvol. 20, no. 3, pp. 273–297, 1995.\\n[23] Y . Freund and R. E. Schapire, “A decision-theoretic generalization of\\non-line learning and an application to boosting,” J. Comput. Syst. Sci. ,\\nvol. 55, no. 1, pp. 119–139, 1997.\\n[24] P. F. Felzenszwalb et al. , “Object detection with discriminatively\\ntrained part-based models,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 32, no. 9, pp. 1627–1645, Sep. 2010.\\n[25] M. Everingham et al., “The pascal visual object classes (VOC) chal-\\nlenge,” Int. J. Comput. Vis. , vol. 88, no. 2, pp. 303–338, 2008.\\n[26] G. E. Hinton and R. R. Salakhutdi nov, “Reducing the dimensionality of\\ndata with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,\\n2006.\\n[27] Y . LeCun et al. , “Deep learning,” Nature, vol. 521, pp. 436–444,\\nMay 2015.\\n[28] N. Liu et al. , “Predicting eye ﬁxations using convolutional neural'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2006.\\n[27] Y . LeCun et al. , “Deep learning,” Nature, vol. 521, pp. 436–444,\\nMay 2015.\\n[28] N. Liu et al. , “Predicting eye ﬁxations using convolutional neural\\nnetworks,” in Proc. CVPR, 2015, pp. 362–370.\\n[29] E. Vig et al. , “Large-scale optimization of hierarchical features\\nfor saliency prediction in natural images,” in Proc. CVPR, 2014,\\npp. 2798–2805.\\n[30] H. Jiang and E. Learned-Miller, “Face detection with the faster\\nR-CNN,” in Proc. FG, 2017, pp. 650–657.\\n[31] D. Chen et al., “Joint cascade face detection and alignment,” in Proc.\\nECCV, 2014, pp. 109–122.\\n[32] D. Chen et al. , “Supervised transformer network for efﬁcient face\\ndetection,” in Proc. ECCV, 2016, pp. 122–138.\\n[33] A. Mateus et al. . (2016). “Efﬁcient and robust pedestrian detection\\nusing deep learning for human-aware navigation.” [Online]. Available:\\nhttps://arxiv.org/abs/1607.04441\\n[34] F. Yang et al. , “Exploit all the layers: Fast and accurate CNN object'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='using deep learning for human-aware navigation.” [Online]. Available:\\nhttps://arxiv.org/abs/1607.04441\\n[34] F. Yang et al. , “Exploit all the layers: Fast and accurate CNN object\\ndetector with scale dependent pooling and cascaded rejection classi-\\nﬁers,” in Proc. CVPR, 2016, pp. 2129–2137.\\n[35] P. N. Druzhkov and V . D. Kustikova, “A survey of deep learning meth-\\nods and software tools for image classiﬁcation and object detection,”\\nPattern Recognit. Image Anal. ,\\n vol. 26, no. 1, pp. 9–15, 2016.\\n[36] W. Pitts and W. S. McCulloch, “How we know universals the perception\\nof auditory and visual forms,” Bull. Math. Biophys. , vol. 9, no. 3,\\npp. 127–147, 1947.\\n[37] D. E. Rumelhart et al., “Learning representations by back-propagating\\nerrors,” Nature, vol. 323, pp. 533–536, Oct. 1986.\\n[38] G. Hinton et al., “Deep neural networks for acoustic modeling in speech\\nrecognition: The shared views of four research groups,” IEEE Signal\\nProcess. Mag., vol. 29, no. 6, pp. 82–97, Nov. 2012.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[38] G. Hinton et al., “Deep neural networks for acoustic modeling in speech\\nrecognition: The shared views of four research groups,” IEEE Signal\\nProcess. Mag., vol. 29, no. 6, pp. 82–97, Nov. 2012.\\n[39] J. Deng et al., “ImageNet: A large-scale hierarchical image database,”\\nin Proc. CVPR, 2009, pp. 248–255.\\n[40] L. Deng et al. , “Binary coding of speech spectrograms using a deep\\nauto-encoder,” in Proc. INTERSPEECH, 2010, pp. 1692–1695.\\n[41] G. Dahl et al., “Phone recognition with the mean -covariance restricted\\nBoltzmann machine,” in Proc. NIPS, 2010, pp. 469–477.\\n[42] G. E. Hinton et al. . (2012). “Improving neural networks by pre-\\nventing co-adaptation of feature detectors.” [Online]. Available:\\nhttps://arxiv.org/abs/1207.0580\\n[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,” in Proc. ICML,\\n2015, pp. 448–456.\\n[44] P. Sermanet et al.. (2013). “OverFeat: Integrated recognition, localiza-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='network training by reducing internal covariate shift,” in Proc. ICML,\\n2015, pp. 448–456.\\n[44] P. Sermanet et al.. (2013). “OverFeat: Integrated recognition, localiza-\\ntion and detection using convolutional networks.” [Online]. Available:\\nhttps://arxiv.org/abs/1312.6229\\n[45] C. Szegedy et al., “Going deeper with convolutions,” in Proc. CVPR,\\n2015, pp. 1–9.\\n[46] K. Simonyan and A. Zisserman. (2014).“Very deep convolutional\\nnetworks for large-scale image recognition.” [Online]. Available:\\nhttps://arxiv.org/abs/1409.1556\\n[47] K. He et al., “Deep residual learning for image recognition,” in Proc.\\nCVPR, 2016, pp. 770–778.\\n[48] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\\nBoltzmann machines,” in Proc. ICML, 2010, pp. 807–814.\\n[49] M. Oquab et al. , “Weakly supervised object recognition with\\nconvolutional neural networks,” in Proc. NIPS, 2014, pp. 1–10.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 19\\n[50] M. Oquab et al. , “Learning and transferring mid-level image\\nrepresentations using convolutional neural networks,” in Proc. CVPR,\\n2014, pp. 1717–1724.\\n[51] F. M. Wadley, “Probit analysis: A statistical treatment of the sigmoid\\nresponse curve,” Ann. Entomol. Soc. Amer., vol. 67, no. 4, pp. 549–553,\\n1947.\\n[52] K. Kavukcuoglu et al. , “Learning invariant features through\\ntopographic ﬁlter maps,” in Proc. CVPR, 2009, pp. 1605–1612.\\n[53] K. Kavukcuoglu et al., “Learning convolutional feature hierarchies for\\nvisual recognition,” in Proc. NIPS, 2010, pp. 1090–1098.\\n[54] M. D. Zeiler et al., “Deconvolutional networks,” in Proc. CVPR, 2010,\\npp. 2528–2535.\\n[55] H. Noh et al. , “Learning deconvolution network for semantic\\nsegmentation,” in Proc. ICCV, 2015, pp. 1520–1528.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='pp. 2528–2535.\\n[55] H. Noh et al. , “Learning deconvolution network for semantic\\nsegmentation,” in Proc. ICCV, 2015, pp. 1520–1528.\\n[56] Z.-Q. Zhao et al., “Plant leaf identiﬁcation via a growing convolution\\nneural network with progressive sample learning,” in Proc. ACCV ,\\n2014, pp. 348–361.\\n[57] A. Babenko et al., “Neural codes for image retrieval,” in Proc. ECCV,\\n2014, pp. 584–599.\\n[58] J. Wan et al. , “Deep learning for content-based image retrieval:\\nA comprehensive study,” in ACM MM, 2014, pp. 157–166.\\n[59] D. Tomè et al. , “Deep convolutional neural networks for pedestrian\\ndetection,” Signal Process., Image Commun. , vol. 47, pp. 482–489,\\nSep. 2016.\\n[60] Y . Xiang et al., “Subcategory-aware convolutional neural networks for\\nobject proposals and detection,” in Proc. WACV, 2017, pp. 924–933.\\n[61] Z.-Q. Zhao et al. , “Pedestrian detection based on fast R-CNN and\\nbatch normalization,” in Proc. ICIC, 2017, pp. 735–746.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object proposals and detection,” in Proc. WACV, 2017, pp. 924–933.\\n[61] Z.-Q. Zhao et al. , “Pedestrian detection based on fast R-CNN and\\nbatch normalization,” in Proc. ICIC, 2017, pp. 735–746.\\n[62] J. Ngiam et al. , “Multimodal deep learning,” in Proc. ICML , 2011,\\npp. 689–696.\\n[63] Z. Wu et al. , “Modeling spatial-temporal clues in a hybrid deep\\nlearning framework for video classiﬁcation,” in Proc. ACM MM, 2015,\\npp. 461–470.\\n[64] K. He et al., “Spatial pyramid pooling in deep convolutional networks\\nfor visual recognition,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 37, no. 9, pp. 1904–1916, Sep. 2015.\\n[65] J. Dai et al. , “R-FCN: Object detection via region-based fully\\nconvolutional networks,” in Proc. NIPS, 2016, pp. 379–387.\\n[66] T.-Y . Lin et al. , “Feature pyramid networks for object detection,” in\\nProc. CVPR, 2017, pp. 936–944.\\n[67] K. He et al., “Mask R-CNN,” in Proc. ICCV, 2017, pp. 2980–2988.\\n[68] D. Erhan et al., “Scalable object detection using deep neural networks,”'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Proc. CVPR, 2017, pp. 936–944.\\n[67] K. He et al., “Mask R-CNN,” in Proc. ICCV, 2017, pp. 2980–2988.\\n[68] D. Erhan et al., “Scalable object detection using deep neural networks,”\\nin Proc. CVPR, 2014, pp. 2155–2162.\\n[69] D. Yoo et al., “AttentionNet: Aggregating weak directions for accurate\\nobject detection,” in Proc. CVPR, 2015, pp. 2659–2667.\\n[70] M. Najibi et al., “G-CNN: An iterative grid based object detector,” in\\nProc. CVPR, 2016, pp. 2369–2377.\\n[71] W. Liu et al., “SSD: Single shot multibox detector,” in Proc. ECCV,\\n2016, pp. 21–37.\\n[72] J. Redmon and A. Farhadi. (2016). “YOLO9000: Better, faster,\\nstronger.” [Online]. Available: https://arxiv.org/abs/1612.08242\\n[73] C.-Y . Fu et al.. (2017). “DSSD : Deconvolutional single shot detector.”\\n[Online]. Available: https://arxiv.org/abs/1701.06659\\n[74] Z. Shen et al. , “DSOD: Learning deeply supervised object detectors\\nfrom scratch,” in Proc. ICCV, 2017, p. 7.\\n[75] G. E. Hinton et al. , “Transforming auto-encoders,” in Proc. ICANN,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[74] Z. Shen et al. , “DSOD: Learning deeply supervised object detectors\\nfrom scratch,” in Proc. ICCV, 2017, p. 7.\\n[75] G. E. Hinton et al. , “Transforming auto-encoders,” in Proc. ICANN,\\n2011, pp. 44–51.\\n[76] G. W. Taylor et al., “Learning invariance through imitation,” in Proc.\\nCVPR, 2011, pp. 2729–2736.\\n[77] X. Ren and D. Ramanan, “Histograms of sparse codes for object\\ndetection,” in Proc. CVPR, 2013, pp. 3246–3253.\\n[78] J. R. R. Uijlings et al. , “Selective search for object recognition,” Int.\\nJ. Comput. Vis., vol. 104, no. 2, pp. 154–171, Apr. 2013.\\n[79] P. Sermanet et al., “Pedestrian detection with unsupervised multi-stage\\nfeature learning,” in Proc. CVPR, 2013, pp. 3626–3633.\\n[80] P. Krähenbühl and V . Koltun, “Geodesic object proposals,” in Proc.\\nECCV, 2014, pp. 725–739.\\n[81] P. Arbeláez et al. , “Multiscale combinatorial grouping,” in Proc.\\nCVPR, 2014, pp. 328–335.\\n[82] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ECCV, 2014, pp. 725–739.\\n[81] P. Arbeláez et al. , “Multiscale combinatorial grouping,” in Proc.\\nCVPR, 2014, pp. 328–335.\\n[82] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals\\nfrom edges,” in Proc. ECCV, 2014, pp. 391–405.\\n[83] W. Kuo et al. , “Deepbox: Learning objectness with convolutional\\nnetworks,” in Proc. ICCV, 2015, pp. 2479–2487.\\n[84] P. O. Pinheiro et al. , “Learning to reﬁne object segments,” in Proc.\\nECCV, 2016, pp. 75–91.\\n[85] Y . Zhang et al., “Improving object detection with deep convolutional\\nnetworks via Bayesian optimizatio n and structured prediction,” in\\nProc. CVPR, 2015, pp. 249–258.\\n[86] S. Gupta et al., “Learning rich features from RGB-D images for object\\ndetection and segmentation,” in Proc. ECCV, 2014, pp. 345–360.\\n[87] W. Ouyang et al., “DeepID-Net: Deformable deep convolutional neural\\nnetworks for object detection,” in Proc. CVPR, 2015, pp. 2403–2412.\\n[88] K. Lenc and A. Vedaldi. (2015). “R-CNN minus R.” [Online].'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='networks for object detection,” in Proc. CVPR, 2015, pp. 2403–2412.\\n[88] K. Lenc and A. Vedaldi. (2015). “R-CNN minus R.” [Online].\\nAvailable: https://arxiv.org/abs/1506.06981\\n[89] S. Lazebnik et al. , “Beyond bags of features: Spatial pyramid\\nmatching for recognizing natural scene categories,” in Proc. CVPR,\\n2006, pp. 2169–2178.\\n[90] F. Perronnin et al., “Improving the Fisher kernel for large-scale image\\nclassiﬁcation,” in Proc. ECCV, 2010, pp. 143–156.\\n[91] J. Xue et al., “Restructuring of deep neural network acoustic models\\nwith singular value decomposition,” in Proc. INTERSPEECH , 2013,\\npp. 2365–2369.\\n[92] S. Ren et al., “Faster R-CNN: Towards real-time object detection with\\nregion proposal networks,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 39, no. 6, pp. 1137–1149, Jun. 2017.\\n[93] C. Szegedy et al., “Rethinking the inception architecture for computer\\nvision,” in Proc. CVPR, 2016, pp. 2818–2826.\\n[94] T.-Y . Lin et al. , “Microsoft COCO: Common objects in context,” in'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[93] C. Szegedy et al., “Rethinking the inception architecture for computer\\nvision,” in Proc. CVPR, 2016, pp. 2818–2826.\\n[94] T.-Y . Lin et al. , “Microsoft COCO: Common objects in context,” in\\nProc. ECCV, 2014, pp. 740–755.\\n[95] S. Bell et al. , “Inside-outside net: Detecting objects in context with\\nskip pooling and recurrent neural networks,” in Proc. CVPR, 2016,\\npp. 2874–2883.\\n[96] A. Arnab and P. H. S. Torr, “Pixe lwise instance segmentation with a\\ndynamically instantiated network,” in Proc. CVPR, 2017, pp. 879–888.\\n[97] J. Dai et al. , “Instance-aware semantic segmentation via multi-task\\nnetwork cascades,” in Proc. CVPR, 2016, pp. 3150–3158.\\n[98] Y . Li et al. , “Fully convolutional instance-aware semantic\\nsegmentation,” in Proc. CVPR, 2017, pp. 4438–4446.\\n[99] M. Jaderberg et al. , “Spatial transformer networks,” in Proc. Adv.\\nNeural Inf. Process. Syst. , 2015, pp. 2017–2025.\\n[100] S. Brahmbhatt et al. , “StuffNet: Using ‘Stuff’ to improve object'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[99] M. Jaderberg et al. , “Spatial transformer networks,” in Proc. Adv.\\nNeural Inf. Process. Syst. , 2015, pp. 2017–2025.\\n[100] S. Brahmbhatt et al. , “StuffNet: Using ‘Stuff’ to improve object\\ndetection,” in Proc. WACV, 2017, pp. 934–943.\\n[101] T. Kong et al., “HyperNet: Towards accurate region proposal generation\\nand joint object detection,” in Proc. CVPR, 2016, pp. 845–853.\\n[102] A. Pentina et al., “Curriculum learning of multiple tasks,” in Proc.\\nCVPR, 2015, pp. 5492–5500.\\n[103] J. Yim et al. , “Rotating your face using multi-task deep neural\\nnetwork,” in Proc. CVPR, 2015, pp. 676–684.\\n[104] J. Li et al.. (2016). “Multi-stage object detection with group recursive\\nlearning.” [Online]. Available: https://arxiv.org/abs/1608.05159\\n[105] Z. Cai et al., “A uniﬁed multi-scale deep convolutional neural network\\nfor fast object detection,” in Proc. ECCV, 2016, pp. 354–370.\\n[106] Y . Zhu et al. , “segDeepM: Exploiting segmentation and context in'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='for fast object detection,” in Proc. ECCV, 2016, pp. 354–370.\\n[106] Y . Zhu et al. , “segDeepM: Exploiting segmentation and context in\\ndeep neural networks for object detection,” in Proc. CVPR, 2015,\\npp. 4703–4711.\\n[107] W. Byeon et al. , “Scene labeling with LSTM recurrent neural\\nnetworks,” in Proc. CVPR, 2015, pp. 3547–3555.\\n[108] B. Moysset et al. . (2016). “Learning to detect and localize\\nmany objects from few examples.” [Online]. Available:\\nhttps://arxiv.org/abs/1611.05664\\n[109] X. Zeng et al. , “Gated bi-directional CNN for object detection,” in\\nProc. ECCV, 2016, pp. 354–369.\\n[110] S. Gidaris and N. Komodakis, “O bject detection via a multi-region\\nand semantic segmentation-aware CNN model,” in Proc. CVPR, 2015,\\npp. 1134–1142.\\n[111] M. Schuster and K. K. Paliwal, “ Bidirectional recurrent neural\\nnetworks,” IEEE Trans. Signal Process. , vol. 45, no. 11,\\npp. 2673–2681, Nov. 1997.\\n[112] S. Zagoruyko et al. (2016). “A multiPath network for object detection.”'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='networks,” IEEE Trans. Signal Process. , vol. 45, no. 11,\\npp. 2673–2681, Nov. 1997.\\n[112] S. Zagoruyko et al. (2016). “A multiPath network for object detection.”\\n[Online]. Available: https://arxiv.org/abs/1604.02135\\n[113] A. Shrivastava et al. , “Training region-based object detectors\\nwith online hard example mining,” in Proc. CVPR, 2016,\\npp. 761–769.\\n[114] S. Ren et al. , “Object detection networks on convolutional feature\\nmaps,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39, no. 7,\\npp. 1476–1481, Jul. 2017.\\n[115] W. Ouyang et al., “Factors in ﬁnetuning deep model for object detection\\nwith long-tail distribution,” in Proc. CVPR, 2016, pp. 864–873.\\n[116] S. Hong et al. . (2016). “PV ANet: Lightweight deep neural\\nnetworks for real-time object detection.” [Online]. Available:\\nhttps://arxiv.org/abs/1611.08588\\n[117] W. Shang et al. , “Understanding and improvi ng convolutional neural\\nnetworks via concatenated rectiﬁed linear units,” in Proc. ICML, 2016,\\npp. 1–9.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='https://arxiv.org/abs/1611.08588\\n[117] W. Shang et al. , “Understanding and improvi ng convolutional neural\\nnetworks via concatenated rectiﬁed linear units,” in Proc. ICML, 2016,\\npp. 1–9.\\n[118] C. Szegedy et al. , “Deep neural networks for object detection,” in\\nProc. NIPS, 2013, pp. 1–9.\\n[119] P. O. Pinheiro et al., “Learning to segment object candidates,” in Pr\\n oc.\\nNIPS, 2015, pp. 1990–1998.\\n[120] C. Szegedy et al. . (2014). “Scalable, high-quality object detection.”\\n[Online]. Available: https://arxiv.org/abs/1412.1441'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n20 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\n[121] M. Everingham et al. . (2011). The PASCAL Visual Object\\nClasses Challenge 2012 (VOC2012) Results (2012). [Online].\\nAvailable: http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html\\n[122] M. D. Zeiler and R. Fergus, “ Visualizing and understanding\\nconvolutional networks,” in Proc. ECCV, 2014, pp. 818–833.\\n[123] S. Xie et al. , “Aggregated residual transformations for deep neural\\nnetworks,” in Proc. CVPR, 2017, pp. 5987–5995.\\n[124] \"J. Dai et al. (2017). “Deformable convolutional networks.” [Online].\\nAvailable: https://arxiv.org/abs/1703.06211\\n[125] C. Rother et al. , “AutoCollage,” ACM Trans. Graph. , vol. 25, no. 3,\\npp. 847–852, 2006.\\n[126] C. Jung and C. Kim, “A uniﬁed spectral-domain approach for saliency'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[125] C. Rother et al. , “AutoCollage,” ACM Trans. Graph. , vol. 25, no. 3,\\npp. 847–852, 2006.\\n[126] C. Jung and C. Kim, “A uniﬁed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,” IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272–1283, Mar. 2012.\\n[127] W.-C. Tu et al. , “Real-time salient object detection with a minimum\\nspanning tree,” in Proc. CVPR, 2016, pp. 2334–2342.\\n[128] J. Yang and M.-H. Yang, “Top-down visual saliency via joint CRF and\\ndictionary learning,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39,\\nno. 3, pp. 576–588, Mar. 2017.\\n[129] P. L. Rosin, “A simple method for detecting salient regions,” Pattern\\nRecognit., vol. 42, no. 11, pp. 2363–2371, Nov. 2009.\\n[130] T. Liu et al., “Learning to detect a salient object,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 33, no. 2, pp. 353–367, Feb. 2011.\\n[131] J. Long et al. , “Fully convolutional networks for semantic\\nsegmentation,” in Proc. CVPR, 2015, pp. 3431–3440.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Anal. Mach. Intell. , vol. 33, no. 2, pp. 353–367, Feb. 2011.\\n[131] J. Long et al. , “Fully convolutional networks for semantic\\nsegmentation,” in Proc. CVPR, 2015, pp. 3431–3440.\\n[132] D. Gao et al. , “Discriminant saliency, the detection of suspicious\\ncoincidences, and applications to visual recognition,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 31, no. 6, pp. 989–1005, Jun. 2009.\\n[133] S. Xie and Z. Tu, “Holistica lly-nested edge detection,” in Proc. ICCV,\\n2015, pp. 1395–1403.\\n[134] M. Kümmerer et al. . (2014). “Deep gaze I: Boosting saliency\\nprediction with feature maps trained on ImageNet.” [Online]. Available:\\nhttps://arxiv.org/abs/1411.1045\\n[135] X. Huang et al. , “SALICON: Reducing the semantic gap in saliency\\nprediction by adapting deep neural networks,” in Proc. ICCV , 2015,\\npp. 262–270.\\n[136] L. Wang et al. , “Deep networks for saliency detection via local\\nestimation and global search,” in Proc. CVPR, 2015, pp. 3183–3192.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='pp. 262–270.\\n[136] L. Wang et al. , “Deep networks for saliency detection via local\\nestimation and global search,” in Proc. CVPR, 2015, pp. 3183–3192.\\n[137] H. Cholakkal et al. . (2016). “Backtracking spatial pyramid pooling\\n(SPP)-based image classiﬁer for weakly supervised top-down salient\\nobject detection.” [Online]. Available: https://arxiv.org/abs/1611.05345\\n[138] R. Zhao et al. , “Saliency detection by multi-context deep learning,”\\nin Proc. CVPR, 2015, pp. 1265–1274.\\n[139] C. Bak et\\n al.. (2016). “Spatio-temporal saliency networks\\nfor dynamic saliency prediction.” [Online]. Available:\\nhttps://arxiv.org/abs/1607.04730\\n[140] S. He et al. , “SuperCNN: A superpixelwi se convolutional neural\\nnetwork for salient object detection,” Int. J. Comput. Vis. , vol. 115,\\nno. 3, pp. 330–344, 2015.\\n[141] X. Li et al., “DeepSaliency: Multi-task deep neural network model for\\nsalient object detection,” IEEE Trans. Image Process., vol. 25, no. 8,\\npp. 3919–3930, Aug. 2016.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='no. 3, pp. 330–344, 2015.\\n[141] X. Li et al., “DeepSaliency: Multi-task deep neural network model for\\nsalient object detection,” IEEE Trans. Image Process., vol. 25, no. 8,\\npp. 3919–3930, Aug. 2016.\\n[142] Y . Tang and X. Wu, “Saliency det ection via combining region-level\\nand pixel-level predictions with CNNs,” in Proc. ECCV , 2016,\\npp. 809–825.\\n[143] G. Li and Y . Yu, “Deep contrast lear ning for salient object detection,”\\nin Proc. CVPR, 2016, pp. 478–487.\\n[144] X. Wang et al. . (2016). “Edge preserving and multi-scale contextual\\nneural network for salient object detection.” [Online]. Available:\\nhttps://arxiv.org/abs/1608.08029\\n[145] M. Cornia et al., “A deep multi-level network for saliency prediction,”\\nin Proc. ICPR, 2016, pp. 3488–3493.\\n[146] G. Li and Y . Yu, “Visual saliency detection based on multiscale\\ndeep CNN features,” IEEE Trans. Image Process., vol. 25, no. 11,\\npp. 5012–5024, Nov. 2016.\\n[147] J. Pan et al. , “Shallow and deep convolutional networks for saliency'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='deep CNN features,” IEEE Trans. Image Process., vol. 25, no. 11,\\npp. 5012–5024, Nov. 2016.\\n[147] J. Pan et al. , “Shallow and deep convolutional networks for saliency\\nprediction,” in Proc. CVPR, 2016, pp. 598–606.\\n[148] J. Kuen et al., “Recurrent attentional networks for saliency detection,”\\nin Proc. CVPR, 2016, pp. 3668–3677.\\n[149] Y . Tang et al. , “Deeply-supervised recurrent convolutional neural\\nnetwork for saliency detection,” inProc. ACM MM, 2016, pp. 397–401.\\n[150] X. Li et al. , “Contextual hypergraph modeling for salient object\\ndetection,” in Proc. ICCV, 2013, pp. 3328–3335.\\n[151] M.-M. Cheng et al., “Global contrast based salient region detection,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 37, no. 3, pp. 569–582,\\nMar. 2015.\\n[152] H. Jiang et al. , “Salient object detection: A discriminative\\nregional feature integration approach,” in Proc. CVPR, 2013,\\npp. 2083–2090.\\n[153] G. Lee et al. , “Deep saliency with encoded low level distance map'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='regional feature integration approach,” in Proc. CVPR, 2013,\\npp. 2083–2090.\\n[153] G. Lee et al. , “Deep saliency with encoded low level distance map\\nand high level features,” in Proc. CVPR, 2016, pp. 660–668.\\n[154] Z. Luo et al. , “Non-local deep features for salient object detection,”\\nin Proc. CVPR, 2017, pp. 6593–6601.\\n[155] Q. Hou et al. . (2016). “Deeply supervised salient object\\ndetection with short connections.” [Online]. Available:\\nhttps://arxiv.org/abs/1611.04849\\n[156] Q. Yan et al.,\\n“Hierarchical saliency detection,” in Proc. CVPR, 2013,\\npp. 1155–1162.\\n[157] Y . Li et al. , “The secrets of salient object segmentation,” in Proc.\\nCVPR, 2014, pp. 280–287.\\n[158] V . Movahedi and J. H. Elder, “Design and perceptual validation\\nof performance measures for salient object segmentation,” in Proc.\\nCVPRW, 2010, pp. 49–56.\\n[159] A. Borji et al. , “Salient object detection: A bench-\\nmark,” IEEE Trans. Image Process. , vol. 24, no. 12,\\npp. 5706–5722, Dec. 2015.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='CVPRW, 2010, pp. 49–56.\\n[159] A. Borji et al. , “Salient object detection: A bench-\\nmark,” IEEE Trans. Image Process. , vol. 24, no. 12,\\npp. 5706–5722, Dec. 2015.\\n[160] C. Peng et al. , “Graphical representation for heterogeneous face\\nrecognition,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39, no. 2,\\npp. 301–312, Feb. 2017.\\n[161] C. Peng et al. , “Face recognition from multiple stylistic sketches:\\nScenarios, datasets, and evaluation,” in Proc. ECCV, 2016, pp. 3–18.\\n[162] X. Gao et al., “Face sketch–photo synthesis and retrieval using sparse\\nrepresentation,” IEEE Trans. Circuits Syst. Video Technol. , vol. 22,\\nno. 8, pp. 1213–1226, Aug. 2012.\\n[163] N. Wang et al. , “A comprehensive survey to face hallucination,” Int.\\nJ. Comput. Vis., vol. 106, no. 1, pp. 9–30, 2014.\\n[164] C. Peng et al. , “Multiple representations-based face sketch–photo\\nsynthesis,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 27, no. 11,\\npp. 2201–2215, Nov. 2016.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[164] C. Peng et al. , “Multiple representations-based face sketch–photo\\nsynthesis,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 27, no. 11,\\npp. 2201–2215, Nov. 2016.\\n[165] A. Majumder et al. , “Automatic facial expression recognition system\\nusing deep network-based data fusion,” IEEE Trans. Cybern., vol. 48,\\nno. 1, pp. 103–114, Jan. 2018.\\n[166] P. Viola and M. J. Jones, “Robust real-time face detection,” Int. J.\\nComput. Vis., vol. 57, no. 2, pp. 137–154, 2004.\\n[167] J. Yu et al. , “Unitbox: An advanced object detection network,” in\\nProc. ACM MM, 2016, pp. 516–520.\\n[168] S. S. Farfade et al., “Multi-view face detection using deep convolutional\\nneural networks,” in Proc. ICMR, 2015, pp. 643–650.\\n[169] S. Yang et al., “From facial parts responses to face detection: A deep\\nlearning approach,” in Proc. ICCV, 2015, pp. 3676–3684.\\n[170] S. Yang et al. , “Face detection through scale-friendly deep\\nconvolutional networks,” in Proc. CVPR, 2017, pp. 1–12.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='learning approach,” in Proc. ICCV, 2015, pp. 3676–3684.\\n[170] S. Yang et al. , “Face detection through scale-friendly deep\\nconvolutional networks,” in Proc. CVPR, 2017, pp. 1–12.\\n[171] Z. Hao et al. , “Scale-aware face detection,” in Proc. CVPR, 2017,\\npp. 1913–1922.\\n[172] H. Wang et al. . (2017). “Face R-CNN.” [Online]. Available:\\nhttps://arxiv.org/abs/1706.01061\\n[173] X. Sun et al. . (2017). “Face detection using deep learning:\\nAn improved faster RCNN approach.” [Online]. Available:\\nhttps://arxiv.org/abs/1701.08289\\n[174] L. Huang et al. . (2015). “DenseBox: Unifying landmark\\nlocalization with end to end object detection.” [Online]. Available:\\nhttps://arxiv.org/abs/1509.04874\\n[175] Y . Li et al., “face detection with end-to-end integration of a ConvNet\\nand a 3D model,” in Proc. ECCV, 2016, pp. 420–436.\\n[176] K. Zhang et al. , “Joint face detection and alignment using multitask\\ncascaded convolutional networks,” IEEE Signal Process. Lett., vol. 23,\\nno. 10, pp. 1499–1503, Oct. 2016.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[176] K. Zhang et al. , “Joint face detection and alignment using multitask\\ncascaded convolutional networks,” IEEE Signal Process. Lett., vol. 23,\\nno. 10, pp. 1499–1503, Oct. 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, “Compact convolutional neural\\nnetwork cascade for face detection,” in Proc. CEUR Workshop, 2016,\\npp. 375–387.\\n[178] H. Qin et al., “Joint training of cascaded CNN for face detection,” in\\nProc. CVPR, 2016, pp. 3456–3465.\\n[179] V . Jain and E. Learned-Miller, “FDDB: A benchmark for face detection\\nin unconstrained settings,” Univ. Massachusetts, Amherst, Amherst,\\nMA, USA, Tech. Rep. UM-CS-2010-009, 2010.\\n[180] H. Li et al. , “A convolutional neural network cascade for face\\ndetection,” in Proc. CVPR, 2015, pp. 5325–5334.\\n[181] B. Yang et al. , “Aggregate channel features for multi-view face\\ndetection,” in Proc. IJCB, 2014, pp. 1–8.\\n[182] N. Markuš et al. . (2013). “Object detection with pixel intensity\\ncomparisons organized in decisi on trees.” [Online]. Available:'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection,” in Proc. IJCB, 2014, pp. 1–8.\\n[182] N. Markuš et al. . (2013). “Object detection with pixel intensity\\ncomparisons organized in decisi on trees.” [Online]. Available:\\nhttps://arxiv.org/abs/1305.4537\\n[183] M. Mathias et al. , “Face detection without bells and whistles,” in\\nProc. ECCV, 2014.\\n[184] J. Li and Y . Zhang, “Learning surf cascade for fast and accurate object\\ndetection,” in Proc. CVPR, 2013.\\n[185] S. Liao et al. , “A fast and accurate unconstrained face detector,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. 2,\\npp. 211–223, Feb. 2016.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 21\\n[186] B. Yang et al., “Convolutional channel features,” in Proc. ICCV, 2015,\\npp. 82–90.\\n[187] R. Ranjan et al. . (2016). “HyperFace: A deep multi-task\\nlearning framework for face det ection, landmark localization,\\npose estimation, and gender recognition.” [Online]. Available:\\nhttps://arxiv.org/abs/1603.01249\\n[188] P. Hu and D. Ramanan, “Finding tiny faces,” in Proc. CVPR, 2017,\\npp. 1522–1530.\\n[189] Z. Jiang and D. Q. Huynh, “Multiple pedestrian tracking from\\nmonocular videos in an interacting multiple model framework,” IEEE\\nTrans. Image Process., vol. 27, no. 3, pp. 1361–1375, Mar. 2018.\\n[190] D. M. Gavrila and S. Munder, “Multi-cue pedestrian detection and\\ntracking from a moving vehicle,” Int. J. Comput. Vis. , vol. 73, no. 1,\\npp. 41–59, Jun. 2007.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[190] D. M. Gavrila and S. Munder, “Multi-cue pedestrian detection and\\ntracking from a moving vehicle,” Int. J. Comput. Vis. , vol. 73, no. 1,\\npp. 41–59, Jun. 2007.\\n[191] S. Xu et al. , “Jointly attentive spatial-temporal pooling networks\\nfor video-based person re-identiﬁcation,” in Proc. ICCV , 2017,\\npp. 4743–4752.\\n[192] Z. Liu et al. , “Stepwise metric promotion for unsupervised video\\nperson re-identiﬁcation,” in Proc. ICCV, 2017, pp. 2448–2457.\\n[193] A. Khan et al. , “Cooperative robots to observe moving targets:\\nReview,”IEEE Trans. Cybern., vol. 48, no. 1, pp. 187–198, Jan. 2018.\\n[194] A. Geiger et al. , “Vision meets robotics: The KITTI dataset,” Int. J.\\nRobot. Res., vol. 32, no. 11, pp. 1231–1237, 2013.\\n[195] Z. Cai et al., “Learning complexity-aware cascades for deep pedestrian\\ndetection,” in Proc. ICCV, 2015, pp. 3361–3369.\\n[196] Y . Tian et al. , “Deep learning strong parts f or pedestrian detection,”\\nin Proc. CVPR, 2015, pp. 1904–1912.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection,” in Proc. ICCV, 2015, pp. 3361–3369.\\n[196] Y . Tian et al. , “Deep learning strong parts f or pedestrian detection,”\\nin Proc. CVPR, 2015, pp. 1904–1912.\\n[197] P. Dollár et al. , “Fast feature pyramids for object detection,” IEEE\\nTrans. Pattern Anal. Mach. Intell. , vol. 36, no. 8, pp. 1532–1545,\\nAug. 2014.\\n[198] S. Zhang et al. , “Filtered channel features for pedestrian detection,”\\nin Proc. CVPR, 2015, pp. 1751–1760.\\n[199] S. Paisitkriangkrai et al. , “Pedestrian detection with spatially pooled\\nfeatures and structured ensemble learning,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 38, no. 6, pp. 1243–1257, Jun. 2016.\\n[200] L. Lin et al. , “Discriminatively trained And-Or graph models for\\nobject shape detection,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 37, no. 5, pp. 959–972, May 2015.\\n[201] M. Mathias et al. , “Handling occlusions with Franken-classiﬁers,” in\\nProc. ICCV, 2013, pp. 1505–1512.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='vol. 37, no. 5, pp. 959–972, May 2015.\\n[201] M. Mathias et al. , “Handling occlusions with Franken-classiﬁers,” in\\nProc. ICCV, 2013, pp. 1505–1512.\\n[202] S. Tang et al. , “Detection and tracking of occluded people,” Int. J.\\nComput.\\nVis., vol. 110, no. 1, pp. 58–69, 2014.\\n[203] L. Zhang et al., “Is faster R-CNN doing well for pedestrian detection?”\\nin Proc. ECCV, 2016, pp. 443–457.\\n[204] Y . Tian et al. , “Deep learning strong parts f or pedestrian detection,”\\nin Proc. ICCV, 2015.\\n[205] J. Liu et al.. (2016). “Multispectral deep neural networks for pedestrian\\ndetection.” [Online]. Available: https://arxiv.org/abs/1611.02644\\n[206] Y . Tian et al. , “Pedestrian detection aided by deep learning semantic\\ntasks,” in Proc. CVPR, 2015, pp. 5079–5087.\\n[207] X. Du et al. , “Fused DNN: A deep neural network fusion approach\\nto fast and robust pedestrian detection,” in Proc. WACV, 2017,\\npp. 953–961.\\n[208] Q. Hu et al. , “Pushing the limits of deep CNNs for pedestrian'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to fast and robust pedestrian detection,” in Proc. WACV, 2017,\\npp. 953–961.\\n[208] Q. Hu et al. , “Pushing the limits of deep CNNs for pedestrian\\ndetection,” IEEE Trans. Circuits Syst. Video Technol. , vol. 28, no. 6,\\npp. 1358–1368, Jun. 2018.\\n[209] D. Tomé et al. , “Reduced memory region based deep convolutional\\nneural network detection,” in Proc. ICCE , Berlin, Germany, 2016,\\npp. 15–19.\\n[210] J. Hosang et al., “Taking a deeper look at pedestrians,” in Proc. CVPR,\\n2015, pp. 4073–4082.\\n[211] J. Li et al.. (2015). “Scale-aware fast R-CNN for pedestrian detection.”\\n[Online]. Available: https://arxiv.org/abs/1510.08160\\n[212] Y . Gao et al. , “Visual-textual joint relevance learning for tag-based\\nsocial image search,” IEEE Trans. Image Process., vol. 22, no. 1,\\npp. 363–376, Jan. 2013.\\n[213] T. Kong et al. , “RON: Reverse connection with objectness prior\\nnetworks for object detection,” in Proc. CVPR, 2017, pp. 5244–5252.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='pp. 363–376, Jan. 2013.\\n[213] T. Kong et al. , “RON: Reverse connection with objectness prior\\nnetworks for object detection,” in Proc. CVPR, 2017, pp. 5244–5252.\\n[214] I. J. Goodfellow et al. , “Generative adversarial nets,” in Proc. NIPS,\\n2014, pp. 2672–2680.\\n[215] Y . Fang et al. , “Object detection meets knowledge graphs,” in Proc.\\nIJCAI, 2017, pp. 1661–1667.\\n[216] S. Welleck et al. , “Saliency-based sequential image attention with\\nmultiset prediction,” in Proc. NIPS, 2017, pp. 5173–5183.\\n[217] S. Azadi et al., “Learning detection with diverse proposals,” in Proc.\\nCVPR, 2017, pp. 7369–7377.\\n[218] S. Sukhbaatar et al. , “End-to-end memory networks,” in Proc. NIPS,\\n2015, pp. 2440–2448.\\n[219]\\nP. Dabkowski and Y . Gal, “Real time image saliency for black box\\nclassiﬁers,” in Proc. NIPS, 2017, pp. 6967–6976.\\n[220] B. Yang et al., “CRAFT objects from images,” in Proc. CVPR, 2016,\\npp. 6043–6051.\\n[221] I. Croitoru et al. , “Unsupervised learning from video to detect fore-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[220] B. Yang et al., “CRAFT objects from images,” in Proc. CVPR, 2016,\\npp. 6043–6051.\\n[221] I. Croitoru et al. , “Unsupervised learning from video to detect fore-\\nground objects in single images,” in Proc. ICCV, 2017, pp. 4345–4353.\\n[222] C. Wang et al. , “Weakly supervised object localization with latent\\ncategory learning,” in Proc. ECCV, 2014, pp. 431–445.\\n[223] D. P. Papadopoulos et al., “Training object class detectors with click\\nsupervision,” in Proc. CVPR, 2017, pp. 180–189.\\n[224] J. Huang et al. , “Speed/accuracy trade-offs for modern convolutional\\nobject detectors,” in Proc. CVPR, 2017, pp. 3296–3297.\\n[225] Q. Li et al., “Mimicking very efﬁcient network for object detection,”\\nin Proc. CVPR, 2017, pp. 7341–7349.\\n[226] G. Hinton et al. , “Distilling the knowledge in a neural network,”\\nComput. Sci., vol. 14, no. 7, pp. 38–39, 2015.\\n[227] A. Romero et al., “FitNets: Hints for thin deep nets,” in Proc. ICLR,\\n2015, pp. 1–13.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Comput. Sci., vol. 14, no. 7, pp. 38–39, 2015.\\n[227] A. Romero et al., “FitNets: Hints for thin deep nets,” in Proc. ICLR,\\n2015, pp. 1–13.\\n[228] X. Chen et al. , “3D object proposals for accurate object class\\ndetection,” in Proc. NIPS, 2015, pp. 424–432.\\n[229] J. Dong et al. , “Visual-inertial-semantic scene representation for 3D\\nobject detection,” in Proc. CVPR, 2017, pp. 960–970.\\n[230] K. Kang et al. , “Object detection in videos with tubelet proposal\\nnetworks,” in Proc. CVPR, 2017, pp. 889–897.\\nZhong-Qiu Zhao (M’10) received the Ph.D. degree\\nin pattern recognition and intelligent system from\\nthe University of Science and Technology of China,\\nHefei, China, in 2007.\\nFrom 2008 to 2009, he held a post-doctoral\\nposition in image processing with the CNRS\\nUMR6168 Lab Sciences de Information et des\\nSystèmes, La Garde, France. From 2013 to 2014,\\nhe was a Research Fellow in image processing with\\nthe Department of Computer Science, Hong Kong\\nBaptist University, Hong Kong. He is currently a'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Systèmes, La Garde, France. From 2013 to 2014,\\nhe was a Research Fellow in image processing with\\nthe Department of Computer Science, Hong Kong\\nBaptist University, Hong Kong. He is currently a\\nProfessor with the Hefei University of Technology, Hefei. His current research\\ninterests include pattern recognition, image processing, and computer vision.\\nPeng Zheng received the bachelor’s degree from\\nthe Hefei University of Technology, Hefei, China,\\nin 2010, where he is currently pursuing the Ph.D.\\ndegree.\\nHis current research in terests include pattern\\nrecognition, image processing, and computer vision.\\nShou-Tao Xu is currently pursuing the master’s\\ndegree with the Hefei University of Technology,\\nHefei, China.\\nHis current research in terests include pattern\\nrecognition, image processing, deep learning, and\\ncomputer vision.\\nXindong Wu (F’11) received the Ph.D. degree in\\nartiﬁcial intelligence from The University of Edin-\\nburgh, Edinburgh, U.K.\\nHe is currently an Alfred and Helen Lamson'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='computer vision.\\nXindong Wu (F’11) received the Ph.D. degree in\\nartiﬁcial intelligence from The University of Edin-\\nburgh, Edinburgh, U.K.\\nHe is currently an Alfred and Helen Lamson\\nEndowed Professor of computer science with the\\nUniversity of Louisiana at Lafayette, Lafayette, LA,\\nUSA. His current research interests include data\\nmining, knowledge-based systems, and Web infor-\\nmation exploration.\\nDr. Wu is a Fellow of the AAAS. He is the\\nSteering Committee Chair of the IEEE International\\nConference on Data Mining. He served as the Editor-in-Chief for the IEEE\\nTRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING (IEEE Com-\\nputer Society) between 2005 and 2008.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f862e",
   "metadata": {},
   "source": [
    "### Embedding and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c7c5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# import chromadb\n",
    "# from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "import uuid\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee89bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1c265502e40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        #huggingface/all-MiniLM-L6-v2\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model is not found\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True) \n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9853c",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12e50227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pickle\n",
    "from typing import List, Any\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectorStoreFAISS:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        persist_dir: str = \"../data/vector_store\",\n",
    "        store_name: str = \"faiss_store\",\n",
    "    ):\n",
    "        self.dim = dim\n",
    "        self.persist_dir = persist_dir\n",
    "        self.store_name = store_name\n",
    "\n",
    "        os.makedirs(self.persist_dir, exist_ok=True)\n",
    "\n",
    "        self.index_path = os.path.join(self.persist_dir, self.store_name + \".index\")\n",
    "        self.meta_path = os.path.join(self.persist_dir, self.store_name + \".pkl\")\n",
    "\n",
    "        self.index = faiss.IndexFlatL2(self.dim)\n",
    "\n",
    "        self.documents = []\n",
    "        self.metadatas = []\n",
    "        self.ids = []\n",
    "        self.embeddings = []\n",
    "\n",
    "        self._load()\n",
    "        print(f\"FAISS Vector Store initialized. Loaded {len(self.documents)} documents.\")\n",
    "\n",
    "    def _save(self):\n",
    "        faiss.write_index(self.index, self.index_path)\n",
    "\n",
    "        data = {\n",
    "            \"documents\": self.documents,\n",
    "            \"metadatas\": self.metadatas,\n",
    "            \"ids\": self.ids,\n",
    "            \"embeddings\": self.embeddings,\n",
    "        }\n",
    "\n",
    "        with open(self.meta_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def _load(self):\n",
    "        if not (os.path.exists(self.index_path) and os.path.exists(self.meta_path)):\n",
    "            print(\"No existing FAISS store found. Starting fresh.\")\n",
    "            return\n",
    "\n",
    "        if os.path.getsize(self.meta_path) == 0:\n",
    "            print(\"Metadata file empty/corrupt. Starting fresh.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            print(\"Loading FAISS index & metadata...\")\n",
    "            self.index = faiss.read_index(self.index_path)\n",
    "\n",
    "            with open(self.meta_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "\n",
    "            self.documents = data.get(\"documents\", [])\n",
    "            self.metadatas = data.get(\"metadatas\", [])\n",
    "            self.ids = data.get(\"ids\", [])\n",
    "            self.embeddings = data.get(\"embeddings\", [])\n",
    "\n",
    "            if self.embeddings and self.index.ntotal == 0:\n",
    "                arr = np.vstack(self.embeddings).astype(\"float32\")\n",
    "                self.index.add(arr)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Load error ({e}). Starting empty.\")\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Mismatch in documents vs embeddings\")\n",
    "\n",
    "        embeddings = np.asarray(embeddings)\n",
    "        if embeddings.ndim == 1:\n",
    "            embeddings = embeddings.reshape(1, -1)\n",
    "\n",
    "        embeddings = embeddings.astype(\"float32\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents...\")\n",
    "\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "        for i, (doc, emb) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{len(self.documents) + i}\"\n",
    "\n",
    "            self.ids.append(doc_id)\n",
    "            self.documents.append(doc.page_content)\n",
    "            metadata = dict(getattr(doc, \"metadata\", {}))\n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "            self.metadatas.append(metadata)\n",
    "            self.embeddings.append(emb)\n",
    "\n",
    "        self._save()\n",
    "        print(\"Done. Total docs:\", len(self.documents))\n",
    "\n",
    "    \n",
    "\n",
    "    def count(self):\n",
    "        return len(self.documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "587a98f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='The Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='P Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='for both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='comments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The\\ndata transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciﬁc training strate gies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='example generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-\\ning initial retrieval-focused pretraining followed by ful l-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more div erse data is crucial for\\nadvancing retrieval model performance, and that leveragin g LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Our model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-\\ntor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhan ced agent systems\\n∗ https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on ﬁne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiﬁcant performance improvements[ 4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ”signal states” repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe ﬁnal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='dient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance\\nof up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='enhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neﬃcient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiﬁcation. Our framework comprises two core components : 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc require ments of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extractio n from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s charact eristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and g eneralization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the ﬁrst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='capability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pa ir classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—suc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]—combined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model’s dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='followed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='we also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='for improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Embedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='to identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constrainin g tokens’ ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='categories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted in to a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ”Question/Query/User” portion is converted into que ries, while the\\n”Answer/Response/Assistant” portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='mantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical va lue of 1, while\\n”no”/”false” are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diﬀeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by us ing the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diﬀerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to\\nto enhance model training eﬃciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='additional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = − 1\\nn\\n∑\\ni\\nlog esim(qi,d +\\ni )/τ\\nesim(qi,d +\\ni )/τ + ∑\\nj esim(qi,d −\\nj )/τ + ∑\\nj̸=i esim(qi,q j )/τ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n∑\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) − sim(xi, x j)\\nτ )) (2)\\n3.3.3 CLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiﬁcation datasets with limited categories may lead to false neg atives from items\\nof diﬀerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='sample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = − 1\\nn\\n∑\\ni\\nlog esim(ti,t +\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/τ +\\n∑\\nn\\nMASK(ti, t −\\ni,n ) ·esim(ti,t −\\ni,n )/τ +\\n∑\\nj̸=i\\nMASK(ti, t j ) ·esim(ti,t j )/τ +\\n∑\\nj̸=i\\n∑\\nn\\nMASK(ti, t −\\nj,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='portance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃcu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='which represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must\\naccurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantic ally equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='semantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcat ions yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='enable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lα\\ni∑ m\\nj=1 lα\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed’s[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusive ly uses retrieval-\\noriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='datasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for η,\\nwhile non-retrieval data constitutes 1 − η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scalin g factor α , a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Sret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ ηRET ·lα\\ni\\nSret\\nif di ∈ RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='passage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface’s sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Classiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-\\nproximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='conﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations f or all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bﬂoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='boards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='both task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding\\nsecured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/c lassiﬁcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1\\nSeed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='NV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99\\nxiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Qwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transform ation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we d eveloped a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='strategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eﬀective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’9 4: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine le arning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Rangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-s hot learners.”\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”F ine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th Int ernational ACM\\nSIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Raghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large languag e models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jaspe r and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv :2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embedd ings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ ıc Magne, and Nils Reimers . ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embed ding: Gen-\\neral text embedding with more and better negative samples.” arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–id f measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='mation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGI R’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with tran sfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='multi-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Through Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embeddin g.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer norma lization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ”Glu variants improve transformer.” arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Wang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapor e. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Generative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345–2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unn atural in-\\nstructions: Tuning language models with (almost) no human labor.” ar Xiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Schiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='models, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis\\nplatform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='linguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation .” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ing comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and\\nMichael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558–3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='explainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association fo r Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriﬁcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='should not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='reason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='manuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Laureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The Faerie Queene’, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='signiﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork it wasn’t an epic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conﬂicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='wary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer’s\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1\\nObject Detection With Deep Learning: A Review\\nZhong-Qiu Zhao , Member, IEEE, Peng Zheng, Shou-Tao Xu, and Xindong Wu , Fellow, IEEE\\nAbstract— Due to object detection’s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by construct-\\ning complex ensembles that combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='classiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently in\\nnetwork architecture, training strategy, and optimization func-\\ntion. In this paper, we provide a review of deep learning-based\\nobject detection frameworks. Our review begins with a brief\\nintroduction on the history of deep learning and its representative\\ntool, namely, the convolutional neural network. Then, we focus\\non typical generic object detection architectures along with some\\nmodiﬁcations and useful tricks to improve detection performance\\nfurther. As distinct speciﬁc detection tasks exhibit different\\ncharacteristics, we also brieﬂy survey several speciﬁc tasks,\\nincluding salient object detection, face detection, and pedestrian\\ndetection. Experimental analyses are also provided to compare'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='characteristics, we also brieﬂy survey several speciﬁc tasks,\\nincluding salient object detection, face detection, and pedestrian\\ndetection. Experimental analyses are also provided to compare\\nvarious methods and draw some meaningful conclusions. Finally,\\nseveral promising directions and tasks are provided to serve as\\nguidelines for future work in both object detection and relevant\\nneural network-based learning systems.\\nIndex Terms— Deep learning, neural network, object detection.\\nI. I NTRODUCTION\\nT\\nO GAIN a complete image understanding, we should\\nnot only concentrate on classifying different images but\\nalso try to precisely estimat e the concepts and locations\\nof objects contained in each image. This task is referred\\nas object detection [1], [S1], which usually consists of dif-\\nferent subtasks such as face detection [2], [S2], pedestrian\\ndetection [3], [S2], and skeleton detection [4], [S3]. As one of\\nthe fundamental computer vision problems, object detection'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ferent subtasks such as face detection [2], [S2], pedestrian\\ndetection [3], [S2], and skeleton detection [4], [S3]. As one of\\nthe fundamental computer vision problems, object detection\\nis able to provide valuable information for semantic under-\\nstanding of images and videos and is related to many applica-\\ntions, including image classiﬁcation [5], [6], human behavior\\nanalysis [7], [S4], face recognition [8], [S5], and autonomous\\ndriving [9], [10]. Meanwhile, inheriting from neural networks\\nManuscript received September 8, 2017; revised March 3, 2018 and\\nJuly 12, 2018; accepted October 15, 2018. This work was supported in part\\nby the National Natural Scienc e Foundation of China under Grant 61672203,\\nGrant 61375047, and Grant 91746209, in part by the National Key Research\\nand Development Program of China under Grant 2016YFB1000901, and in\\npart by the Anhui Natural Science F unds for Distinguished Young Scholar\\nunder Grant 170808J08. (Corresponding author: Zhong-Qiu Zhao.)'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='part by the Anhui Natural Science F unds for Distinguished Young Scholar\\nunder Grant 170808J08. (Corresponding author: Zhong-Qiu Zhao.)\\nZ.-Q. Zhao, P. Zheng, and S.-T. Xu are with the College of Computer\\nScience and Information Engineering, Hefei University of Technology, Hefei\\n230009, China (e-mail: zhongqiuzhao@gmail.com).\\nX. Wu is with the School of Computing and Informatics, University of\\nLouisiana at Lafayette, Lafayette, LA 70504 USA.\\nThis paper has supplementary downloadable material available at\\nhttp://ieeexplore.ieee.org, provided by the authors.\\nColor versions of one or more of the ﬁgures in this paper are available\\nonline at http://ieeexplore.ieee.org.\\nDigital Object Identiﬁer 10.1109/TNNLS.2018.2876865\\nand related learning systems, the progress in these ﬁelds\\nwill develop neural network algorithms and will also have\\ngreat impacts on object detection techniques that can be\\nconsidered as learning systems [11]–[14], [S6]. However, due'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='will develop neural network algorithms and will also have\\ngreat impacts on object detection techniques that can be\\nconsidered as learning systems [11]–[14], [S6]. However, due\\nto large variations in viewpoints, poses, occlusions, and light-\\ning conditions, it is difﬁcult to perfectly accomplish object\\ndetection with an additional object localization task. Therefore,\\nmuch attention has been attr acted to this ﬁeld in recent\\nyears [15]–[18].\\nThe problem deﬁnition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object be longs to (object classiﬁca-\\ntion). Therefore, the pipeline of traditional object detection\\nmodels can be mainly divided into three stages: informative\\nregion selection, feature extraction, and classiﬁcation.\\nA. Informative Region Selection\\nAs different objects may appear in any positions of the\\nimage and have different aspect ratios or sizes, it is a natural'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='A. Informative Region Selection\\nAs different objects may appear in any positions of the\\nimage and have different aspect ratios or sizes, it is a natural\\nchoice to scan the whole image with a multiscale sliding\\nwindow. Although this exhaustive strategy can ﬁnd out all\\npossible positions of the objects, its shortcomings are also\\nobvious. Due to a large number of candidate windows, it is\\ncomputationally expensive and produces too many redundant\\nwindows. However, if only a ﬁxed number of sliding window\\ntemplates is applied, unsatisfactory regions may be produced.\\nB. Feature Extraction\\nTo recognize different objects, we need to extract visual\\nfeatures that can provide a semantic and robust represen-\\ntation. Scale-invariant feature transform [19], histograms of\\noriented gradients (HOG) [20], and Haar-like [21] features are\\nthe representative ones. This is due to the fact that these\\nfeatures can produce representations associated with complex'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='oriented gradients (HOG) [20], and Haar-like [21] features are\\nthe representative ones. This is due to the fact that these\\nfeatures can produce representations associated with complex\\ncells in human brain [19]. However, due to the diversity of\\nappearances, illumination conditions, and backgrounds, it is\\ndifﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nC. Classiﬁcation\\nBesides, a classiﬁer is needed to distinguish a target object\\nfrom all the other categories and to make the representations\\nmore hierarchical, semantic, and informative for visual recog-\\nnition. Usually, the supported vector machine (SVM) [22],\\nAdaBoost [23], and deformable part-based model (DPM) [24]\\nare good choices. Among these classiﬁers, the DPM is a\\nﬂexible model by combining object parts with deformation\\ncost to handle severe deformations. In DPM, with the aid\\nof a graphical model, carefully designed low-level features'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ﬂexible model by combining object parts with deformation\\ncost to handle severe deformations. In DPM, with the aid\\nof a graphical model, carefully designed low-level features\\nand kinematically inspired part decompositions are combined.\\n2162-237X © 2019 IEEE. Personal u se is perm itted, but republication/redistribution requires IEEE permission.\\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nFig. 1. Application dom ains of object detection.\\nDiscriminative learning of graphical models allows for build-\\ning high-precision part-based models for a variety of object\\nclasses.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state-of-the-art results have\\nbeen obtained on PASCAL visual object classes (VOC) object\\ndetection competition [25] and real-time embedded systems\\nhave been obtained with a low burden on hardware. However,\\nsmall gains are obtained during 2010–2012 by only building\\nensemble systems and employing minor variants of successful\\nmethods [15]. This fact is due to the following reasons: 1) the\\ngeneration of candidate bounding boxes (BBs) with a sliding\\nwindow strategy is redundant, inefﬁcient, and inaccurate and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='methods [15]. This fact is due to the following reasons: 1) the\\ngeneration of candidate bounding boxes (BBs) with a sliding\\nwindow strategy is redundant, inefﬁcient, and inaccurate and\\n2) the semantic gap cannot be bridged by the combination\\nof manually engineered low-level descriptors and discrimina-\\ntively trained shallow models.\\nThanks to the emergency of deep neural networks\\n(DNNs) [6], [26], [S7], a more signiﬁcant gain is obtained\\nwith the introduction of regions with convolutional neural\\nnetwork (CNN) features (R-CNN) [15]. DNNs, or the most\\nrepresentative CNNs, act in a quite different way from tra-\\nditional approaches. They have d eeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso, the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [27].\\nSince the proposal of R-CNN, a great deal of improved'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='learn informative object representations without the need to\\ndesign features manually [27].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including fast R-CNN that\\njointly optimizes classiﬁcation and bounding box regres-\\nsion tasks [16], faster R-CNN that takes an additional sub-\\nnetwork to generate region proposals [17], and you only\\nlook once (YOLO) that accomplishes object detection via a\\nﬁxed-grid regression [18]. All of them bring different degrees\\nof detection performance improvements over the primary\\nR-CNN and make real-time and accurate object detection more\\nachievable.\\nIn this paper, a systematic review is provided to\\nsummarize representative models and their different char-\\nacteristics in several application domains, including generic\\nobject detection [15]–[17], salient object detection [28], [29],\\nface detection [30]–[32], and pedestrian detection [33], [34].\\nTheir relationships are depicted in Fig. 1. Based on basic'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object detection [15]–[17], salient object detection [28], [29],\\nface detection [30]–[32], and pedestrian detection [33], [34].\\nTheir relationships are depicted in Fig. 1. Based on basic\\nCNN architectures, the generic object detection is achieved\\nwith bounding box regression, while salient object detec-\\ntion is accomplished with local contrast enhancement and\\npixel-level segmentation. Face detection and pedestrian detec-\\ntion are closely related to ge neric object detection and\\nmainly accomplished with multiscale adaption and multi-\\nfeature fusion/boosting forest, respectively. The dotted lines\\nindicate that the corresponding domains are associated with\\neach other under certain conditions. It should be noticed\\nthat the covered domains are diversiﬁed. Pedestrian and face\\nimages have regular structures, while general objects and scene\\nimages have more complex variations in geometric structures\\nand layouts. Therefore, different deep models are required by\\nvarious images.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='images have more complex variations in geometric structures\\nand layouts. Therefore, different deep models are required by\\nvarious images.\\nThere has been a relevant pion eer effort [35] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classiﬁcation and object detection but\\npays little attention on detailing speciﬁc algorithms. Different\\nfrom it, our work not only reviews deep learning-based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section II,\\na brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section III. Then, reviews\\nof CNN applied in several speciﬁc tasks, including salient\\nobject detection, face detection, and pedestrian detection, are'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tion architectures are presented in Section III. Then, reviews\\nof CNN applied in several speciﬁc tasks, including salient\\nobject detection, face detection, and pedestrian detection, are\\nexhibited in Section IV–VI, respectively. Several promising\\nfuture directions are proposed in Section VII. At last, some\\nconcluding remarks are presented in Section VIII.\\nII. B\\nRIEF OVERVIEW OF DEEP LEARNING\\nPrior to an overview on deep learning-based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. History: Birth, Decline, and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date\\nback to the 1940s [36], and the original intention was to\\nsimulate the human brain system to solve general learning\\nproblems in a principled way. It was popular in the 1980s and\\n1990s with the proposal of the back-propagation algorithm'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='simulate the human brain system to solve general learning\\nproblems in a principled way. It was popular in the 1980s and\\n1990s with the proposal of the back-propagation algorithm\\nby Rumelhart et al. [37]. However, due to the overﬁtting of\\ntraining, lack of large-scale training data, limited computation\\npower, and insigniﬁcance in performance compared with other\\nmachine learning tools, neural networks fell out of fashion in\\nthe early 2000s.\\nDeep learning has become popular since 2006 [26], [S7],\\nwith a breakthrough in speech recognition [38]. The recovery\\nof deep learning can be attributed to the following factors.\\n1) The emergence of large-scale annotated training data,\\nsuch as ImageNet [39], to fully exhibit its very large\\nlearning capacity.\\n2) Fast development of high-performance parallel comput-\\ning systems, such as GPU clusters.\\n3) Signiﬁcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ing systems, such as GPU clusters.\\n3) Signiﬁcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npretraining guided by autoencoder [40] or restricted\\nBoltzmann machine [41], a good initialization is pro-\\nvided. With dropout and data augmentation, the over-\\nﬁtting problem in training has been relieved [6], [42].'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 3\\nWith batch normalization (BN), the training of very\\nDNNs becomes quite efﬁcient [43]. Meanwhile, various\\nnetwork structures, such as AlexNet [6], Overfeat [44],\\nGoogLeNet [45], Visual Geometry Group (VGG) [46],\\nand Residual Net (ResNet) [47], have been extensively\\nstudied to improve the performance.\\nWhat prompts deep learning to have a huge impact on\\nthe entire academic commun ity? It may owe to the con-\\ntribution of Hinton’s group, whose continuous efforts have\\ndemonstrated that deep learning would bring a revolutionary\\nbreakthrough on grand challenges rather than just obvious\\nimprovements on small data sets. Their success results from\\ntraining a large CNN on 1.2 million labeled images together\\nwith a few techniques [6] [e.g., rectiﬁed linear unit (ReLU)'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='improvements on small data sets. Their success results from\\ntraining a large CNN on 1.2 million labeled images together\\nwith a few techniques [6] [e.g., rectiﬁed linear unit (ReLU)\\noperation [48] and “dropout” regularization].\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [27].\\nA typical CNN architecture, which is referred to as VGG16,\\ncan be found in Fig. S1 in the supplementary material. Each\\nlayer of CNN is known as a feature map. The feature map\\nof the input layer is a 3-D matrix of pixel intensities for\\ndifferent color channels (e.g., RGB). The feature map of\\nany internal layer is an induced multichannel image, whose\\n“pixel” can be viewed as a speciﬁc feature. Every neu-\\nron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive ﬁeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as ﬁltering and pooling. Filtering (convolution)'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='from the previous layer (receptive ﬁeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as ﬁltering and pooling. Filtering (convolution)\\noperation convolutes a ﬁlter matrix (learned weights) with\\nthe values of a receptive ﬁeld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain ﬁnal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling, and local contrast normalization [52],\\nsummarizes the responses of a receptive ﬁeld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an ini-\\ntial feature hierarchy is constructed, which can be ﬁne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the ﬁnal layer with different activation functions [6]\\nis added to get a speciﬁc conditional probability for each'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='layers to adapt to different visual tasks. According to the tasks\\ninvolved, the ﬁnal layer with different activation functions [6]\\nis added to get a speciﬁc conditional probability for each\\noutput neuron. The whole network can be optimized on an\\nobjective function (e.g., mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 co nvolutional (conv) layers,\\n3 FC layers, 3 max-pooling layers, and a softmax classiﬁcation\\nlayer. The conv feature maps are produced by convoluting\\n3*3 ﬁlter windows, and feature map resolutions are reduced\\nwith 2 stride max-pooling layers. An arbitrary test image of the\\nsame size as training samples can be processed with the trained\\nnetwork. Rescaling or cropping operations may be needed if\\ndifferent sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarized as follows.\\n1) Hierarchical feature rep resentation, which is the'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='different sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarized as follows.\\n1) Hierarchical feature rep resentation, which is the\\nmultilevel representations from pixel to high-level\\nsemantic features learned by a hierarchical multistage\\nFig. 2. Two types of frameworks: region proposal based and\\nregression/classiﬁcation based. SPP: spatial pyramid pooling [64], FRCN:\\nfaster R-CNN [16], RPN: region pr oposal network [17], FCN: fully con-\\nvolutional network [65], BN: batch nor malization [43], and Deconv layers:\\ndeconvolution layers [54] .\\nstructure [15], [53], can be learned from data automati-\\ncally and hidden factors of input data can be disentan-\\ngled through multilevel nonlinear mappings.\\n2) Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expres-\\nsive capability.\\n3) The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g., fast'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='architecture provides an exponentially increased expres-\\nsive capability.\\n3) The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g., fast\\nR-CNN combines classiﬁcation and bounding box\\nregression into a multitask learning manner).\\n4) Beneﬁtting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can\\nbe recast as high-dimensiona l data transform problems\\nand solved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research ﬁelds, such as image superresolu-\\ntion reconstruction [54], [55], image classiﬁcation [5], [56],\\nimage retrieval [57], [58], face recognition [8], [S5], pedes-\\ntrian detection [59]–[61], and video analysis [62], [63].\\nIII. G\\nENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image and labeling them with\\nrectangular BBs to show the conﬁdences of existence. The'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image and labeling them with\\nrectangular BBs to show the conﬁdences of existence. The\\nframeworks of generic object detection methods can mainly\\nbe categorized into two types (see Fig. 2). One follows the tra-\\nditional object detection pipeline, generating region proposals\\nat ﬁrst and then classifying each proposal into different object\\ncategories. The other regards object detection as a regression\\nor classiﬁcation problem, adopting a uniﬁed framework to\\nachieve ﬁnal results (categories and locations) directly. The\\nregion proposal-based methods mainly include R-CNN [15],\\nspatial pyramid pooling (SPP)-net [64], Fast R-CNN [16],\\nFaster R-CNN [17], region-based fully convolutional network\\n(R-FCN) [65], feature pyramid networks (FPN) [66], and\\nMask R-CNN [67], some of which are correlated with each\\nother (e.g., SPP-net modiﬁes R-CNN with an SPP layer).'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='(R-FCN) [65], feature pyramid networks (FPN) [66], and\\nMask R-CNN [67], some of which are correlated with each\\nother (e.g., SPP-net modiﬁes R-CNN with an SPP layer).\\nThe regression /classiﬁcation-based methods mainly include\\nMultiBox [68], AttentionNet [69], G-CNN [70], YOLO [18],\\nSingle Shot MultiBox Detector (SSD) [71], YOLOv2 [72],\\ndeconvolutional single shot detector (DSSD) [73], and deeply\\nsupervised object detectors (DSOD) [74]. The correlations\\nbetween these two pipelines are bridged by the anchors\\nintroduced in Faster R-CNN. Details of these methods are as\\nfollows.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n4 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nFig. 3. Flowchart of R-CNN [15], which consists of three stages: 1) extracts\\nBU region proposals, 2) computes features for each proposal using a CNN,\\nand then 3) classiﬁes each region with class-speciﬁc linear SVMs.\\nA. Region Proposal-Based Framework\\nThe region proposal-based framework, a two-step process,\\nmatches the attentional mechanism of the human brain to\\nsome extent, which gives a coarse scan of the whole scenario\\nﬁrst and then focuses on regions of interest (RoIs). Among\\nthe prerelated works [44], [75], [76], the most representative\\none is Overfeat [44]. This model inserts CNN into the sliding\\nwindow method, which predicts BBs directly from locations\\nof the topmost feature map after obtaining the conﬁdences of\\nunderlying object categories.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='window method, which predicts BBs directly from locations\\nof the topmost feature map after obtaining the conﬁdences of\\nunderlying object categories.\\n1) R-CNN: It is of signiﬁcance to improve the quality\\nof candidate BBs and to take a deep architecture to extract\\nhigh-level features. To solve these problems, R-CNN was\\nproposed by Girshick et al. [15] and obtained a mean average\\nprecision (mAP) of 53 .3% with more than 30% improvement\\nover the previous best result (DPM histograms of sparse\\ncodes [77]) on PASCAL VOC 2012. Fig. 3 shows the ﬂow-\\nchart of R-CNN, which can be divided into three stages as\\nfollows.\\na) Region Proposal Generation: The R-CNN adopts\\nselective search [78] to generate about 2000 region proposals\\nfor each image. The selective search method relies on simple\\nbottom-up (BU) grouping and saliency cues to provide more\\naccurate candidate boxes of arbitrary sizes quickly and to\\nreduce the searching space in object detection [24], [39].'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='bottom-up (BU) grouping and saliency cues to provide more\\naccurate candidate boxes of arbitrary sizes quickly and to\\nreduce the searching space in object detection [24], [39].\\nb) CNN-Based Deep Feature Extraction: In this stage,\\neach region proposal is warped or cropped into a ﬁxed\\nresolution, and the CNN module in [6] is utilized to extract\\na 4096-dimensional feature as the ﬁnal representation. Due\\nto large learning capacity, dominant expressive power, and\\nhierarchical structure of CNNs, a high-level, semantic, and\\nrobust feature representation for each region proposal can be\\nobtained.\\nc) Classiﬁcation and Localization: With pretrained\\ncategory-speciﬁc linear SVMs for multiple classes, different\\nregion proposals are scored on a set of positive regions and\\nbackground (negative) regions. The scored regions are then\\nadjusted with bounding box regression and ﬁltered with a\\ngreedy nonmaximum suppression (NMS) to produce ﬁnal BBs\\nfor preserved object locations.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='adjusted with bounding box regression and ﬁltered with a\\ngreedy nonmaximum suppression (NMS) to produce ﬁnal BBs\\nfor preserved object locations.\\nWhen there are scarce or insufﬁcient labeled data,\\npretraining is usually conducted. Instead of unsupervised\\npretraining [79], R-CNN ﬁrst conducts supervised pretraining\\non ImageNet Large-Scale Visual Recognition Competition,\\na very large auxiliary data set, and then takes a domain-speciﬁc\\nﬁne-tuning. This scheme has been adopted by most of the\\nsubsequent approaches [16], [17].\\nIn spite of its improvements over traditional methods and\\nsigniﬁcance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n1) Due to the existence of FC layers, the CNN requires a\\nﬁxed size (e.g., 227 × 227) input image, which directly\\nleads to the recomputation of the whole CNN for each\\nevaluated region, taking a great deal of time in the testing\\nperiod.\\n2) Training of R-CNN is a multistage pipeline. At ﬁrst,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='leads to the recomputation of the whole CNN for each\\nevaluated region, taking a great deal of time in the testing\\nperiod.\\n2) Training of R-CNN is a multistage pipeline. At ﬁrst,\\na convolutional network (ConvNet) on object proposals\\nis ﬁne-tuned. Then, the softmax classiﬁer learned by\\nﬁne-tuning is replaced by SVMs to ﬁt in with ConvNet\\nfeatures. Finally, bounding-box regressors are trained.\\n3) Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on\\nthe disk. It will take a long time to process a relatively\\nsmall training set with very deep networks, such as\\nVGG16. At the same time, the storage memory required\\nby these features should also be a matter of concern.\\n4) Although selective search can generate region propos-\\nals with relatively high recalls, the obtained region\\nproposals are still redundant and this procedure is\\ntime-consuming (around 2 s to extract 2000 region\\nproposals).\\nTo solve these problems, many methods have been'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='proposals are still redundant and this procedure is\\ntime-consuming (around 2 s to extract 2000 region\\nproposals).\\nTo solve these problems, many methods have been\\nproposed. Geodesic object proposals [80] takes a much faster\\ngeodesic-based segmentation to replace traditional graph\\ncuts. Mutiscale combinatorial grouping [81] searches different\\nscales of the image for multiple hierarchical segmentations and\\ncombinatorially groups different regions to produce proposals.\\nInstead of extracting visually distinct segments, the edge boxes\\nmethod [82] adopts the idea that objects are more likely to\\nexist in BBs with fewer contours straggling their boundaries.\\nAlso, some studies tried to r erank or reﬁne preextracted\\nregion proposals to remove unn ecessary ones and obtained a\\nlimited number of valuable ones, such as DeepBox [83] and\\nSharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='limited number of valuable ones, such as DeepBox [83] and\\nSharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na Bayesian optimization-based search algorithm to guide\\nthe regressions of different BBs sequentially and trained\\nclass-speciﬁc CNN classiﬁers with a structured loss to penal-\\nize the localization inaccuracy explicitly. Gupta et al. [86]\\nimproved object detection for RGB-D images with seman-\\ntically rich image and depth features and learned a new\\ngeocentric embedding for dep th images to encode each pixel.\\nThe combination of object detectors and superpixel classi-\\nﬁcation framework gains a promising result on the seman-\\ntic scene segmentation task. Ouyang et al. [87] proposed a\\ndeformable deep CNN (DeepID-Net) that introduces a novel\\ndeformation constrained pooling (def-pooling) layer to impose\\ngeometric penalty on the deformation of various object parts'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='deformable deep CNN (DeepID-Net) that introduces a novel\\ndeformation constrained pooling (def-pooling) layer to impose\\ngeometric penalty on the deformation of various object parts\\nand makes an ensemble of models with different settings.\\nLenc and Vedaldi [88] provided an analysis on the role of\\nproposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth BBs with K -means clustering.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 5\\nFig. 4. Architecture of SPP-net for object detection [64].\\nHowever, more candidate boxes are required to achieve com-\\nparable results to those of R-CNN.\\n2) SPP-Net: FC layers must take a ﬁxed-size input. That\\nis why R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. [64] took the theory of\\nspatial pyramid matching (SPM) [89], [90] into consideration\\nand proposed a novel CNN architecture named SPP-net. SPM\\ntakes several ﬁner to coarser scales to partition the image into'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='spatial pyramid matching (SPM) [89], [90] into consideration\\nand proposed a novel CNN architecture named SPP-net. SPM\\ntakes several ﬁner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Fig. 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the ﬁfth conv layer (conv5) to project region\\nproposals of arbitrary sizes to ﬁxed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses but also have relationships with their spatial\\npositions [64]. The layer after the ﬁnal conv layer is referred to\\nas the SPP layer. If the number of feature maps in conv5 is 256,\\ntaking a three-level pyramid, the ﬁnal feature vector for each\\nregion proposal obtained after the SPP layer has a dimension\\nof 256 × (1\\n2 + 22 + 42) = 5376.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='taking a three-level pyramid, the ﬁnal feature vector for each\\nregion proposal obtained after the SPP layer has a dimension\\nof 256 × (1\\n2 + 22 + 42) = 5376.\\nSPP-net not only gains better results with a correct estima-\\ntion of different region proposals in their corresponding scales\\nbut also improves detection efﬁciency in the testing period\\nwith the sharing of computation cost before SPP layer among\\ndifferent proposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and efﬁciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost the\\nsame multistage pipeline as R-CNN, including feature extrac-\\ntion, network ﬁne-tuning, SVM training, and bounding-box\\nregressor ﬁtting. Therefore, an additional expense on storage\\nspace is still required. In addition, the conv layers preceding\\nthe SPP layer cannot be updated with the ﬁne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='space is still required. In addition, the conv layers preceding\\nthe SPP layer cannot be updated with the ﬁne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multitask loss on classiﬁca tion and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Fig. 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a ﬁxed-length feature\\nvector is extracted from each region proposal with an RoI\\nFig. 5. Architecture of Fast R-CNN [16].\\npooling layer. The RoI pooling l ayer is a special case of the\\nSPP layer, which has only one pyramid level. Each feature\\nvector is then fed into a sequence of FC layers before ﬁnally\\nbranching into two sibling output layers. One output layer is\\nresponsible for producing softmax probab ilities for all C + 1'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='vector is then fed into a sequence of FC layers before ﬁnally\\nbranching into two sibling output layers. One output layer is\\nresponsible for producing softmax probab ilities for all C + 1\\ncategories ( C object classes plus one “background” class)\\nand the other output layer encodes reﬁned bounding-box\\npositions with four real-valued numbers. All parameters in\\nthese procedures (except the generation of region proposals)\\nare optimized via a multitask loss in an end-to-end way.\\nThe multitasks loss L is deﬁned in the following to jointly\\ntrain classiﬁcation and bounding-box regression:\\nL(p,u,t\\nu ,v) = Lcls(p,u) + λ[u ≥ 1]Lloc(tu ,v) (1)\\nwhere Lcls(p,u) =− log pu calculates the log loss for ground\\ntruth class u,a n d pu is driven from the discrete probability\\ndistribution p = (p0,··· , pC ) over the C +1 outputs from the\\nlast FC layer. Lloc(tu ,v) is deﬁned over the predicted offsets\\ntu = (tu\\nx ,tu\\ny ,tu\\nw,tu\\nh ) and ground-truth bounding-box regression'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='distribution p = (p0,··· , pC ) over the C +1 outputs from the\\nlast FC layer. Lloc(tu ,v) is deﬁned over the predicted offsets\\ntu = (tu\\nx ,tu\\ny ,tu\\nw,tu\\nh ) and ground-truth bounding-box regression\\ntargets v = (vx ,v y ,v w,v h ),w h e r ex, y,w, and h denote\\nthe two coordinates of the box center, width, and height,\\nrespectively. Each t\\nu adopts the parameter settings in [15] to\\nspecify an object proposal with a log-space height/width shift\\nand scale-invariant translati on. The Iverson bracket indicator\\nfunction [u ≥ 1] is employed to omit all background RoIs.\\nTo provide more robustness against outliers and eliminate the\\nsensitivity in exploding gradients, a smooth L\\n1 loss is adopted\\nto ﬁt bounding-box regressors as follows:\\nLloc(tu,v) =\\n∑\\ni∈x,y,w,h\\nsmoothL1\\n(tu\\ni − vi\\n) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if |x| < 1\\n|x|−0 .5o t h e r w i s e. (3)\\nTo accelerate the pipeline of Fast R-CNN, another two\\ntricks are of necessity. On the one hand, if training sam-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='i − vi\\n) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if |x| < 1\\n|x|−0 .5o t h e r w i s e. (3)\\nTo accelerate the pipeline of Fast R-CNN, another two\\ntricks are of necessity. On the one hand, if training sam-\\nples (i.e., RoIs) come from different images, backpropagation\\nthrough the SPP layer becomes highly inefﬁcient. Fast R-CNN\\nsamples minibatches hi erarchically, namely, N images sam-\\npled randomly at ﬁrst and then R/N RoIs sampled in each\\nimage, where R represents the number of RoIs. Critically,\\ncomputation and memory are shared by RoIs from the same\\nimage in the forward and backward pass. On the other hand,\\nmuch time is spent in computing the FC layers during the\\nforward pass [16]. The truncated singular value decomposition\\n(SVD) [91] can be utilized to compress large FC layers and\\nto accelerate the testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to accelerate the testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single stage with a multitask loss. It saves the additional'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nFig. 6. RPN in Faster R-CNN [17]. K predeﬁned anchor boxes are\\nconvoluted with each sliding window to produce ﬁxed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nexpense on storage space and improves both accuracy and\\nefﬁciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such\\nas selective search and Edgebox, to generate a candidate pool\\nof isolated region proposals. Region proposal computation\\nis also a bottleneck in improving efﬁciency. To solve this\\nproblem, Ren et al. [17], [92] introduced an additional region\\nproposal network (RPN), which acts in a nearly cost-free way'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='is also a bottleneck in improving efﬁciency. To solve this\\nproblem, Ren et al. [17], [92] introduced an additional region\\nproposal network (RPN), which acts in a nearly cost-free way\\nby sharing full-image conv features with detection network.\\nRPN is achieved with an FCN, which has the ability to\\npredict object bounds and scores at each position simultane-\\nously. Similar to [78], RPN takes an image of arbitrary size to\\ngenerate a set of rectangular object proposals. RPN operates\\non a speciﬁc conv layer with th e preceding layers shared with\\nthe object detection network.\\nThe architecture of RPN is shown in Fig. 6. The network\\nslides over the conv feature map and fully connects to an n×n\\nspatial window. A low-dimensional vector (512-dimensional\\nfor VGG16) is obtained in each s liding window and fed into\\ntwo sibling FC layers, namely, box-classiﬁcation layer (cls)\\nand box-regression layer (reg). This architecture is imple-\\nmented with an n × n conv layer followed by two sibling'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='two sibling FC layers, namely, box-classiﬁcation layer (cls)\\nand box-regression layer (reg). This architecture is imple-\\nmented with an n × n conv layer followed by two sibling\\n1 × 1 conv layers. To increase nonlinearity, ReLU is applied\\nto the output of the n × n conv layer.\\nThe regressions toward true BBs are achieved by comparing\\nproposals relative to reference boxes (anchors). In the Faster\\nR-CNN, anchors of three scales and three aspect ratios are\\nadopted. The loss function is similar to (1)\\nL(p\\ni ,ti ) = 1\\nNcls\\n∑\\ni\\nLcls\\n(\\npi , p∗\\ni\\n)\\n+ λ 1\\nNreg\\n∑\\ni\\np∗\\ni Lreg\\n(\\nti ,t∗\\ni\\n)\\n(4)\\nwhere pi is the predicted probability of the ith anchor being an\\nobject. The ground truth label p∗\\ni is 1 if the anchor is positive,\\notherwise 0. ti stores four parameterized coordinates of the\\npredicted bounding box while t∗\\ni is related to the ground-truth\\nbox overlapping with a positive anchor.Lcls is a binary log loss\\nand Lreg is a smoothed L1 loss similar to (2). These two terms'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='predicted bounding box while t∗\\ni is related to the ground-truth\\nbox overlapping with a positive anchor.Lcls is a binary log loss\\nand Lreg is a smoothed L1 loss similar to (2). These two terms\\nare normalized with the minibatch size ( Ncls) and the number\\nof anchor locations ( Nreg), respectively. In the form of FCNs,\\nFaster R-CNN can be trained end-to-end by backpropagation\\nand SGD in an alternate training manner.\\nWith the proposal of Faster R-CNN, region proposal-based\\nCNN architectures for object detection can really be trained in\\nan end-to-end way. Also, a frame rate of 5 frames per second\\n(fps) on a GPU is achieved with the state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces objectlike regi ons (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a preva-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='instead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a preva-\\nlent family [16], [17] of deep networks for object detection\\nis composed of two subnetworks: a shared fully convolu-\\ntional subnetwork (independe nt of RoIs) and an unshared\\nRoI-wise subnetwork. This decomposition originates from\\npioneering classiﬁcation archit ectures (e.g., AlexNet [6] and\\nVGG16 [46]) which consist of a convolutional subnetwork and\\nseveral FC layers separated by a s peciﬁc spatial pooling layer.\\nRecent state-of-the-art image c lassiﬁcation networks, such\\nas ResNets [47] and GoogLeNets [45], [93], are fully convo-\\nlutional. To adapt to these architectures, it is natural to con-\\nstruct a fully convolutional object detection network without\\nRoI-wise subnetwork. However, it turns out to be inferior\\nwith such a naive solution [47]. This inconsistency is due\\nto the dilemma of respecting translation variance in object'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RoI-wise subnetwork. However, it turns out to be inferior\\nwith such a naive solution [47]. This inconsistency is due\\nto the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classiﬁcation. In other words, shifting an object inside\\nan image should be indiscriminative in image classiﬁcation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregionwise layers. Therefore, Dai et al. [65] proposed an\\nR-FCNs (see Fig. S2 in the supplementary material).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k\\n2 position-sensitive\\ns c o r em a p sw i t haﬁ x e dg r i do fk × k ﬁrst and a position-\\nsensitive RoI pooling layer is then appended to aggregate\\nthe responses from these score maps. Finally, in each'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='s c o r em a p sw i t haﬁ x e dg r i do fk × k ﬁrst and a position-\\nsensitive RoI pooling layer is then appended to aggregate\\nthe responses from these score maps. Finally, in each\\nRoI, k\\n2 position-sensitive scores are averaged to produce a\\nC + 1-d vector and softmax responses across categories are\\ncomputed. Another 4 k2-d conv layer is appended to obtain\\nclass-agnostic BBs.\\nWith R-FCN, more powerful classiﬁcation networks\\ncan be adopted to accomplish object detection in a fully\\nconvolutional architecture by sharing nearly all the layers,\\nand the state-of-the-art results are obtained on both PASCAL\\nVOC and Microsoft COCO [94] data sets at a test speed\\nof 170 ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in many\\nobject detection systems to improve scale invariance [24], [64]\\n[Fig. 7(a)]. However, training time and memory consumption\\nincrease rapidly. To this end, some techniques take only'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object detection systems to improve scale invariance [24], [64]\\n[Fig. 7(a)]. However, training time and memory consumption\\nincrease rapidly. To this end, some techniques take only\\na single input scale to represent high-level semantics and\\nincrease the robustness to scale changes [Fig. 7(b)], and image\\npyramids are built at test time which results in an inconsistency\\nbetween train/test-time infer ences [16], [17]. The in-network\\nfeature hierarchy in a deep ConvNet produces feature maps of\\ndifferent spatial resolutions while introduces large semantic\\ngaps caused by different depths [Fig. 7(c)]. To avoid using\\nlow-level features, pioneer works [71], [95] usually build the\\npyramid starting from middle layers or just sum transformed'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 7\\nFig. 7. Main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single-scale features are adopted for\\nfaster detection. (c) Alternative to the featurized image pyramid is to reuse\\nthe pyramidal feature hierarchy com puted by a ConvNet. (d) FPN integrates\\nboth (b) and (c). Blue outlines indicate feature maps and thicker outlines\\ndenote semantically stronger features.\\nfeature responses, missing the higher resolution maps of the\\nfeature hierarchy.\\nDifferent from these approaches, FPN [66] holds an archi-\\ntecture with a BU pathway, a top-down (TD) pathway and\\nseveral lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and semanti-\\ncally weak features [Fig. 7(d)]. The BU pathway, which is the'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='several lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and semanti-\\ncally weak features [Fig. 7(d)]. The BU pathway, which is the\\nbasic forward backbone ConvNet, produces a feature hierarchy\\nby downsampling the corresponding feature maps with a stride\\nof 2. The layers owning the same size of output maps are\\ngrouped into the same network stage and the output of the last\\nlayer of each stage is chosen as the reference set of feature\\nmaps to build the following TD pathway.\\nTo build the TD pathway, feature maps from higher network\\nstages are upsampled at ﬁrst and then enhanced with those of\\nthe same spatial size from the BU pathway via lateral connec-\\ntions. A 1 × 1 conv layer is appended to the upsampled map\\nto reduce channel dimensions and the mergence is achieved\\nby elementwise addition. Finally, a 3 × 3 convolution is also\\nappended to each merged map to reduce the aliasing effect'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to reduce channel dimensions and the mergence is achieved\\nby elementwise addition. Finally, a 3 × 3 convolution is also\\nappended to each merged map to reduce the aliasing effect\\nof upsampling and the ﬁnal feature map is generated. This\\nprocess is iterated until the ﬁnest resolution map is generated.\\nAs feature pyramid can extract rich semantics from all levels\\nand be trained end to end with all scales, the state-of-the-\\nart representation can be obtained without sacriﬁcing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g., region proposal generation) and to many\\nother computer vision tasks (e.g., instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ing task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nThe multitask scheme will create spurious edge and exhibit\\nsystematic errors on overlapping instances [98]. To solve this\\nproblem, parallel to the existing branches in Faster R-CNN\\nfor classiﬁcation and bounding box regression, the Mask R-\\nCNN [67] adds a branch to predict segmentation masks in a\\npixel-to-pixel manner (Fig. 8).\\nDifferent from the other two branches that are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m × m mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nFig. 8. Mask R-CNN framework fo r instance segmentation [67].\\nrepresentation requires fewer parameters but is more accurate\\nthan that in [97]. Formally, besides the two losses in (1) for'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Fig. 8. Mask R-CNN framework fo r instance segmentation [67].\\nrepresentation requires fewer parameters but is more accurate\\nthan that in [97]. Formally, besides the two losses in (1) for\\nclassiﬁcation and bounding box regression, an additional loss\\nfor segmentation mask branch is deﬁned to reach a multitask\\nloss. This loss is only associated with ground-truth class and\\nrelies on the classiﬁcation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classiﬁcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely, RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='adopts a simple and quantization-free layer, namely, RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch\\nonly adds a small computationa l burden and its cooperation\\nwith other tasks provides complementary information for\\nobject detection. As a result, Mask R-CNN is simple to\\nimplement with promising instance segmentation and object\\ndetection results. In a word, Mask R-CNN is a ﬂexible\\nand efﬁcient framework for instance-level recognition, which\\ncan be easily generalized to other tasks (e.g., human pose\\nestimation [7], [S4]) with minimal modiﬁcation.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and efﬁcient framework for instance-level recognition, which\\ncan be easily generalized to other tasks (e.g., human pose\\nestimation [7], [S4]) with minimal modiﬁcation.\\n8) Multitask Learning, Multiscale Representation, and Con-\\ntextual Modeling: Although the Faster R-CNN gets promising\\nresults with several hundred proposals, it still struggles in\\nsmall-size object detection and localization, mainly due to\\nthe coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO data set which consists\\nof objects at a broad range of scal es, less prototypical images,\\nand requires more precise loca lization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmultitask learning [100], multiscale representation [95], and\\ncontext modeling [101] to combine complementary informa-\\ntion from multiple sources.\\nMultitask learning learns a useful representation for mul-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='context modeling [101] to combine complementary informa-\\ntion from multiple sources.\\nMultitask learning learns a useful representation for mul-\\ntiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. [100] introduced conv features trained for\\nobject segmentation and “stuff” (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet). Dai et al. [97] presented multitask network\\ncascades of three networks, namely, class-agnostic region\\nproposal generation, pixel-level instance segmentation, and\\nregional instance classiﬁcation. Li et al. [104] incorporated the'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n8 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nweakly supervised object segmentation cues and region-based\\nobject detection into a multistage architecture to fully exploit\\nthe learned segmentation features.\\nMultiscale representation combines activations from\\nmultiple layers with skip-lay er connections to provide\\nsemantic information of different spatial resolutions [66].\\nCai et al. [105] proposed the multiscale CNN (MS-CNN)\\nto ease the inconsistency between the sizes of objects\\nand receptive ﬁelds with multiple scale-independent\\noutput layers. Yang et al. [34] investigated two strategies,\\nnamely, scale-dependent pooling (SDP) and layerwise\\ncascaded rejection classiﬁers (CRCs), to exploit appropriate\\nscale-dependent conv features. Kong et al. [101] proposed\\nthe HyperNet to calculate the shared features between RPN'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='cascaded rejection classiﬁers (CRCs), to exploit appropriate\\nscale-dependent conv features. Kong et al. [101] proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps fro m different resolutions into a\\nuniform space.\\nContextual modeling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. [106] proposed the SegDeepM to\\nexploit object segmentation which reduces the dependency\\non initial candidate boxes with the Markov random ﬁeld.\\nMoysset et al. [108] took advantage of four directional 2-D\\nlong short-term memories (LSTMs) [107] to convey global\\ncontext between different local regions and reduced trainable\\nparameters with local parameter sharing. Zeng et al. [109] pro-\\nposed a novel gated bidirectional-net (GBD-Net) by introduc-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='context between different local regions and reduced trainable\\nparameters with local parameter sharing. Zeng et al. [109] pro-\\nposed a novel gated bidirectional-net (GBD-Net) by introduc-\\ning gated functions to control message transmission between\\ndifferent support regions.\\nThe combination incorporates different components above\\ninto the same model to improve detection performance fur-\\nther. Gidaris and Komodakis [110] proposed the multire-\\ngion CNN (MR-CNN) model to capture different aspects of\\nan object, the distinct appear ances of various object parts,\\nand semantic segmentation-aware features. To obtain con-\\ntextual and multiscale representations, Bell et al. [95] pro-\\nposed the inside–outside net (ION) by exploiting informa-\\ntion both inside and outside the RoI with spatial recurrent\\nneural networks [111] and skip pooling [101]. Zagoruyko\\net al. [112] proposed the MultiPath architecture by introducing\\nthree modiﬁcations to the Fast R-CNN, including multiscale'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='neural networks [111] and skip pooling [101]. Zagoruyko\\net al. [112] proposed the MultiPath architecture by introducing\\nthree modiﬁcations to the Fast R-CNN, including multiscale\\nskip connections [95], a modiﬁed foveal structure [110], and\\na novel loss function summing different intersection over\\nunion (IoU) losses.\\n9) Thinking in Deep Learning-Based Object Detection:\\nApart from the above-mentioned approaches, there are still\\nmany important factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. [113] proposed an effective online mining\\nalgorithm (OHEM) for automatic selection of the hard exam-\\nples, which leads to a more eff ective and efﬁcient training.\\nInstead of concentrating on feature extraction, Ren et al.\\n[114] made a detailed analysis on object classiﬁers and found\\nthat it is of particular importance for object detection to con-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Instead of concentrating on feature extraction, Ren et al.\\n[114] made a detailed analysis on object classiﬁers and found\\nthat it is of particular importance for object detection to con-\\nstruct a deep and convolutional per-region classiﬁer carefully,\\nespecially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signiﬁcant scale variation, occlusion,\\nor truncation, especially when only 2-D object detection is\\ninvolved. To address this problem, Xiang et al. [60] proposed\\na novel subcategory-aware RPN, which guides the generation\\nof region proposals with subcategory information related to\\nobject poses and jointly optimize object detection and subcat-\\negory classiﬁcation.\\nOuyang et al. [115] found that the samples from differ-\\nent classes follow a long-tailed distribution, which indicates\\nthat different classes with distinct numbers of samples have\\ndifferent degrees of impacts on feature learning. To this end,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ent classes follow a long-tailed distribution, which indicates\\nthat different classes with distinct numbers of samples have\\ndifferent degrees of impacts on feature learning. To this end,\\nobjects are ﬁrst clustered into visually similar class groups,\\nand then, a hierarchical feature learning scheme is adopted to\\nlearn deep representations for each group separately.\\nIn order to minimize the computational cost and achieve\\nthe state-of-the-art performance, with the “deep and thin”\\ndesign principle and following the pipeline of Fast R-CNN,\\nHong et al. [116] proposed the architecture of PV ANET,\\nwhich adopts some building blocks including concatenated\\nReLU [117], Inception [45], and HyperNet [101] to reduce\\nthe expense on multiscale feature extraction and trains the\\nnetwork with BN [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 fps).'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='rate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 fps).\\nB. Regression/Classiﬁcation-Based Framework\\nRegion proposal-based frameworks are composed of several\\ncorrelated stages, including region proposal generation, feature\\nextraction with CNN, classiﬁcation, and bounding box regres-\\nsion, which are usually trained separately. Even in the recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different component s becomes the bottleneck in the\\nreal-time application.\\nOne-step frameworks based on global regression/\\nclassiﬁcation, mapping straightly from image pixels to\\nbounding\\nbox coordinates and cl ass probabilities, can reduce\\ntime expense. We ﬁrst review some pioneer CNN models\\nand then focus on two signiﬁcant frameworks, namely,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='bounding\\nbox coordinates and cl ass probabilities, can reduce\\ntime expense. We ﬁrst review some pioneer CNN models\\nand then focus on two signiﬁcant frameworks, namely,\\nYOLO [18] and SSD [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiﬁcation task.\\nSzegedy et al. [118] formulated the object detection task\\nas a DNN-based regression, generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has difﬁculty in handling\\noverlapping objects, and BBs generated by direct upsampling\\nis far from perfect.\\nPinheiro et al. [119] proposed a CNN model with two\\nbranches: one generates class agnostic segmentation masks\\nand the other predicts the likelihood of a given patch centered\\non an object. Inference is efﬁcient since class scores and\\nsegmentation can be obtained in a single model with most\\nof the CNN operations shared.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 9\\nFig. 9. Main idea of YOLO [18].\\nErhan et al. [68] and Szegedy et al. [120] proposed the\\nregression-based MultiBox to produce scored class-agnostic\\nregion proposals. A uniﬁed loss was introduced to bias both\\nlocalization and conﬁdences of multiple components to predict\\nthe coordinates of class-agnostic BBs. However, a large num-\\nber of additional parameters are introduced to the ﬁnal layer.\\nYoo et al. [69] adopted an iterative classiﬁcation approach\\nto handle object detection and proposed an impressive end-\\nto-end CNN architecture named AttentionNet. Starting from\\nthe top-left and bottom-right corners of an image, Attention-\\nNet points to a target object by generating quantized weak\\ndirections and converges to an accurate object boundary box'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the top-left and bottom-right corners of an image, Attention-\\nNet points to a target object by generating quantized weak\\ndirections and converges to an accurate object boundary box\\nwith an ensemble of iterative predictions. However, the model\\nbecomes quite inefﬁcient when handling multiple categories\\nwith a progressive two-step procedure.\\nNajibi et al. [70] proposed a proposal-free iterative\\ngrid-based object detector (G-CNN), which models object\\ndetection as ﬁnding a path from a ﬁxed grid to boxes tightly\\nsurrounding the objects [70]. Starting with a ﬁxed multiscale\\nbounding box grid, G-CNN trains a regressor to move and\\nscale elements of the grid toward objects iteratively. However,\\nG-CNN has a difﬁculty in dealing with small or highly\\noverlapping objects.\\n2) YOLO: Redmon et al. [18] proposed a novel framework\\ncalled YOLO, which makes the use of the whole topmost\\nfeature map to predict both conﬁdences for multiple categories\\nand BBs. The basic idea of YOLO is exhibited in Fig. 9.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='called YOLO, which makes the use of the whole topmost\\nfeature map to predict both conﬁdences for multiple categories\\nand BBs. The basic idea of YOLO is exhibited in Fig. 9.\\nYOLO divides the input image into an S × S grid and each\\ngrid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B BBs and their\\ncorresponding conﬁdence scores. Formally, conﬁdence scores\\nare deﬁned as Pr(Object) ∗IOU\\ntruth\\npred , which indicates how likely\\nthere exist objects (Pr (Object) ≥ 0) and shows conﬁdences\\nof its prediction (IOU truth\\npred\\n). At the same time, regardless\\nof the number of boxes, C conditional class probabilities\\n(Pr(Classi |Object)) should also be predicted in each grid cell.\\nIt should be noticed that only the contribution from the grid\\ncell containing an obj ect is calculated.\\nAt test time, class-speciﬁc conﬁdence scores for each box\\nare achieved by multiplying the individual box conﬁdence'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='cell containing an obj ect is calculated.\\nAt test time, class-speciﬁc conﬁdence scores for each box\\nare achieved by multiplying the individual box conﬁdence\\npredictions with the conditional class probabilities as follows :\\nPr(Object) ∗ IOU\\ntruth\\npred ∗ Pr(Classi|Object)\\n= Pr(Classi) ∗ IOUtruth\\npred (5)\\nwhere the existing probability of class-speciﬁc objects in the\\nbox and the ﬁtness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized :\\nλ\\ncoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nobj\\nij [(xi −ˆxi )2 + (yi −ˆyi )2]\\n+ λcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nobj\\nij\\n[ ( √\\nwi −\\n√\\nˆwi )2 + (\\n√\\nhi −\\n√\\nˆhi\\n) 2]\\n+\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nobj\\nij\\n(Ci − ˆCi )2\\n+ λnoobj\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n/BD\\nnoobj\\nij\\n(Ci − ˆCi )2\\n+\\nS2\\n∑\\ni=0\\n/BD\\nobj\\ni\\n∑\\nc∈classes\\n(pi (c) −ˆpi (c))2. (6)\\nIn a certain cell i, (xi , yi ) denote the center of the box relative\\nto the bounds of the grid cell,(wi ,hi ) are the normalized width'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='+\\nS2\\n∑\\ni=0\\n/BD\\nobj\\ni\\n∑\\nc∈classes\\n(pi (c) −ˆpi (c))2. (6)\\nIn a certain cell i, (xi , yi ) denote the center of the box relative\\nto the bounds of the grid cell,(wi ,hi ) are the normalized width\\nand height relative to the image size, Ci represents the conﬁ-\\ndence scores, /BD\\nobj\\ni indicates the exist ence of objects, and /BD\\nobj\\nij\\ndenotes that the prediction is conducted by the jth bounding\\nbox predictor. Note that only when an object is present in\\nthat grid cell, the loss function penalizes classiﬁcation errors.\\nSimilarly, when the predictor is “responsible” for the ground\\ntruth box (i.e., the highest IoU of any predictor in that grid cell\\nis achieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 × 1 reduction layers followed by 3 × 3c o n v\\nlayers. The network can process images in real time at 45 fps\\nand a simpliﬁed version Fast YOLO can reach 155 fps'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='modules with 1 × 1 reduction layers followed by 3 × 3c o n v\\nlayers. The network can process images in real time at 45 fps\\nand a simpliﬁed version Fast YOLO can reach 155 fps\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on the background,\\nwhich makes the cooperation with Fast R-CNN become pos-\\nsible. An improved version, YOLOv2, was later proposed\\nin [72], which adopts several impressive strategies, such as\\nBN, anchor boxes, dimension cluster, and multiscale training.\\n3) SSD: YOLO has a difﬁculty in dealing with small\\nobjects in groups, which is caused by strong spatial con-\\nstraints imposed on bounding box predictions [18]. Mean-\\nwhile, YOLO struggles to generalize to objects in new/unusual\\naspect ratios/conﬁgurations an d produces relatively coarse\\nfeatures due to multiple downsampling operations.\\nAiming at these problems, Liu et al. [71] proposed an SSD,\\nwhich was inspired by the anchors adopted in MultiBox [68],'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='features due to multiple downsampling operations.\\nAiming at these problems, Liu et al. [71] proposed an SSD,\\nwhich was inspired by the anchors adopted in MultiBox [68],\\nRPN [17], and multiscale representation [95]. Given a speciﬁc\\nfeature map, instead of ﬁxed grids adopted in YOLO, the SSD\\ntakes the advantage of a set of default anchor boxes with\\ndifferent aspect ratios and scales to discretize the output space\\nof BBs. To handle objects with various sizes, the network\\nfuses predictions from multiple feature maps with different\\nresolutions.\\nThe architecture of SSD is demonstrated in Fig. 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales\\nand aspect ratios and their associated conﬁdences. The net-\\nwork is trained with a weighted sum of localization loss'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nTABLE I\\nOVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES\\nFig. 10. Architecture of SSD 300 [71]. SSD adds se veral feature layers to the end of VGG16 backbone ne twork to predict the offsets to default anchor\\nboxes and their associated conﬁdences. Final detection r esults are obtained by conducting NMS on multiscale reﬁned BBs.\\n(e.g., Smooth L1) and conﬁdence loss (e.g., Softmax), which\\nis similar to (1). Final detection results are obtained by\\nconducting NMS on multiscale reﬁned BBs.\\nIntegrating with hard negative mining, data augmentation,\\nand a larger number of carefully chosen default anchors,\\nSSD signiﬁcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO while being three\\ntimes faster. The SSD300 (input image size is 300 × 300)'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SSD signiﬁcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO while being three\\ntimes faster. The SSD300 (input image size is 300 × 300)\\nruns at 59 fps, which is more accurate and efﬁcient than\\nYOLO. However, SSD is not skilled at dealing with small\\nobjects, which can be relieved by adopting better feature\\nextractor backbone (e.g., ResNet101), adding deconvolution\\nlayers with skip connections to introduce additional large-scale\\ncontext [73], and designing better network structure (e.g., stem\\nblock and dense block) [74].\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark data sets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121], and Microsoft COCO [94].\\nThe evaluated approaches include R-CNN [15], SPP-\\nnet [64], Fast R-CNN [16], networks on convolutional\\nfeature maps (NOC) [114], Bayes [85], MR-CNN&\\nS-CNN [105], Faster R-CNN [17], HyperNet [101], ION [95],\\nMS-GR [104], StuffNet [100], SSD300 [71], SSD512 [71],'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='feature maps (NOC) [114], Bayes [85], MR-CNN&\\nS-CNN [105], Faster R-CNN [17], HyperNet [101], ION [95],\\nMS-GR [104], StuffNet [100], SSD300 [71], SSD512 [71],\\nOHEM [113], SDP +CRC [34], G-CNN [70], SubCNN [60],\\nGBD-Net [109], PV ANET [116], YOLO [18], YOLOv2 [72],\\nR-FCN [65], FPN [66], Mask R-CNN [67], DSSD [73],\\nand DSOD [74]. If no speciﬁc instructions for the adopted\\nframework are provided, the utilized model is a VGG16 [46]\\npretrained on 1000-way ImageNet classiﬁcation task [39].\\nDue to the limitation of the paper length, we only provide an\\noverview, including proposal, learning method, loss function,\\nprograming language, and platform, of the prominent\\narchitectures in Table I. Deta iled experimental settings, which\\ncan be found in the original papers, are missed. In addition\\nto the comparisons of detectio n accuracy, another comparison\\nis provided to evaluate their test consumption on PASCAL\\nVOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to the comparisons of detectio n accuracy, another comparison\\nis provided to evaluate their test consumption on PASCAL\\nVOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 data sets consist of 20 categories. The evaluation terms\\nare AP in each single category and mAP across all the 20 cat-\\negories. Comparative results are exhibited in Tables II and III,\\nfrom which the following remarks can be obtained.\\n1) If incorporated with a proper way, more powerful back-\\nbone CNN models can deﬁnitely improve the object\\ndetection performance (the comparison among R-CNN\\nwith AlexNet, R-CNN with VGG16 and SPP-net with\\nZF-Net [122]).\\n2) With the introduction of the SPP layer (SPP-net), end-\\nto-end multitask architecture (FRCN), and RPN (Faster\\nR-CNN), object detection performance is improved\\ngradually and apparently.\\n3) Due to a large number of trainable parameters, in order\\nto obtain multilevel robust features, data augmentation\\nis very important for deep learning-based models (Faster'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='gradually and apparently.\\n3) Due to a large number of trainable parameters, in order\\nto obtain multilevel robust features, data augmentation\\nis very important for deep learning-based models (Faster\\nR-CNN with “07,” “07 + 12,” and “07 + 12 + coco”).\\n4) Apart from basic models, there are still many\\nother factors affecting object detection performance,\\nsuch as multiscale and multiregion feature extrac-\\ntion (e.g., MR-CNN), modiﬁed classiﬁcation networks\\n(e.g., NOC), additional information from other corre-\\nlated tasks (e.g., StuffNet, HyperNet), multiscale rep-\\nresentation (e.g., ION), and mining of hard negative\\nsamples (e.g., OHEM).\\n5) As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN, and ﬁne-grained features,\\nthe localization errors ar e corrected (YOLOv2).'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN, and ﬁne-grained features,\\nthe localization errors ar e corrected (YOLOv2).\\n6) By combining many recent tricks and modeling the\\nwhole network as a fully convolutional one, R-FCN'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 11\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 T EST SET (%)\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 T EST SET (%)\\nachieves a more obvious improvement of detection per-\\nformance over other approaches.\\n2) Microsoft COCO: Microsoft COCO is composed\\nof 300 000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requi rement on object localization,\\nthis data set is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are given in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='different degrees of IoUs and on different object sizes. The\\nresults are given in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n1) Multiscale training and test are beneﬁcial in improv-\\ning object detection perform ance, which provide addi-\\ntional information in different resolutions (R-FCN).\\nFPN and DSSD provide some better ways to build\\nfeature pyramids to achieve multiscale representation.\\nThe complementary information from other related tasks\\nis also helpful for accurate object localization (Mask\\nR-CNN with instance segmentation task).\\n2) Overall, region proposal-based methods, such as Faster\\nR-CNN and R-FCN, perform better than regression/\\nclassiﬁcation-based approaches, namely, YOLO and\\nSSD, due to the fact that quite a lot of localization\\nerrors are produced by regression/classiﬁcation-based\\napproaches.\\n3) Context modeling is helpful to locate small objects,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SSD, due to the fact that quite a lot of localization\\nerrors are produced by regression/classiﬁcation-based\\napproaches.\\n3) Context modeling is helpful to locate small objects,\\nwhich provides additional information by consult-\\ning nearby objects and surroundings (GBD-Net and\\nmultipath).\\n4) Due to the existence of a large number of nonstandard\\nsmall objects, the results on this data set are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g., ResNeXt [123]) and\\nuseful strategies (e.g., mu ltitask learning [67], [124]),\\nthe performance can be improved.\\n5) The success of DSOD in training from scratch stresses\\nthe importance of the network design to release the\\nrequirements for perfect pretrained classiﬁers on relevant\\ntasks and a large number of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA\\nTitan X GPU. Except for “SS” which is processed with CPU,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA\\nTitan X GPU. Except for “SS” which is processed with CPU,\\nthe other procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n1) By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test\\ntime is further reduced with the uniﬁed multitask learn-\\ning (FRCN) and removal of additional region proposal\\ngeneration stage (Faster R-CNN). It is also helpful to\\ncompress the parameters of FC layers with SVD [91]\\n(PA VNET and FRCN).\\n2) It takes additional test time to extract multiscale fea-\\ntures and contextual information (ION and MR-RCNN&\\nS-RCNN).\\n3) It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time\\nconsumption can be reduced by adding as many lay-\\ners into shared fully convolutional layers as possible\\n(FRCN).'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO T EST DEV SET (%)\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 T EST SET\\n4) Regression-based models can usually be processed\\nin real time at the cost of a drop in accuracy\\ncompared with region proposal-based models. Also,\\nregion proposal-based models can be modiﬁed into\\nreal-time systems with the introduction of other\\ntricks [116] (PV ANET), such as BN [43] and residual\\nconnections [123].\\nIV . S\\nALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous appli-\\ncations incorporate the visual saliency to improve their perfor-\\nmance, such as image cropping [125] and segmentation [126],'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='most dominant object regions in an image. Numerous appli-\\ncations incorporate the visual saliency to improve their perfor-\\nmance, such as image cropping [125] and segmentation [126],\\nimage retrieval [57], and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely, BU [127] and TD [128]. Local\\nfeature contrast plays the central role in BU salient object\\ndetection, regardless of the semantic contents of the scene.\\nTo learn local feature contrast , various local and global fea-\\ntures are extracted from pixels, e.g., edges [129] and spa-\\ntial information [130]. However, high-level and multiscale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low-contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is\\ntask-oriented and takes prior knowledge about object cate-\\ngories to guide the generation of salient maps. Taking semantic'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='salient objects are obtained. TD salient object detection is\\ntask-oriented and takes prior knowledge about object cate-\\ngories to guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep Learning in Salient Object Detection\\nDue to the signiﬁcance for providing high-level and mul-\\ntiscale feature representation a nd the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133], and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Vig et al. [29] follows a completely\\nautomatic data-driven approach to perform a large-scale search'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='salient object detection.\\nThe early work by Vig et al. [29] follows a completely\\nautomatic data-driven approach to perform a large-scale search\\nfor optimal features, namely, an ensemble of deep networks\\nwith different layers and parameters. To address the problem\\nof limited training data, Kummerer et al. [134] proposed the\\nDeep Gaze by transferring from the AlexNet to generate a\\nhigh-dimensional feature space and create a saliency map.\\nA similar architecture was proposed by Huang et al. [135] to\\nintegrate saliency prediction into pretrained object recognition\\nDNNs. The transfer is accomp lished by ﬁne-tuning DNNs’\\nweights with an objective function based on the saliency\\nevaluation metrics, such as similarity, KL-divergence, and\\nnormalized scanpath saliency.\\nSome works combined local and global visual\\nclues to improve salient object detection performance.\\nWang et al. [136] trained two independent deep CNNs\\n(DNN-L and DNN-G) to capture local information and global'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='clues to improve salient object detection performance.\\nWang et al. [136] trained two independent deep CNNs\\n(DNN-L and DNN-G) to capture local information and global\\ncontrast and predicted saliency maps by integrating both\\nlocal estimation and global search. Cholakkal et al. [137]\\nproposed a weakly supervised saliency detection framework\\nto combine visual saliency from BU and TD saliency'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 13\\nTABLE VI\\nCOMPARISON BETWEEN STATE-OF -THE -ART METHODS\\nmaps and reﬁned the results with a multiscale superpixel-\\naveraging. Zhao et al. [138] proposed a multicontext deep\\nlearning framework, which utilizes a uniﬁed learning\\nframework to model global and local context jointly with\\nthe aid of superpixel segmentation. To predict saliency in\\nvideos, Bak et al. [139] fused two static saliency models,\\nnamely, spatial stream net and temporal stream net, into a\\ntwo-stream framework with a novel empirically grounded\\ndata augmentation technique.\\nComplementary information from semantic segmentation\\nand context modeling is beneﬁcial . To learn internal represen-\\ntations of saliency efﬁciently, He et al. [140] proposed a novel\\nsuperpixelwise CNN approach called SuperCNN, in which'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and context modeling is beneﬁcial . To learn internal represen-\\ntations of saliency efﬁciently, He et al. [140] proposed a novel\\nsuperpixelwise CNN approach called SuperCNN, in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully CNN, Li et al. [141] proposed a\\nmultitask deep saliency model, in which intrinsic correlations\\nbetween saliency detection and semantic segmentation are set\\nup. However, due to the conv layers with large receptive\\nﬁelds and pooling layers, blurry object boundaries and coarse\\nsaliency maps are produced. Tang and Wu [142] proposed\\na novel saliency detection framework (CRPSD) [142], which\\ncombines the region-level saliency estimation and pixel-level\\nsaliency prediction together with three closely related CNNs.\\nLi and Yu [143]proposed a deep contrast network to combine\\nsegmentwise spatial pooling and pixel-level fully convolutional\\nstreams [143].\\nThe proper integration of multiscale feature maps is also'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Li and Yu [143]proposed a deep contrast network to combine\\nsegmentwise spatial pooling and pixel-level fully convolutional\\nstreams [143].\\nThe proper integration of multiscale feature maps is also\\nof signiﬁcance for improving de tection performance. Based\\non Fast R-CNN, Wang et al. [144] proposed the RegionNet\\nby performing salient object detection with end-to-end edge\\npreserving and multiscale contextual modeling. Liu et al. [28]\\nproposed a multiresolution CNN (Mr-CNN) to predict eye ﬁx-\\nations, which is achieved by learning both BU visual saliency\\nand TD visual factors from raw image data simultaneously.\\nCornia et al. [145] proposed an architecture that combines fea-\\ntures extracted at different levels of the CNN. Li and Yu [146]\\nproposed a multiscale deep CNN framework to extract three\\nscales of deep contrast featur es, namely, the mean-subtracted\\nregion, the bounding box of its immediate neighboring regions,\\nand the masked entire image, from each candidate region.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scales of deep contrast featur es, namely, the mean-subtracted\\nregion, the bounding box of its immediate neighboring regions,\\nand the masked entire image, from each candidate region.\\nIt is efﬁcient and accurate to train a direct pixelwise\\nCNN architecture to predict salient objects with the aids\\nof recurrent neural networks and deconvolution networks.\\nPan et al. [147] formulated saliency prediction as a mini-\\nmization optimization on the Euclidean distance between the\\npredicted saliency map and the ground truth and proposed\\ntwo kinds of architectures: a shallow one trained from scratch\\nand a deeper one adapted fro m a deconvoluted VGG net-\\nwork. Asconvolutional–deconvolution networks are not expert\\nin recognizing objects of multiple scales, Kuen et al. [148]\\nproposed a recurrent attentional convolutional–deconvolution\\nnetwork with several spatial transformer and recurrent network\\nunits to conquer this problem. To fuse local, global, and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='proposed a recurrent attentional convolutional–deconvolution\\nnetwork with several spatial transformer and recurrent network\\nunits to conquer this problem. To fuse local, global, and\\ncontextual information of salient objects, Tang et al. [149]\\ndeveloped a deeply supervised recurrent CNN to perform a\\nfull image-to-image saliency detection.\\nB. Experimental Evaluation\\nFour representative data sets, including Evaluation on Com-\\nplex Scene Saliency Dataset (ECSSD) [156], HKU-IS [146],\\nPASCALS [157], and SOD [158], are used to evaluate several\\nstate-of-the-art methods. ECSSD consists of 1000 structurally\\ncomplex but semantically meaningful natural images. HKU-IS\\nis a large-scale data set containing over 4000 challenging\\nimages. Most of these images have more than one salient\\nobject and own low contrast. PASCALS is a subset chosen\\nfrom the validation set of PASCAL VOC 2010 segmentation\\ndata set and is composed of 850 natural images. The SOD data'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object and own low contrast. PASCALS is a subset chosen\\nfrom the validation set of PASCAL VOC 2010 segmentation\\ndata set and is composed of 850 natural images. The SOD data\\nset possesses 300 images containing multiple salient objects.\\nThe training and validation sets f or different data sets are kept\\nthe same as those in [152].\\nTwo standard metrics, namely, F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values precomputed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is deﬁned as follows:\\nF\\nβ = (1 + β2)Presion × Recall\\nβ2Presion + Recall (7)\\nwhere β2 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation :\\nMAE = 1\\nH × W\\nH∑\\ni=1\\nW∑\\nj=1\\n| ˆS(i, j) = ˆZ(i, j)| (8)\\nwhere ˆZ and ˆS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MAE = 1\\nH × W\\nH∑\\ni=1\\nW∑\\nj=1\\n| ˆS(i, j) = ˆZ(i, j)| (8)\\nwhere ˆZ and ˆS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected nonsalient pixels [159].\\nThe following approaches are evaluated: contextual hyper-\\ngraph modeling (CHM) [150], RC [151], discriminative'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n14 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\nregional feature integration (DRFI) [152], MC [138], mul-\\ntiscale deep CNN features (MDF) [146], local estimation\\nand global search (LEGS) [136], DSR [149], multi-task deep\\nneural network [141], CRPSD [142], deep contrast learn-\\ning (DCL) [143], encoded low level distance (ELD) [153],\\nnonlocal deep features (NLDF) [154], and deep supervision\\nwith short connections (DSSC) [155]. Among these meth-\\nods, CHM, RC, and DRFI are classical ones with the best\\nperformance [159], while the other methods are all associated\\nwith CNN. F-measure and MAE scores are given in Table VI.\\nFrom Table VI, we can ﬁnd that CNN-based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a more'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='From Table VI, we can ﬁnd that CNN-based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a more\\naccurate saliency. ELD refers to low-level handcrafted features\\nfor complementary information. LEGS adopts generic region\\nproposals to provide initial salient regions, which may be\\ninsufﬁcient for salient detection. DSR and MT act in different\\nways by introducing a recurrent network and semantic seg-\\nmentation, which provide insights for future improvements.\\nCRPSD, DCL, NLDF, and DSSC are all based on multiscale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF,\\nand DSSC perform the best on these four data sets. DSSC\\nearns the best performance by modeling scale-to-scale short\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of the CNN-based methods need to model'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='earns the best performance by modeling scale-to-scale short\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of the CNN-based methods need to model\\nvisual saliency along region boundaries with the aid of super-\\npixel segmentation. Meanwhile, the extraction of multiscale\\ndeep CNN features is of signiﬁcance for measuring local\\nconspicuity. Finally, it is necessary to strengthen local con-\\nnections between different CNN layers as well as to utilize\\ncomplementary information from local and global context.\\nV. F\\nACE DETECTION\\nFace detection is essential to many face applications\\nand acts as an important preprocessing procedure to\\nface recognition [160]–[162], f ace synthesis [163], [164], and\\nfacial expression analysis [165]. Different from generic object\\ndetection, this task is to recognize and locate face regions\\ncovering a very large range of scales (30–300 pts versus\\n10–1000 pts). At the same time, faces have their unique object'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection, this task is to recognize and locate face regions\\ncovering a very large range of scales (30–300 pts versus\\n10–1000 pts). At the same time, faces have their unique object\\nstructural conﬁgurations (e.g., the distribution of different\\nface parts) and characteristics (e.g., skin color). All these\\ndifferences lead to special attention to this task. However, large\\nvisual variations of faces, such a s occlusions, pose variations,\\nand illumination changes, impose great challenges for this task\\nin real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiﬁers with Haar-like features\\nand AdaBoost, achieving good performance with real-time\\nefﬁciency. However, this detector may degrade signiﬁcantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. [24] proposed a deformable part model (DPM)\\nfor face detection. However, for these traditional face detection'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='human faces. Different from this cascade structure, Felzen-\\nszwalb et al. [24] proposed a deformable part model (DPM)\\nfor face detection. However, for these traditional face detection\\nmethods, high computational e xpenses and lar ge quantities\\nof annotations are required to achieve a reasonable result.\\nFig. 11. ROC curves of state-of-the-a rt methods on FDDB. (a) Discrete ROC\\ncurves. (b) Continuous ROC curves.\\nIn addition, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep Learning in Face Detection\\nRecently, some CNN-based face detection approaches have\\nbeen proposed [167]–[169]. As less accurate localization\\nresults from independent regressions of object coordinates,\\nYu et al. [167] proposed a novel IoU loss function for pre-\\ndicting the four bounds of box jointly. Farfade et al. [168]\\nproposed a deep dense face detector (DDFD) to conduct\\nmultiview face detection, which is able to detect faces in'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='dicting the four bounds of box jointly. Farfade et al. [168]\\nproposed a deep dense face detector (DDFD) to conduct\\nmultiview face detection, which is able to detect faces in\\na wide range of orientations without the requirement of\\npose/landmark annotations. Yang et al. [169] proposed a novel\\ndeep learning-based face detec tion framework, which collects\\nthe responses from local facial parts (e.g., eyes, nose, and\\nmouths) to address face detection under severe occlusions\\nand unconstrained pose variations. Yang et al. [170] proposed\\na scale-friendly detection network named ScaleFace, which\\nsplits a large range of target scales into smaller subranges.\\nDifferent specialized subnetworks are constructed on these\\nsubscales and combined into a single one to conduct end-to-\\nend optimization. Hao et al. [171] designed an efﬁcient CNN\\nto predict the scale distribution histogram of the faces and took\\nthis histogram to guide the zoomed-in view and zoomed-out'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='end optimization. Hao et al. [171] designed an efﬁcient CNN\\nto predict the scale distribution histogram of the faces and took\\nthis histogram to guide the zoomed-in view and zoomed-out\\nview of the image. Since the faces are approximately in\\nuniform scale after zoom, compared with other state-of-the-art\\nbaselines, better performance is achieved with a less computa-\\ntion cost. In addition, some generic detection frameworks are\\nextended to face detection with different modiﬁcations, e.g.,\\nFaster R-CNN [30], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3-D modeling and face landmarks, in a multitask\\nlearning manner. Huang et al. [174] proposed a uniﬁed end-to-\\nend FCN framework called DenseBox to jointly conduct face\\ndetection and landmark localization. Li et al. [175] proposed\\na multitask discriminative learning framework that integrates\\na ConvNet with a ﬁxed 3-D mean face model in an end-to-end'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 15\\nmanner. In the framework, two issues are addressed to trans-\\nfer from generic object detection to face detection, namely,\\neliminating predeﬁned anchor boxes by a 3-D mean face\\nmodel and replacing RoI pooling layer with a conﬁguration\\npooling layer. Zhang et al. [176] proposed a deep cascaded\\nmultitask framework named multitask cascaded convolutional\\nnetworks (MTCNN) which exploits the inherent correlations\\nbetween face detection and alignment in the unconstrained\\nenvironment to boost up detection performance in a coarse-\\nto-ﬁne manner.\\nReducing computational expenses is of necessity in real\\napplications. To achieve real-time detection on the mobile plat-\\nform, Kalinovskii and Spitsyn [177] proposed a new solution\\nof frontal face detection based on compact CNN cascades. This'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='applications. To achieve real-time detection on the mobile plat-\\nform, Kalinovskii and Spitsyn [177] proposed a new solution\\nof frontal face detection based on compact CNN cascades. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify, and reﬁne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al. [32]\\nproposed a cascaded CNN denoted by supervised transformer\\nnetwork. This network takes a multitask RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang and Nevatia [8] proposed a\\nthree-stage cascade structu re based on FCNs, while in each\\nstage, a multiscale FCN is utilized to reﬁne the positions of\\npossible faces. Qin et al. [178] proposed a uniﬁed framework\\nthat achieves better results with the complementary informa-\\ntion from different jointly trained CNNs.\\nB. Experimental Evaluation'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='possible faces. Qin et al. [178] proposed a uniﬁed framework\\nthat achieves better results with the complementary informa-\\ntion from different jointly trained CNNs.\\nB. Experimental Evaluation\\nThe FDDB [179] data set has a total of 2845 pictures in\\nwhich 5171 faces are annotated with an elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the\\nreceiver operating characteristic (ROC) curve for the discrete\\nscores can reﬂect the dependence of the detected face fractions\\non the number of false alarms. Compared with annotations,\\nany detection with an IoU ratio exceeding 0.5 is treated as\\npositive. Each annotation is only associated with one detection.\\nThe ROC curve for the continuous scores is the reﬂection of\\nface localization quality.\\nThe evaluated models cover DDFD [168], Cascade-CNN\\n[180], aggregate channel features (ACF)-multiscale [181],\\nPico [182], Head-Hunter [183], Joint Cascade [31], SURF-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='face localization quality.\\nThe evaluated models cover DDFD [168], Cascade-CNN\\n[180], aggregate channel features (ACF)-multiscale [181],\\nPico [182], Head-Hunter [183], Joint Cascade [31], SURF-\\nmultiview [184], Viola–Jones [166], NPDFace [185],\\nFaceness [169], convolutional channel features (CCF) [186],\\nMTCNN [176], Conv3-D [175], Hyperface [187],\\nUnitBox [167], locally decorrelated channel\\nfeatures (LDCF +) [S2], DeepIR [173], hybrid-resolution\\nmodel with elliptical regressor (HR-ER) [188], Face-R-\\nCNN [172], and ScaleFace [170]. ACF-multiscale, Pico,\\nHeadHunter, Joint Cascade, SURF-multiview, Viola-Jones,\\nNPDFace, and LDCF + are built on classic hand-crafted\\nfeatures while the rest methods are based on deep CNN\\nfeatures. The ROC curves are shown in Fig. 11.\\nIn Fig. 11(a), in spite of relatively competitive results pro-\\nduced by LDCF +, it can be observed that most of the classic\\nmethods perform with similar results and are outperformed by'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In Fig. 11(a), in spite of relatively competitive results pro-\\nduced by LDCF +, it can be observed that most of the classic\\nmethods perform with similar results and are outperformed by\\nCNN-based methods by a signiﬁcant margin. In Fig. 11(b),\\nit can be observed that most of the CNN-based methods earn\\nsimilar true positive rates between 60% and 70% while DeepIR\\nand HR-ER perform much better than them. Among classic\\nmethods, Joint Cascade is still competitive. As earlier works,\\nDDFD and CCF directly make use of generated feature maps\\nand obtain relatively poor results. CascadeCNN builds cas-\\ncaded CNNs to locate face regions, which is efﬁcient but inac-\\ncurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3-D, and Hyperface proves the effectiveness of multitask\\nlearning. HR-ER and ScaleFace adaptively detect faces of'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='time-consuming. The outstanding performance of MTCNN,\\nConv3-D, and Hyperface proves the effectiveness of multitask\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales and make a balance between accuracy and\\nefﬁciency. DeepIR and Face-R-CNN are two extensions of the\\nFaster R-CNN architecture to face detection, which validate\\nthe signiﬁcance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN-based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cas cades, adapting scale-aware\\ndetection, and learning multitask shared CNN features.\\nVI. P\\nEDESTRIAN DETECTION\\nRecently, pedestrian detec tion has been intensively\\nstudied, which has a close relationship to pedestrian'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection, and learning multitask shared CNN features.\\nVI. P\\nEDESTRIAN DETECTION\\nRecently, pedestrian detec tion has been intensively\\nstudied, which has a close relationship to pedestrian\\ntracking [189], [190], person reidentiﬁcation [191], [192],\\nand robot navigation [193], [194]. Prior to the recent\\nprogress in deep CNN (DCNN)-based methods [195], [196],\\nsome researchers combined boosted decision forests\\nwith hand-crafted features to obtain pedestrian\\ndetectors [197]–[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes in typ-\\nical scenarios of pedestrian det ection (e.g., automatic driving\\nand intelligent surveillance), t he application of RoI pooling\\nlayer in generic object detection pipeline may result in “plain”\\nfeatures due to collapsing bins. In the meantime, the main'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and intelligent surveillance), t he application of RoI pooling\\nlayer in generic object detection pipeline may result in “plain”\\nfeatures due to collapsing bins. In the meantime, the main\\nsource of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different conﬁgurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep Learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature-\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some studies have been conducted to analyze\\nthe reasons. Zhang et al. [203] attempted to adapt generic'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='information and occlusion handling are incorporated [202].\\nThereby, some studies have been conducted to analyze\\nthe reasons. Zhang et al. [203] attempted to adapt generic\\nFaster R-CNN [17] to pedestrian detection. They modiﬁed the\\ndownstream classiﬁer by adding boosted forests to shared,\\nhigh-resolution conv feature maps and taking an RPN to han-\\ndle small instances and hard neg ative examples. To deal with'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n16 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\ncomplex occlusions existing in pedestrian images, inspired\\nby DPM [24], Tian et al. [204] proposed a deep learning\\nframework called DeepParts, which makes decisions based\\non an ensemble of extensive part detectors. DeepParts has\\nadvantages in dealing with weakly labeled data, low IoU\\npositive proposals, and partial occlusion.\\nOther researchers also tried to combine complementary\\ninformation from multiple data sources. CompACT-Deep\\nadopts a complexity-aware cascade to combine hand-crafted\\nfeatures and ﬁne-tuned DCNNs [195]. Based on Faster\\nR-CNN, Liu et al. [205] proposed multispectral DNNs for\\npedestrian detection to combine complementary information\\nfrom color and thermal images. Tian et al. [206] proposed\\na task-assistant CNN to jointly learn multiple tasks with'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='pedestrian detection to combine complementary information\\nfrom color and thermal images. Tian et al. [206] proposed\\na task-assistant CNN to jointly learn multiple tasks with\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. [207] pro-\\nposed a DNN fusion architecture for fast and robust pedes-\\ntrian detection. Based on the candidate BBs generated with\\nSSD detectors [71], multiple binary classiﬁers are processed\\nparallelly to conduct soft-rejection-based network fusion by\\nconsulting their aggregated degree of conﬁdences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel, and a large VGG16 model [195]. DeepParts contains\\n45 ﬁne-tuned DCNN models, and a set of strategies, includ-\\ning bounding box shifting handling and part selection, are\\nrequired to arrive at the reported results [204]. Therefore,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='45 ﬁne-tuned DCNN models, and a set of strategies, includ-\\ning bounding box shifting handling and part selection, are\\nrequired to arrive at the reported results [204]. Therefore,\\nthe modiﬁcation and simpliﬁcation are of signiﬁcance to\\nreduce the burden on both software and hardware to satisfy\\nreal-time detection demand. Tome et al. [59] proposed a novel\\nsolution to adapt generic object detection pipeline to pedestrian\\ndetection by optimizing most of its stages. Hu et al. [208]\\ntrained an ensemble of boosted decision models by reusing\\nthe conv feature maps, and a further improvement was gained\\nwith simple pixel labeling and additional complementary\\nhand-crafted features. Tome et al. [209] proposed a reduced\\nmemory region-based deep CNN architecture, which fuses\\nregional responses from both ACF detectors and SVM classi-\\nﬁers into R-CNN. Ribeiro et al. [33] addressed the problem of\\nhuman-aware navigation and proposed a vision-based person'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='regional responses from both ACF detectors and SVM classi-\\nﬁers into R-CNN. Ribeiro et al. [33] addressed the problem of\\nhuman-aware navigation and proposed a vision-based person\\ntracking system guided by multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian data set [3]. The data set was collected from the\\nvideos of a vehicle driving through an urban environment and\\nconsists of 250 000 frames with about 2300 unique pedestrians\\nand 350 000 annotated BBs. Three kinds of labels, namely,\\n“Person (clear identiﬁcations), ” “Person? (unclear identiﬁca-\\ntions),” and “People (large group of individuals),” are assigned\\nto different BBs. The perform ance is measured with the\\nlog-average miss rate (L-AMR) which is computed evenly\\nspaced in log-space in the range 10\\n−2 to 1 by averaging miss\\nrate at the rate of nine false positives per image [3]. According\\nto the differences in the height and visible part of the BBs,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='spaced in log-space in the range 10\\n−2 to 1 by averaging miss\\nrate at the rate of nine false positives per image [3]. According\\nto the differences in the height and visible part of the BBs,\\na total of nine popular settings are adopted to evaluate different\\nproperties of these models. Details of these settings are as\\nin [3].\\nEvaluated methods include Checkerboards + [198],\\nLDCF++ [S2], SCF +AlexNet [210], SA-FastRCNN [211],\\nMS-CNN [105], DeepParts [204], CompACT-Deep [195],\\nRPN+BF [203], and F-DNN +SS [207]. The ﬁrst two\\nmethods are based on hand-crafted features while the rest\\nones rely on deep CNN features. All results are exhibited\\nin Table VII. From this table, we observe that different\\nfrom other tasks, classic handcrafted features can still earn\\ncompetitive results with boosted decision forests [203],\\nACF [197], and HOG +LUV channels [S2]. As an early\\nattempt to adapt CNN to pedestrian detection, the features\\ngenerated by SCF +AlexNet are not so discriminant and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ACF [197], and HOG +LUV channels [S2]. As an early\\nattempt to adapt CNN to pedestrian detection, the features\\ngenerated by SCF +AlexNet are not so discriminant and\\nproduce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely, local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due\\nto complexity, it is too time-consuming to achieve real-time\\ndetection. The multiscale representation of MS-CNN improves\\nthe accuracy of pedestrian locations. SA-FastRCNN extends\\nFast R-CNN to automatically detect pedestrians according\\nto their different scales, which has trouble when there are\\npartial occlusions. RPN+BF combines the detectors produced\\nby Faster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN +SS, which is composed\\nof multiple parallel classiﬁers with soft rejections, performs'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='by Faster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN +SS, which is composed\\nof multiple parallel classiﬁers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN, and MS-CNN.\\nIn short, CNN-based methods can provide more accu-\\nrate candidate boxes and multilevel semantic information for\\nidentifying and loca ting pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classiﬁers, extracting multiscale and part-based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. P\\nROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor the future work.\\nThe ﬁrst one is small object detection such as occurring'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor the future work.\\nThe ﬁrst one is small object detection such as occurring\\nin COCO data set and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n1) Multitask Joint Optim ization and Multimodal\\nInformation Fusion: Due to the correlations between\\ndifferent tasks within and outside object detection,\\nmultitask joint optimization has already been studied\\nby many researchers [16], [17]. However, apart from\\nthe tasks mentioned in Section III-A8, it is desirable to\\nthink over the characteristics of different subtasks of\\nobject detection (e.g., superpixel semantic segmentation\\nin salient object detection) and extend multitask\\noptimization to other applications such as instance\\nsegmentation [66], multiobject tracking [202], and'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in salient object detection) and extend multitask\\noptimization to other applications such as instance\\nsegmentation [66], multiobject tracking [202], and\\nmultiperson pose estimation [S4]. In addition, given'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 17\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF STATE-OF -THE -ART MODELS ON CALTECH PEDESTRIAN DATA SET.\\nALL NUMBERS ARE REPORTED IN L-AMR\\na speciﬁc application, the information from different\\nmodalities, such as text [212] , thermal data [205], and\\nimages [65], can be fused together to achieve a more\\ndiscriminant network.\\n2) Scale Adaption: Objects usually exist in different scales,\\nwhich are more apparent in face detection and pedes-\\ntrian detection. To increase the robustness to scale\\nchanges, it is demanded to train scale-invariant, mul-\\ntiscale or scale-adaptive det ectors. For scale-invariant\\ndetectors, more powerful backbone architectures (e.g.,\\nResNext [123]), negative sample mining [113], reverse\\nconnection [213], and subcategory modeling [60] are all'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detectors, more powerful backbone architectures (e.g.,\\nResNext [123]), negative sample mining [113], reverse\\nconnection [213], and subcategory modeling [60] are all\\nbeneﬁcial. For multiscale detectors, both the FPN [66]\\nthat produces multiscale feature maps and the generative\\nadversarial network [214] that narrows representation\\ndifferences between small objects and the large ones\\nwith a low-cost architecture provide insights into gen-\\nerating meaningful feature p yramid. For scale-adaptive\\ndetectors, it is useful to combine knowledge graph [215],\\nattentional mechanism [216], cascade network [180],\\nand scale distribution estimation [171] to detect objects\\nadaptively.\\n3) Spatial Correlations and Contextual Modeling: Spatial\\ndistribution plays an important role in object detec-\\ntion. Therefore, region proposal generation and grid\\nregression are taken to obtain probable object loca-\\ntions. However, the correlations between multiple pro-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tion. Therefore, region proposal generation and grid\\nregression are taken to obtain probable object loca-\\ntions. However, the correlations between multiple pro-\\nposals and object categorie s are ignored. In addition,\\nthe global structure information is abandoned by the\\nposition-sensitive score maps in R-FCN. To solve these\\nproblems, we can refer to diverse subset selection [217]\\nand sequential reasoning tasks [218] for possible solu-\\ntions. It is also meaningful to mask salient parts and\\ncouple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor\\nand accomplish real-time object detection, with the emergence\\nof the large-scale image and video data. The following three\\naspects can be taken into account.\\n1) Cascade Network: In a cascade network, a cas-\\ncade of detectors is built in different stages or\\nlayers [180], [220]. Easily distinguishable examples are\\nrejected at shallow layers so that features and classiﬁers'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='cade of detectors is built in different stages or\\nlayers [180], [220]. Easily distinguishable examples are\\nrejected at shallow layers so that features and classiﬁers\\nat later stages can handle more difﬁcult samples with\\nthe aid of the decisions from previous stages. However,\\ncurrent cascades are built in a greedy manner, where\\nprevious stages in cascade are ﬁxed when training a new\\nstage. Therefore, the optim izations of different CNNs\\nare isolated, which stresses the necessity of end-to-end\\noptimization for CNN cascade. At the same time, it is\\nalso a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n2) Unsupervised and Weakly Supervised Learning: It is\\nvery time-consuming to manually draw large quantities\\nof BBs. To release this burden, semantic prior [55],\\nunsupervised object discovery [221], multiple instance\\nlearning [222], and DNN prediction [47] can be inte-\\ngrated to make the best use of image-level supervision'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='unsupervised object discovery [221], multiple instance\\nlearning [222], and DNN prediction [47] can be inte-\\ngrated to make the best use of image-level supervision\\nto assign object category tags to corresponding object\\nregions and reﬁne object boundaries. Furthermore,\\nweakly annotations (e.g., center-click annotations [223])\\nare also helpful for achieving high-quality detectors with\\nmodest annotation efforts, especially aided by the mobile\\nplatform.\\n3) Network Optimization: Given speciﬁc applications and\\nplatforms, it is signiﬁcant to make a balance among\\nspeed, memory, and accuracy by selecting an optimal\\ndetection architecture [116], [224]. However, despite\\nthat detection accuracy is reduced, it is more mean-\\ningful to learn compact models with a fewer number\\nof parameters [209]. This situation can be relieved by\\nintroducing better pretraining schemes [225], knowledge\\ndistillation [226], and hint learning [227]. DSOD also\\nprovides a promising guideline to train from scratch'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='introducing better pretraining schemes [225], knowledge\\ndistillation [226], and hint learning [227]. DSOD also\\nprovides a promising guideline to train from scratch\\nto bridge the gap between different image sources and\\ntasks [74].\\nThe third one is to extend typical methods for\\n2-D object detection to adapt 3-D object detection\\nand video object detection, with the requirements from\\nautonomous driving, intelligent t ransportation, and intelligent\\nsurveillance.\\n1) 3-D Object Detection: With the applications of 3-D\\nsensors (e.g., Light Detection and Ranging and cam-\\nera), additional depth information can be utilized to\\nbetter understand the images in 2-D and extend the\\nimage-level knowledge to the real world. However, sel-\\ndom of these 3-D-aware techniques aim to place correct\\n3-D BBs around detected objects. To achieve better\\nbounding results, multiview representation [181] and\\n3-D proposal network [228] may provide some guide-\\nlines to encode depth information with the aid of inertial'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='bounding results, multiview representation [181] and\\n3-D proposal network [228] may provide some guide-\\nlines to encode depth information with the aid of inertial\\nsensors (accelerometer and gyrometer) [229].'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n18 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\n2) Video Object Detection: Temporal information across\\ndifferent frames plays an important role in under-\\nstanding the behaviors of different objects. However,\\nthe accuracy suffers from degenerated object appear-\\nances (e.g., motion blur and video defocus) in videos\\nand the network is usually not trained end to end. To this\\nend, spatiotemporal tubelets [230], optical ﬂow [199],\\nand LSTM [107] should be considered to fundamentally\\nmodel object associations between consecutive frames.\\nVIII. C\\nONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation, and background\\nswitches, deep learning-based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='dealing with occlusion, scale transformation, and background\\nswitches, deep learning-based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning-based object detection frameworks\\nthat handle different subproblems, such as occlusion, clutter,\\nand low resolution, with different degrees of modiﬁcations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely, salient object\\ndetection, face detection, and pedestrian detection, are also\\nbrieﬂy reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object detec-\\ntion landscape. This review is also meaningful for the develop-\\nments in neural networks and related learning systems, which\\nprovides valuable insights and guidelines for future progress.\\nR\\nEFERENCES'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tion landscape. This review is also meaningful for the develop-\\nments in neural networks and related learning systems, which\\nprovides valuable insights and guidelines for future progress.\\nR\\nEFERENCES\\n[1] P. F. Felzenszwalb et al. , “Object detection with discriminatively\\ntrained part-based models,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 32, no. 9, pp. 1627–1645, Sep. 2010.\\n[2] K.-K. Sung and T. Poggio, “Example-based learning for view-based\\nhuman face detection,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39–51, Jan. 1998.\\n[3] C. Wojek et al. , “Pedestrian detection: An evaluation of the state\\nof the art,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 34, no. 4,\\npp. 743–761, Apr. 2012.\\n[4] H. Kobatake and Y . Yoshinaga, “Detection of spicules on mammogram\\nbased on skeleton analysis,” I E E ET r a n s .M e d .I m a g ., vol. 15, no. 3,\\npp. 235–245, Jun. 1996.\\n[5] Y . Jia et al., “Caffe: Convolutional architecture for fast feature embed-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='based on skeleton analysis,” I E E ET r a n s .M e d .I m a g ., vol. 15, no. 3,\\npp. 235–245, Jun. 1996.\\n[5] Y . Jia et al., “Caffe: Convolutional architecture for fast feature embed-\\nding,” in Proc. ACM MM, 2014, pp. 675–678.\\n[6] A. Krizhevsky et al., “ImageNet classiﬁcation with deep convolutional\\nneural networks,” in Proc. NIPS, 2012, pp. 1097–1105.\\n[7] Z. Cao et al. , “Realtime multi-person 2D pose estimation using part\\nafﬁnity ﬁelds,” in Proc. CVPR, 2017, pp. 1302–1310.\\n[8] Z. Yang and R. Nevatia, “A multi-scale cascade fully convolutional\\nnetwork face detector,” in Proc. ICPR, 2016, pp. 633–638.\\n[9] C. Chen et al., “DeepDriving: Learning affordance for direct perception\\nin autonomous driving,” in Proc. ICCV, 2015, pp. 2722–2730.\\n[10] X. Chen et al. , “Multi-view 3D object detection network for\\nautonomous driving,” in Proc. CVPR, 2017, pp. 6526–6534.\\n[11] A. Dundar et al., “Embedded streaming deep neural networks acceler-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[10] X. Chen et al. , “Multi-view 3D object detection network for\\nautonomous driving,” in Proc. CVPR, 2017, pp. 6526–6534.\\n[11] A. Dundar et al., “Embedded streaming deep neural networks acceler-\\nator with applications,” IEEE Trans. Neural Netw. Learn. Syst., vol. 28,\\nno. 7, pp. 1572–1583, Jul. 2017.\\n[12] R. J. Cintra et al., “Low-complexity approximate convolutional neural\\nnetworks,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 29, no. 12,\\npp. 5981–5992, 2018.\\n[13] S. H. Khan et al., “Cost-sensitive learning of deep feature representa-\\ntions from imbalanced data,” IEEE Trans. Neural Netw. Learn. Syst. ,\\nvol. 29, no. 8, pp. 3573–3587, Aug. 2018.\\n[14] A. Stuhlsatz et al. , “Feature extraction with deep neural networks by\\na generalized discriminant analysis,” IEEE Trans. Neural Netw. Learn.\\nSyst., vol. 23, no. 4, pp. 596–608, Apr. 2012.\\n[15] R. Girshick et al. , “Rich feature hierarchies for accurate object\\ndetection and semantic segmentation,” in Proc. CVPR, 2014,\\npp. 580–587.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Syst., vol. 23, no. 4, pp. 596–608, Apr. 2012.\\n[15] R. Girshick et al. , “Rich feature hierarchies for accurate object\\ndetection and semantic segmentation,” in Proc. CVPR, 2014,\\npp. 580–587.\\n[16] R. Girshick, “Fast R-CNN,” in Proc. ICCV, 2015, pp. 1440–1448.\\n[17] S. Ren et al., “Faster R-CNN: Towards real-time object detection with\\nregion proposal networks,” in Proc. NIPS, 2015, pp. 91–99.\\n[18] J. Redmon et al. , “You only look once: Uniﬁed, real-time object\\ndetection,” in Proc. CVPR, 2016, pp. 779–788.\\n[19] D. G. Lowe, “Distinctive image features from scale-invariant key-\\npoints,” Int. J. Comput. Vis. , vol. 60, no. 2, pp. 91–110, 2004.\\n[20] N. Dalal and B. Triggs, “Histogr ams of oriented gradients for human\\ndetection,” in Proc. CVPR, 2005, pp. 886–893.\\n[21] R. Lienhart and J. Maydt, “An extended set of Haar-like features for\\nrapid object detection,” in Proc. ICIP, 2002, p. 1.\\n[22] C. Cortes and V . Vapnik, “Support vector machine,” Mach. Learn. ,\\nvol. 20, no. 3, pp. 273–297, 1995.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='rapid object detection,” in Proc. ICIP, 2002, p. 1.\\n[22] C. Cortes and V . Vapnik, “Support vector machine,” Mach. Learn. ,\\nvol. 20, no. 3, pp. 273–297, 1995.\\n[23] Y . Freund and R. E. Schapire, “A decision-theoretic generalization of\\non-line learning and an application to boosting,” J. Comput. Syst. Sci. ,\\nvol. 55, no. 1, pp. 119–139, 1997.\\n[24] P. F. Felzenszwalb et al. , “Object detection with discriminatively\\ntrained part-based models,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 32, no. 9, pp. 1627–1645, Sep. 2010.\\n[25] M. Everingham et al., “The pascal visual object classes (VOC) chal-\\nlenge,” Int. J. Comput. Vis. , vol. 88, no. 2, pp. 303–338, 2008.\\n[26] G. E. Hinton and R. R. Salakhutdi nov, “Reducing the dimensionality of\\ndata with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,\\n2006.\\n[27] Y . LeCun et al. , “Deep learning,” Nature, vol. 521, pp. 436–444,\\nMay 2015.\\n[28] N. Liu et al. , “Predicting eye ﬁxations using convolutional neural'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2006.\\n[27] Y . LeCun et al. , “Deep learning,” Nature, vol. 521, pp. 436–444,\\nMay 2015.\\n[28] N. Liu et al. , “Predicting eye ﬁxations using convolutional neural\\nnetworks,” in Proc. CVPR, 2015, pp. 362–370.\\n[29] E. Vig et al. , “Large-scale optimization of hierarchical features\\nfor saliency prediction in natural images,” in Proc. CVPR, 2014,\\npp. 2798–2805.\\n[30] H. Jiang and E. Learned-Miller, “Face detection with the faster\\nR-CNN,” in Proc. FG, 2017, pp. 650–657.\\n[31] D. Chen et al., “Joint cascade face detection and alignment,” in Proc.\\nECCV, 2014, pp. 109–122.\\n[32] D. Chen et al. , “Supervised transformer network for efﬁcient face\\ndetection,” in Proc. ECCV, 2016, pp. 122–138.\\n[33] A. Mateus et al. . (2016). “Efﬁcient and robust pedestrian detection\\nusing deep learning for human-aware navigation.” [Online]. Available:\\nhttps://arxiv.org/abs/1607.04441\\n[34] F. Yang et al. , “Exploit all the layers: Fast and accurate CNN object'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='using deep learning for human-aware navigation.” [Online]. Available:\\nhttps://arxiv.org/abs/1607.04441\\n[34] F. Yang et al. , “Exploit all the layers: Fast and accurate CNN object\\ndetector with scale dependent pooling and cascaded rejection classi-\\nﬁers,” in Proc. CVPR, 2016, pp. 2129–2137.\\n[35] P. N. Druzhkov and V . D. Kustikova, “A survey of deep learning meth-\\nods and software tools for image classiﬁcation and object detection,”\\nPattern Recognit. Image Anal. ,\\n vol. 26, no. 1, pp. 9–15, 2016.\\n[36] W. Pitts and W. S. McCulloch, “How we know universals the perception\\nof auditory and visual forms,” Bull. Math. Biophys. , vol. 9, no. 3,\\npp. 127–147, 1947.\\n[37] D. E. Rumelhart et al., “Learning representations by back-propagating\\nerrors,” Nature, vol. 323, pp. 533–536, Oct. 1986.\\n[38] G. Hinton et al., “Deep neural networks for acoustic modeling in speech\\nrecognition: The shared views of four research groups,” IEEE Signal\\nProcess. Mag., vol. 29, no. 6, pp. 82–97, Nov. 2012.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[38] G. Hinton et al., “Deep neural networks for acoustic modeling in speech\\nrecognition: The shared views of four research groups,” IEEE Signal\\nProcess. Mag., vol. 29, no. 6, pp. 82–97, Nov. 2012.\\n[39] J. Deng et al., “ImageNet: A large-scale hierarchical image database,”\\nin Proc. CVPR, 2009, pp. 248–255.\\n[40] L. Deng et al. , “Binary coding of speech spectrograms using a deep\\nauto-encoder,” in Proc. INTERSPEECH, 2010, pp. 1692–1695.\\n[41] G. Dahl et al., “Phone recognition with the mean -covariance restricted\\nBoltzmann machine,” in Proc. NIPS, 2010, pp. 469–477.\\n[42] G. E. Hinton et al. . (2012). “Improving neural networks by pre-\\nventing co-adaptation of feature detectors.” [Online]. Available:\\nhttps://arxiv.org/abs/1207.0580\\n[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,” in Proc. ICML,\\n2015, pp. 448–456.\\n[44] P. Sermanet et al.. (2013). “OverFeat: Integrated recognition, localiza-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='network training by reducing internal covariate shift,” in Proc. ICML,\\n2015, pp. 448–456.\\n[44] P. Sermanet et al.. (2013). “OverFeat: Integrated recognition, localiza-\\ntion and detection using convolutional networks.” [Online]. Available:\\nhttps://arxiv.org/abs/1312.6229\\n[45] C. Szegedy et al., “Going deeper with convolutions,” in Proc. CVPR,\\n2015, pp. 1–9.\\n[46] K. Simonyan and A. Zisserman. (2014).“Very deep convolutional\\nnetworks for large-scale image recognition.” [Online]. Available:\\nhttps://arxiv.org/abs/1409.1556\\n[47] K. He et al., “Deep residual learning for image recognition,” in Proc.\\nCVPR, 2016, pp. 770–778.\\n[48] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\\nBoltzmann machines,” in Proc. ICML, 2010, pp. 807–814.\\n[49] M. Oquab et al. , “Weakly supervised object recognition with\\nconvolutional neural networks,” in Proc. NIPS, 2014, pp. 1–10.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 19\\n[50] M. Oquab et al. , “Learning and transferring mid-level image\\nrepresentations using convolutional neural networks,” in Proc. CVPR,\\n2014, pp. 1717–1724.\\n[51] F. M. Wadley, “Probit analysis: A statistical treatment of the sigmoid\\nresponse curve,” Ann. Entomol. Soc. Amer., vol. 67, no. 4, pp. 549–553,\\n1947.\\n[52] K. Kavukcuoglu et al. , “Learning invariant features through\\ntopographic ﬁlter maps,” in Proc. CVPR, 2009, pp. 1605–1612.\\n[53] K. Kavukcuoglu et al., “Learning convolutional feature hierarchies for\\nvisual recognition,” in Proc. NIPS, 2010, pp. 1090–1098.\\n[54] M. D. Zeiler et al., “Deconvolutional networks,” in Proc. CVPR, 2010,\\npp. 2528–2535.\\n[55] H. Noh et al. , “Learning deconvolution network for semantic\\nsegmentation,” in Proc. ICCV, 2015, pp. 1520–1528.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='pp. 2528–2535.\\n[55] H. Noh et al. , “Learning deconvolution network for semantic\\nsegmentation,” in Proc. ICCV, 2015, pp. 1520–1528.\\n[56] Z.-Q. Zhao et al., “Plant leaf identiﬁcation via a growing convolution\\nneural network with progressive sample learning,” in Proc. ACCV ,\\n2014, pp. 348–361.\\n[57] A. Babenko et al., “Neural codes for image retrieval,” in Proc. ECCV,\\n2014, pp. 584–599.\\n[58] J. Wan et al. , “Deep learning for content-based image retrieval:\\nA comprehensive study,” in ACM MM, 2014, pp. 157–166.\\n[59] D. Tomè et al. , “Deep convolutional neural networks for pedestrian\\ndetection,” Signal Process., Image Commun. , vol. 47, pp. 482–489,\\nSep. 2016.\\n[60] Y . Xiang et al., “Subcategory-aware convolutional neural networks for\\nobject proposals and detection,” in Proc. WACV, 2017, pp. 924–933.\\n[61] Z.-Q. Zhao et al. , “Pedestrian detection based on fast R-CNN and\\nbatch normalization,” in Proc. ICIC, 2017, pp. 735–746.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object proposals and detection,” in Proc. WACV, 2017, pp. 924–933.\\n[61] Z.-Q. Zhao et al. , “Pedestrian detection based on fast R-CNN and\\nbatch normalization,” in Proc. ICIC, 2017, pp. 735–746.\\n[62] J. Ngiam et al. , “Multimodal deep learning,” in Proc. ICML , 2011,\\npp. 689–696.\\n[63] Z. Wu et al. , “Modeling spatial-temporal clues in a hybrid deep\\nlearning framework for video classiﬁcation,” in Proc. ACM MM, 2015,\\npp. 461–470.\\n[64] K. He et al., “Spatial pyramid pooling in deep convolutional networks\\nfor visual recognition,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 37, no. 9, pp. 1904–1916, Sep. 2015.\\n[65] J. Dai et al. , “R-FCN: Object detection via region-based fully\\nconvolutional networks,” in Proc. NIPS, 2016, pp. 379–387.\\n[66] T.-Y . Lin et al. , “Feature pyramid networks for object detection,” in\\nProc. CVPR, 2017, pp. 936–944.\\n[67] K. He et al., “Mask R-CNN,” in Proc. ICCV, 2017, pp. 2980–2988.\\n[68] D. Erhan et al., “Scalable object detection using deep neural networks,”'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Proc. CVPR, 2017, pp. 936–944.\\n[67] K. He et al., “Mask R-CNN,” in Proc. ICCV, 2017, pp. 2980–2988.\\n[68] D. Erhan et al., “Scalable object detection using deep neural networks,”\\nin Proc. CVPR, 2014, pp. 2155–2162.\\n[69] D. Yoo et al., “AttentionNet: Aggregating weak directions for accurate\\nobject detection,” in Proc. CVPR, 2015, pp. 2659–2667.\\n[70] M. Najibi et al., “G-CNN: An iterative grid based object detector,” in\\nProc. CVPR, 2016, pp. 2369–2377.\\n[71] W. Liu et al., “SSD: Single shot multibox detector,” in Proc. ECCV,\\n2016, pp. 21–37.\\n[72] J. Redmon and A. Farhadi. (2016). “YOLO9000: Better, faster,\\nstronger.” [Online]. Available: https://arxiv.org/abs/1612.08242\\n[73] C.-Y . Fu et al.. (2017). “DSSD : Deconvolutional single shot detector.”\\n[Online]. Available: https://arxiv.org/abs/1701.06659\\n[74] Z. Shen et al. , “DSOD: Learning deeply supervised object detectors\\nfrom scratch,” in Proc. ICCV, 2017, p. 7.\\n[75] G. E. Hinton et al. , “Transforming auto-encoders,” in Proc. ICANN,'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[74] Z. Shen et al. , “DSOD: Learning deeply supervised object detectors\\nfrom scratch,” in Proc. ICCV, 2017, p. 7.\\n[75] G. E. Hinton et al. , “Transforming auto-encoders,” in Proc. ICANN,\\n2011, pp. 44–51.\\n[76] G. W. Taylor et al., “Learning invariance through imitation,” in Proc.\\nCVPR, 2011, pp. 2729–2736.\\n[77] X. Ren and D. Ramanan, “Histograms of sparse codes for object\\ndetection,” in Proc. CVPR, 2013, pp. 3246–3253.\\n[78] J. R. R. Uijlings et al. , “Selective search for object recognition,” Int.\\nJ. Comput. Vis., vol. 104, no. 2, pp. 154–171, Apr. 2013.\\n[79] P. Sermanet et al., “Pedestrian detection with unsupervised multi-stage\\nfeature learning,” in Proc. CVPR, 2013, pp. 3626–3633.\\n[80] P. Krähenbühl and V . Koltun, “Geodesic object proposals,” in Proc.\\nECCV, 2014, pp. 725–739.\\n[81] P. Arbeláez et al. , “Multiscale combinatorial grouping,” in Proc.\\nCVPR, 2014, pp. 328–335.\\n[82] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ECCV, 2014, pp. 725–739.\\n[81] P. Arbeláez et al. , “Multiscale combinatorial grouping,” in Proc.\\nCVPR, 2014, pp. 328–335.\\n[82] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals\\nfrom edges,” in Proc. ECCV, 2014, pp. 391–405.\\n[83] W. Kuo et al. , “Deepbox: Learning objectness with convolutional\\nnetworks,” in Proc. ICCV, 2015, pp. 2479–2487.\\n[84] P. O. Pinheiro et al. , “Learning to reﬁne object segments,” in Proc.\\nECCV, 2016, pp. 75–91.\\n[85] Y . Zhang et al., “Improving object detection with deep convolutional\\nnetworks via Bayesian optimizatio n and structured prediction,” in\\nProc. CVPR, 2015, pp. 249–258.\\n[86] S. Gupta et al., “Learning rich features from RGB-D images for object\\ndetection and segmentation,” in Proc. ECCV, 2014, pp. 345–360.\\n[87] W. Ouyang et al., “DeepID-Net: Deformable deep convolutional neural\\nnetworks for object detection,” in Proc. CVPR, 2015, pp. 2403–2412.\\n[88] K. Lenc and A. Vedaldi. (2015). “R-CNN minus R.” [Online].'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='networks for object detection,” in Proc. CVPR, 2015, pp. 2403–2412.\\n[88] K. Lenc and A. Vedaldi. (2015). “R-CNN minus R.” [Online].\\nAvailable: https://arxiv.org/abs/1506.06981\\n[89] S. Lazebnik et al. , “Beyond bags of features: Spatial pyramid\\nmatching for recognizing natural scene categories,” in Proc. CVPR,\\n2006, pp. 2169–2178.\\n[90] F. Perronnin et al., “Improving the Fisher kernel for large-scale image\\nclassiﬁcation,” in Proc. ECCV, 2010, pp. 143–156.\\n[91] J. Xue et al., “Restructuring of deep neural network acoustic models\\nwith singular value decomposition,” in Proc. INTERSPEECH , 2013,\\npp. 2365–2369.\\n[92] S. Ren et al., “Faster R-CNN: Towards real-time object detection with\\nregion proposal networks,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 39, no. 6, pp. 1137–1149, Jun. 2017.\\n[93] C. Szegedy et al., “Rethinking the inception architecture for computer\\nvision,” in Proc. CVPR, 2016, pp. 2818–2826.\\n[94] T.-Y . Lin et al. , “Microsoft COCO: Common objects in context,” in'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[93] C. Szegedy et al., “Rethinking the inception architecture for computer\\nvision,” in Proc. CVPR, 2016, pp. 2818–2826.\\n[94] T.-Y . Lin et al. , “Microsoft COCO: Common objects in context,” in\\nProc. ECCV, 2014, pp. 740–755.\\n[95] S. Bell et al. , “Inside-outside net: Detecting objects in context with\\nskip pooling and recurrent neural networks,” in Proc. CVPR, 2016,\\npp. 2874–2883.\\n[96] A. Arnab and P. H. S. Torr, “Pixe lwise instance segmentation with a\\ndynamically instantiated network,” in Proc. CVPR, 2017, pp. 879–888.\\n[97] J. Dai et al. , “Instance-aware semantic segmentation via multi-task\\nnetwork cascades,” in Proc. CVPR, 2016, pp. 3150–3158.\\n[98] Y . Li et al. , “Fully convolutional instance-aware semantic\\nsegmentation,” in Proc. CVPR, 2017, pp. 4438–4446.\\n[99] M. Jaderberg et al. , “Spatial transformer networks,” in Proc. Adv.\\nNeural Inf. Process. Syst. , 2015, pp. 2017–2025.\\n[100] S. Brahmbhatt et al. , “StuffNet: Using ‘Stuff’ to improve object'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[99] M. Jaderberg et al. , “Spatial transformer networks,” in Proc. Adv.\\nNeural Inf. Process. Syst. , 2015, pp. 2017–2025.\\n[100] S. Brahmbhatt et al. , “StuffNet: Using ‘Stuff’ to improve object\\ndetection,” in Proc. WACV, 2017, pp. 934–943.\\n[101] T. Kong et al., “HyperNet: Towards accurate region proposal generation\\nand joint object detection,” in Proc. CVPR, 2016, pp. 845–853.\\n[102] A. Pentina et al., “Curriculum learning of multiple tasks,” in Proc.\\nCVPR, 2015, pp. 5492–5500.\\n[103] J. Yim et al. , “Rotating your face using multi-task deep neural\\nnetwork,” in Proc. CVPR, 2015, pp. 676–684.\\n[104] J. Li et al.. (2016). “Multi-stage object detection with group recursive\\nlearning.” [Online]. Available: https://arxiv.org/abs/1608.05159\\n[105] Z. Cai et al., “A uniﬁed multi-scale deep convolutional neural network\\nfor fast object detection,” in Proc. ECCV, 2016, pp. 354–370.\\n[106] Y . Zhu et al. , “segDeepM: Exploiting segmentation and context in'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='for fast object detection,” in Proc. ECCV, 2016, pp. 354–370.\\n[106] Y . Zhu et al. , “segDeepM: Exploiting segmentation and context in\\ndeep neural networks for object detection,” in Proc. CVPR, 2015,\\npp. 4703–4711.\\n[107] W. Byeon et al. , “Scene labeling with LSTM recurrent neural\\nnetworks,” in Proc. CVPR, 2015, pp. 3547–3555.\\n[108] B. Moysset et al. . (2016). “Learning to detect and localize\\nmany objects from few examples.” [Online]. Available:\\nhttps://arxiv.org/abs/1611.05664\\n[109] X. Zeng et al. , “Gated bi-directional CNN for object detection,” in\\nProc. ECCV, 2016, pp. 354–369.\\n[110] S. Gidaris and N. Komodakis, “O bject detection via a multi-region\\nand semantic segmentation-aware CNN model,” in Proc. CVPR, 2015,\\npp. 1134–1142.\\n[111] M. Schuster and K. K. Paliwal, “ Bidirectional recurrent neural\\nnetworks,” IEEE Trans. Signal Process. , vol. 45, no. 11,\\npp. 2673–2681, Nov. 1997.\\n[112] S. Zagoruyko et al. (2016). “A multiPath network for object detection.”'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='networks,” IEEE Trans. Signal Process. , vol. 45, no. 11,\\npp. 2673–2681, Nov. 1997.\\n[112] S. Zagoruyko et al. (2016). “A multiPath network for object detection.”\\n[Online]. Available: https://arxiv.org/abs/1604.02135\\n[113] A. Shrivastava et al. , “Training region-based object detectors\\nwith online hard example mining,” in Proc. CVPR, 2016,\\npp. 761–769.\\n[114] S. Ren et al. , “Object detection networks on convolutional feature\\nmaps,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39, no. 7,\\npp. 1476–1481, Jul. 2017.\\n[115] W. Ouyang et al., “Factors in ﬁnetuning deep model for object detection\\nwith long-tail distribution,” in Proc. CVPR, 2016, pp. 864–873.\\n[116] S. Hong et al. . (2016). “PV ANet: Lightweight deep neural\\nnetworks for real-time object detection.” [Online]. Available:\\nhttps://arxiv.org/abs/1611.08588\\n[117] W. Shang et al. , “Understanding and improvi ng convolutional neural\\nnetworks via concatenated rectiﬁed linear units,” in Proc. ICML, 2016,\\npp. 1–9.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='https://arxiv.org/abs/1611.08588\\n[117] W. Shang et al. , “Understanding and improvi ng convolutional neural\\nnetworks via concatenated rectiﬁed linear units,” in Proc. ICML, 2016,\\npp. 1–9.\\n[118] C. Szegedy et al. , “Deep neural networks for object detection,” in\\nProc. NIPS, 2013, pp. 1–9.\\n[119] P. O. Pinheiro et al., “Learning to segment object candidates,” in Pr\\n oc.\\nNIPS, 2015, pp. 1990–1998.\\n[120] C. Szegedy et al. . (2014). “Scalable, high-quality object detection.”\\n[Online]. Available: https://arxiv.org/abs/1412.1441'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n20 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\\n[121] M. Everingham et al. . (2011). The PASCAL Visual Object\\nClasses Challenge 2012 (VOC2012) Results (2012). [Online].\\nAvailable: http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html\\n[122] M. D. Zeiler and R. Fergus, “ Visualizing and understanding\\nconvolutional networks,” in Proc. ECCV, 2014, pp. 818–833.\\n[123] S. Xie et al. , “Aggregated residual transformations for deep neural\\nnetworks,” in Proc. CVPR, 2017, pp. 5987–5995.\\n[124] \"J. Dai et al. (2017). “Deformable convolutional networks.” [Online].\\nAvailable: https://arxiv.org/abs/1703.06211\\n[125] C. Rother et al. , “AutoCollage,” ACM Trans. Graph. , vol. 25, no. 3,\\npp. 847–852, 2006.\\n[126] C. Jung and C. Kim, “A uniﬁed spectral-domain approach for saliency'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[125] C. Rother et al. , “AutoCollage,” ACM Trans. Graph. , vol. 25, no. 3,\\npp. 847–852, 2006.\\n[126] C. Jung and C. Kim, “A uniﬁed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,” IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272–1283, Mar. 2012.\\n[127] W.-C. Tu et al. , “Real-time salient object detection with a minimum\\nspanning tree,” in Proc. CVPR, 2016, pp. 2334–2342.\\n[128] J. Yang and M.-H. Yang, “Top-down visual saliency via joint CRF and\\ndictionary learning,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39,\\nno. 3, pp. 576–588, Mar. 2017.\\n[129] P. L. Rosin, “A simple method for detecting salient regions,” Pattern\\nRecognit., vol. 42, no. 11, pp. 2363–2371, Nov. 2009.\\n[130] T. Liu et al., “Learning to detect a salient object,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 33, no. 2, pp. 353–367, Feb. 2011.\\n[131] J. Long et al. , “Fully convolutional networks for semantic\\nsegmentation,” in Proc. CVPR, 2015, pp. 3431–3440.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Anal. Mach. Intell. , vol. 33, no. 2, pp. 353–367, Feb. 2011.\\n[131] J. Long et al. , “Fully convolutional networks for semantic\\nsegmentation,” in Proc. CVPR, 2015, pp. 3431–3440.\\n[132] D. Gao et al. , “Discriminant saliency, the detection of suspicious\\ncoincidences, and applications to visual recognition,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 31, no. 6, pp. 989–1005, Jun. 2009.\\n[133] S. Xie and Z. Tu, “Holistica lly-nested edge detection,” in Proc. ICCV,\\n2015, pp. 1395–1403.\\n[134] M. Kümmerer et al. . (2014). “Deep gaze I: Boosting saliency\\nprediction with feature maps trained on ImageNet.” [Online]. Available:\\nhttps://arxiv.org/abs/1411.1045\\n[135] X. Huang et al. , “SALICON: Reducing the semantic gap in saliency\\nprediction by adapting deep neural networks,” in Proc. ICCV , 2015,\\npp. 262–270.\\n[136] L. Wang et al. , “Deep networks for saliency detection via local\\nestimation and global search,” in Proc. CVPR, 2015, pp. 3183–3192.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='pp. 262–270.\\n[136] L. Wang et al. , “Deep networks for saliency detection via local\\nestimation and global search,” in Proc. CVPR, 2015, pp. 3183–3192.\\n[137] H. Cholakkal et al. . (2016). “Backtracking spatial pyramid pooling\\n(SPP)-based image classiﬁer for weakly supervised top-down salient\\nobject detection.” [Online]. Available: https://arxiv.org/abs/1611.05345\\n[138] R. Zhao et al. , “Saliency detection by multi-context deep learning,”\\nin Proc. CVPR, 2015, pp. 1265–1274.\\n[139] C. Bak et\\n al.. (2016). “Spatio-temporal saliency networks\\nfor dynamic saliency prediction.” [Online]. Available:\\nhttps://arxiv.org/abs/1607.04730\\n[140] S. He et al. , “SuperCNN: A superpixelwi se convolutional neural\\nnetwork for salient object detection,” Int. J. Comput. Vis. , vol. 115,\\nno. 3, pp. 330–344, 2015.\\n[141] X. Li et al., “DeepSaliency: Multi-task deep neural network model for\\nsalient object detection,” IEEE Trans. Image Process., vol. 25, no. 8,\\npp. 3919–3930, Aug. 2016.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='no. 3, pp. 330–344, 2015.\\n[141] X. Li et al., “DeepSaliency: Multi-task deep neural network model for\\nsalient object detection,” IEEE Trans. Image Process., vol. 25, no. 8,\\npp. 3919–3930, Aug. 2016.\\n[142] Y . Tang and X. Wu, “Saliency det ection via combining region-level\\nand pixel-level predictions with CNNs,” in Proc. ECCV , 2016,\\npp. 809–825.\\n[143] G. Li and Y . Yu, “Deep contrast lear ning for salient object detection,”\\nin Proc. CVPR, 2016, pp. 478–487.\\n[144] X. Wang et al. . (2016). “Edge preserving and multi-scale contextual\\nneural network for salient object detection.” [Online]. Available:\\nhttps://arxiv.org/abs/1608.08029\\n[145] M. Cornia et al., “A deep multi-level network for saliency prediction,”\\nin Proc. ICPR, 2016, pp. 3488–3493.\\n[146] G. Li and Y . Yu, “Visual saliency detection based on multiscale\\ndeep CNN features,” IEEE Trans. Image Process., vol. 25, no. 11,\\npp. 5012–5024, Nov. 2016.\\n[147] J. Pan et al. , “Shallow and deep convolutional networks for saliency'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='deep CNN features,” IEEE Trans. Image Process., vol. 25, no. 11,\\npp. 5012–5024, Nov. 2016.\\n[147] J. Pan et al. , “Shallow and deep convolutional networks for saliency\\nprediction,” in Proc. CVPR, 2016, pp. 598–606.\\n[148] J. Kuen et al., “Recurrent attentional networks for saliency detection,”\\nin Proc. CVPR, 2016, pp. 3668–3677.\\n[149] Y . Tang et al. , “Deeply-supervised recurrent convolutional neural\\nnetwork for saliency detection,” inProc. ACM MM, 2016, pp. 397–401.\\n[150] X. Li et al. , “Contextual hypergraph modeling for salient object\\ndetection,” in Proc. ICCV, 2013, pp. 3328–3335.\\n[151] M.-M. Cheng et al., “Global contrast based salient region detection,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 37, no. 3, pp. 569–582,\\nMar. 2015.\\n[152] H. Jiang et al. , “Salient object detection: A discriminative\\nregional feature integration approach,” in Proc. CVPR, 2013,\\npp. 2083–2090.\\n[153] G. Lee et al. , “Deep saliency with encoded low level distance map'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='regional feature integration approach,” in Proc. CVPR, 2013,\\npp. 2083–2090.\\n[153] G. Lee et al. , “Deep saliency with encoded low level distance map\\nand high level features,” in Proc. CVPR, 2016, pp. 660–668.\\n[154] Z. Luo et al. , “Non-local deep features for salient object detection,”\\nin Proc. CVPR, 2017, pp. 6593–6601.\\n[155] Q. Hou et al. . (2016). “Deeply supervised salient object\\ndetection with short connections.” [Online]. Available:\\nhttps://arxiv.org/abs/1611.04849\\n[156] Q. Yan et al.,\\n“Hierarchical saliency detection,” in Proc. CVPR, 2013,\\npp. 1155–1162.\\n[157] Y . Li et al. , “The secrets of salient object segmentation,” in Proc.\\nCVPR, 2014, pp. 280–287.\\n[158] V . Movahedi and J. H. Elder, “Design and perceptual validation\\nof performance measures for salient object segmentation,” in Proc.\\nCVPRW, 2010, pp. 49–56.\\n[159] A. Borji et al. , “Salient object detection: A bench-\\nmark,” IEEE Trans. Image Process. , vol. 24, no. 12,\\npp. 5706–5722, Dec. 2015.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='CVPRW, 2010, pp. 49–56.\\n[159] A. Borji et al. , “Salient object detection: A bench-\\nmark,” IEEE Trans. Image Process. , vol. 24, no. 12,\\npp. 5706–5722, Dec. 2015.\\n[160] C. Peng et al. , “Graphical representation for heterogeneous face\\nrecognition,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39, no. 2,\\npp. 301–312, Feb. 2017.\\n[161] C. Peng et al. , “Face recognition from multiple stylistic sketches:\\nScenarios, datasets, and evaluation,” in Proc. ECCV, 2016, pp. 3–18.\\n[162] X. Gao et al., “Face sketch–photo synthesis and retrieval using sparse\\nrepresentation,” IEEE Trans. Circuits Syst. Video Technol. , vol. 22,\\nno. 8, pp. 1213–1226, Aug. 2012.\\n[163] N. Wang et al. , “A comprehensive survey to face hallucination,” Int.\\nJ. Comput. Vis., vol. 106, no. 1, pp. 9–30, 2014.\\n[164] C. Peng et al. , “Multiple representations-based face sketch–photo\\nsynthesis,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 27, no. 11,\\npp. 2201–2215, Nov. 2016.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[164] C. Peng et al. , “Multiple representations-based face sketch–photo\\nsynthesis,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 27, no. 11,\\npp. 2201–2215, Nov. 2016.\\n[165] A. Majumder et al. , “Automatic facial expression recognition system\\nusing deep network-based data fusion,” IEEE Trans. Cybern., vol. 48,\\nno. 1, pp. 103–114, Jan. 2018.\\n[166] P. Viola and M. J. Jones, “Robust real-time face detection,” Int. J.\\nComput. Vis., vol. 57, no. 2, pp. 137–154, 2004.\\n[167] J. Yu et al. , “Unitbox: An advanced object detection network,” in\\nProc. ACM MM, 2016, pp. 516–520.\\n[168] S. S. Farfade et al., “Multi-view face detection using deep convolutional\\nneural networks,” in Proc. ICMR, 2015, pp. 643–650.\\n[169] S. Yang et al., “From facial parts responses to face detection: A deep\\nlearning approach,” in Proc. ICCV, 2015, pp. 3676–3684.\\n[170] S. Yang et al. , “Face detection through scale-friendly deep\\nconvolutional networks,” in Proc. CVPR, 2017, pp. 1–12.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='learning approach,” in Proc. ICCV, 2015, pp. 3676–3684.\\n[170] S. Yang et al. , “Face detection through scale-friendly deep\\nconvolutional networks,” in Proc. CVPR, 2017, pp. 1–12.\\n[171] Z. Hao et al. , “Scale-aware face detection,” in Proc. CVPR, 2017,\\npp. 1913–1922.\\n[172] H. Wang et al. . (2017). “Face R-CNN.” [Online]. Available:\\nhttps://arxiv.org/abs/1706.01061\\n[173] X. Sun et al. . (2017). “Face detection using deep learning:\\nAn improved faster RCNN approach.” [Online]. Available:\\nhttps://arxiv.org/abs/1701.08289\\n[174] L. Huang et al. . (2015). “DenseBox: Unifying landmark\\nlocalization with end to end object detection.” [Online]. Available:\\nhttps://arxiv.org/abs/1509.04874\\n[175] Y . Li et al., “face detection with end-to-end integration of a ConvNet\\nand a 3D model,” in Proc. ECCV, 2016, pp. 420–436.\\n[176] K. Zhang et al. , “Joint face detection and alignment using multitask\\ncascaded convolutional networks,” IEEE Signal Process. Lett., vol. 23,\\nno. 10, pp. 1499–1503, Oct. 2016.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[176] K. Zhang et al. , “Joint face detection and alignment using multitask\\ncascaded convolutional networks,” IEEE Signal Process. Lett., vol. 23,\\nno. 10, pp. 1499–1503, Oct. 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, “Compact convolutional neural\\nnetwork cascade for face detection,” in Proc. CEUR Workshop, 2016,\\npp. 375–387.\\n[178] H. Qin et al., “Joint training of cascaded CNN for face detection,” in\\nProc. CVPR, 2016, pp. 3456–3465.\\n[179] V . Jain and E. Learned-Miller, “FDDB: A benchmark for face detection\\nin unconstrained settings,” Univ. Massachusetts, Amherst, Amherst,\\nMA, USA, Tech. Rep. UM-CS-2010-009, 2010.\\n[180] H. Li et al. , “A convolutional neural network cascade for face\\ndetection,” in Proc. CVPR, 2015, pp. 5325–5334.\\n[181] B. Yang et al. , “Aggregate channel features for multi-view face\\ndetection,” in Proc. IJCB, 2014, pp. 1–8.\\n[182] N. Markuš et al. . (2013). “Object detection with pixel intensity\\ncomparisons organized in decisi on trees.” [Online]. Available:'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection,” in Proc. IJCB, 2014, pp. 1–8.\\n[182] N. Markuš et al. . (2013). “Object detection with pixel intensity\\ncomparisons organized in decisi on trees.” [Online]. Available:\\nhttps://arxiv.org/abs/1305.4537\\n[183] M. Mathias et al. , “Face detection without bells and whistles,” in\\nProc. ECCV, 2014.\\n[184] J. Li and Y . Zhang, “Learning surf cascade for fast and accurate object\\ndetection,” in Proc. CVPR, 2013.\\n[185] S. Liao et al. , “A fast and accurate unconstrained face detector,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. 2,\\npp. 211–223, Feb. 2016.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nZHAO et al.: OBJECT DETECTION WITH DEEP LEARNING 21\\n[186] B. Yang et al., “Convolutional channel features,” in Proc. ICCV, 2015,\\npp. 82–90.\\n[187] R. Ranjan et al. . (2016). “HyperFace: A deep multi-task\\nlearning framework for face det ection, landmark localization,\\npose estimation, and gender recognition.” [Online]. Available:\\nhttps://arxiv.org/abs/1603.01249\\n[188] P. Hu and D. Ramanan, “Finding tiny faces,” in Proc. CVPR, 2017,\\npp. 1522–1530.\\n[189] Z. Jiang and D. Q. Huynh, “Multiple pedestrian tracking from\\nmonocular videos in an interacting multiple model framework,” IEEE\\nTrans. Image Process., vol. 27, no. 3, pp. 1361–1375, Mar. 2018.\\n[190] D. M. Gavrila and S. Munder, “Multi-cue pedestrian detection and\\ntracking from a moving vehicle,” Int. J. Comput. Vis. , vol. 73, no. 1,\\npp. 41–59, Jun. 2007.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[190] D. M. Gavrila and S. Munder, “Multi-cue pedestrian detection and\\ntracking from a moving vehicle,” Int. J. Comput. Vis. , vol. 73, no. 1,\\npp. 41–59, Jun. 2007.\\n[191] S. Xu et al. , “Jointly attentive spatial-temporal pooling networks\\nfor video-based person re-identiﬁcation,” in Proc. ICCV , 2017,\\npp. 4743–4752.\\n[192] Z. Liu et al. , “Stepwise metric promotion for unsupervised video\\nperson re-identiﬁcation,” in Proc. ICCV, 2017, pp. 2448–2457.\\n[193] A. Khan et al. , “Cooperative robots to observe moving targets:\\nReview,”IEEE Trans. Cybern., vol. 48, no. 1, pp. 187–198, Jan. 2018.\\n[194] A. Geiger et al. , “Vision meets robotics: The KITTI dataset,” Int. J.\\nRobot. Res., vol. 32, no. 11, pp. 1231–1237, 2013.\\n[195] Z. Cai et al., “Learning complexity-aware cascades for deep pedestrian\\ndetection,” in Proc. ICCV, 2015, pp. 3361–3369.\\n[196] Y . Tian et al. , “Deep learning strong parts f or pedestrian detection,”\\nin Proc. CVPR, 2015, pp. 1904–1912.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection,” in Proc. ICCV, 2015, pp. 3361–3369.\\n[196] Y . Tian et al. , “Deep learning strong parts f or pedestrian detection,”\\nin Proc. CVPR, 2015, pp. 1904–1912.\\n[197] P. Dollár et al. , “Fast feature pyramids for object detection,” IEEE\\nTrans. Pattern Anal. Mach. Intell. , vol. 36, no. 8, pp. 1532–1545,\\nAug. 2014.\\n[198] S. Zhang et al. , “Filtered channel features for pedestrian detection,”\\nin Proc. CVPR, 2015, pp. 1751–1760.\\n[199] S. Paisitkriangkrai et al. , “Pedestrian detection with spatially pooled\\nfeatures and structured ensemble learning,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 38, no. 6, pp. 1243–1257, Jun. 2016.\\n[200] L. Lin et al. , “Discriminatively trained And-Or graph models for\\nobject shape detection,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 37, no. 5, pp. 959–972, May 2015.\\n[201] M. Mathias et al. , “Handling occlusions with Franken-classiﬁers,” in\\nProc. ICCV, 2013, pp. 1505–1512.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='vol. 37, no. 5, pp. 959–972, May 2015.\\n[201] M. Mathias et al. , “Handling occlusions with Franken-classiﬁers,” in\\nProc. ICCV, 2013, pp. 1505–1512.\\n[202] S. Tang et al. , “Detection and tracking of occluded people,” Int. J.\\nComput.\\nVis., vol. 110, no. 1, pp. 58–69, 2014.\\n[203] L. Zhang et al., “Is faster R-CNN doing well for pedestrian detection?”\\nin Proc. ECCV, 2016, pp. 443–457.\\n[204] Y . Tian et al. , “Deep learning strong parts f or pedestrian detection,”\\nin Proc. ICCV, 2015.\\n[205] J. Liu et al.. (2016). “Multispectral deep neural networks for pedestrian\\ndetection.” [Online]. Available: https://arxiv.org/abs/1611.02644\\n[206] Y . Tian et al. , “Pedestrian detection aided by deep learning semantic\\ntasks,” in Proc. CVPR, 2015, pp. 5079–5087.\\n[207] X. Du et al. , “Fused DNN: A deep neural network fusion approach\\nto fast and robust pedestrian detection,” in Proc. WACV, 2017,\\npp. 953–961.\\n[208] Q. Hu et al. , “Pushing the limits of deep CNNs for pedestrian'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to fast and robust pedestrian detection,” in Proc. WACV, 2017,\\npp. 953–961.\\n[208] Q. Hu et al. , “Pushing the limits of deep CNNs for pedestrian\\ndetection,” IEEE Trans. Circuits Syst. Video Technol. , vol. 28, no. 6,\\npp. 1358–1368, Jun. 2018.\\n[209] D. Tomé et al. , “Reduced memory region based deep convolutional\\nneural network detection,” in Proc. ICCE , Berlin, Germany, 2016,\\npp. 15–19.\\n[210] J. Hosang et al., “Taking a deeper look at pedestrians,” in Proc. CVPR,\\n2015, pp. 4073–4082.\\n[211] J. Li et al.. (2015). “Scale-aware fast R-CNN for pedestrian detection.”\\n[Online]. Available: https://arxiv.org/abs/1510.08160\\n[212] Y . Gao et al. , “Visual-textual joint relevance learning for tag-based\\nsocial image search,” IEEE Trans. Image Process., vol. 22, no. 1,\\npp. 363–376, Jan. 2013.\\n[213] T. Kong et al. , “RON: Reverse connection with objectness prior\\nnetworks for object detection,” in Proc. CVPR, 2017, pp. 5244–5252.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='pp. 363–376, Jan. 2013.\\n[213] T. Kong et al. , “RON: Reverse connection with objectness prior\\nnetworks for object detection,” in Proc. CVPR, 2017, pp. 5244–5252.\\n[214] I. J. Goodfellow et al. , “Generative adversarial nets,” in Proc. NIPS,\\n2014, pp. 2672–2680.\\n[215] Y . Fang et al. , “Object detection meets knowledge graphs,” in Proc.\\nIJCAI, 2017, pp. 1661–1667.\\n[216] S. Welleck et al. , “Saliency-based sequential image attention with\\nmultiset prediction,” in Proc. NIPS, 2017, pp. 5173–5183.\\n[217] S. Azadi et al., “Learning detection with diverse proposals,” in Proc.\\nCVPR, 2017, pp. 7369–7377.\\n[218] S. Sukhbaatar et al. , “End-to-end memory networks,” in Proc. NIPS,\\n2015, pp. 2440–2448.\\n[219]\\nP. Dabkowski and Y . Gal, “Real time image saliency for black box\\nclassiﬁers,” in Proc. NIPS, 2017, pp. 6967–6976.\\n[220] B. Yang et al., “CRAFT objects from images,” in Proc. CVPR, 2016,\\npp. 6043–6051.\\n[221] I. Croitoru et al. , “Unsupervised learning from video to detect fore-'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[220] B. Yang et al., “CRAFT objects from images,” in Proc. CVPR, 2016,\\npp. 6043–6051.\\n[221] I. Croitoru et al. , “Unsupervised learning from video to detect fore-\\nground objects in single images,” in Proc. ICCV, 2017, pp. 4345–4353.\\n[222] C. Wang et al. , “Weakly supervised object localization with latent\\ncategory learning,” in Proc. ECCV, 2014, pp. 431–445.\\n[223] D. P. Papadopoulos et al., “Training object class detectors with click\\nsupervision,” in Proc. CVPR, 2017, pp. 180–189.\\n[224] J. Huang et al. , “Speed/accuracy trade-offs for modern convolutional\\nobject detectors,” in Proc. CVPR, 2017, pp. 3296–3297.\\n[225] Q. Li et al., “Mimicking very efﬁcient network for object detection,”\\nin Proc. CVPR, 2017, pp. 7341–7349.\\n[226] G. Hinton et al. , “Distilling the knowledge in a neural network,”\\nComput. Sci., vol. 14, no. 7, pp. 38–39, 2015.\\n[227] A. Romero et al., “FitNets: Hints for thin deep nets,” in Proc. ICLR,\\n2015, pp. 1–13.'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Comput. Sci., vol. 14, no. 7, pp. 38–39, 2015.\\n[227] A. Romero et al., “FitNets: Hints for thin deep nets,” in Proc. ICLR,\\n2015, pp. 1–13.\\n[228] X. Chen et al. , “3D object proposals for accurate object class\\ndetection,” in Proc. NIPS, 2015, pp. 424–432.\\n[229] J. Dong et al. , “Visual-inertial-semantic scene representation for 3D\\nobject detection,” in Proc. CVPR, 2017, pp. 960–970.\\n[230] K. Kang et al. , “Object detection in videos with tubelet proposal\\nnetworks,” in Proc. CVPR, 2017, pp. 889–897.\\nZhong-Qiu Zhao (M’10) received the Ph.D. degree\\nin pattern recognition and intelligent system from\\nthe University of Science and Technology of China,\\nHefei, China, in 2007.\\nFrom 2008 to 2009, he held a post-doctoral\\nposition in image processing with the CNRS\\nUMR6168 Lab Sciences de Information et des\\nSystèmes, La Garde, France. From 2013 to 2014,\\nhe was a Research Fellow in image processing with\\nthe Department of Computer Science, Hong Kong\\nBaptist University, Hong Kong. He is currently a'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Systèmes, La Garde, France. From 2013 to 2014,\\nhe was a Research Fellow in image processing with\\nthe Department of Computer Science, Hong Kong\\nBaptist University, Hong Kong. He is currently a\\nProfessor with the Hefei University of Technology, Hefei. His current research\\ninterests include pattern recognition, image processing, and computer vision.\\nPeng Zheng received the bachelor’s degree from\\nthe Hefei University of Technology, Hefei, China,\\nin 2010, where he is currently pursuing the Ph.D.\\ndegree.\\nHis current research in terests include pattern\\nrecognition, image processing, and computer vision.\\nShou-Tao Xu is currently pursuing the master’s\\ndegree with the Hefei University of Technology,\\nHefei, China.\\nHis current research in terests include pattern\\nrecognition, image processing, deep learning, and\\ncomputer vision.\\nXindong Wu (F’11) received the Ph.D. degree in\\nartiﬁcial intelligence from The University of Edin-\\nburgh, Edinburgh, U.K.\\nHe is currently an Alfred and Helen Lamson'),\n",
       " Document(metadata={'producer': 'Aspose.Pdf for .NET 8.3.0; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-01-25T14:32:22-05:00', 'ieee issue id': '6104215', 'ieee article id': '8627998', 'title': 'Object Detection With Deep Learning: A Review', 'ieee publication id': '5962385', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Neural Networks and Learning Systems; ;PP;99;10.1109/TNNLS.2018.2876865', 'moddate': '2019-01-25T15:24:09-05:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='computer vision.\\nXindong Wu (F’11) received the Ph.D. degree in\\nartiﬁcial intelligence from The University of Edin-\\nburgh, Edinburgh, U.K.\\nHe is currently an Alfred and Helen Lamson\\nEndowed Professor of computer science with the\\nUniversity of Louisiana at Lafayette, Lafayette, LA,\\nUSA. His current research interests include data\\nmining, knowledge-based systems, and Web infor-\\nmation exploration.\\nDr. Wu is a Fellow of the AAAS. He is the\\nSteering Committee Chair of the IEEE International\\nConference on Data Mining. He served as the Editor-in-Chief for the IEEE\\nTRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING (IEEE Com-\\nputer Society) between 2005 and 2008.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e34d52c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 315 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10/10 [00:40<00:00,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (315, 384)\n",
      "Loading FAISS index & metadata...\n",
      "FAISS Vector Store initialized. Loaded 945 documents.\n",
      "Adding 315 documents...\n",
      "Done. Total docs: 1260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Initialize vectorstore\n",
    "vectorstore = VectorStoreFAISS(dim=384)\n",
    "\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcfd26b",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1136dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles multi-doc retrieval from FAISS\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStoreFAISS, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0):\n",
    "        print(f\"\\n🔍 Retrieving documents for query: '{query}'\")\n",
    "\n",
    "        # Embed query\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query]).astype(\"float32\")\n",
    "\n",
    "        # Search FAISS\n",
    "        distances, indices = self.vector_store.index.search(query_embedding, top_k)\n",
    "\n",
    "        results = []\n",
    "        for rank, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            if idx >= len(self.vector_store.documents):\n",
    "                continue\n",
    "\n",
    "            similarity = 1 / (1 + dist)\n",
    "\n",
    "            if similarity < score_threshold:\n",
    "                continue\n",
    "\n",
    "            results.append({\n",
    "                \"rank\": rank + 1,\n",
    "                \"content\": self.vector_store.documents[idx],\n",
    "                \"metadata\": self.vector_store.metadatas[idx],\n",
    "                \"id\": self.vector_store.ids[idx],\n",
    "                \"distance\": float(dist),\n",
    "                \"similarity_score\": float(similarity),\n",
    "            })\n",
    "\n",
    "        print(f\"✅ Retrieved {len(results)} docs (after filtering).\")\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1348f5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Retrieving documents for query: 'what is attention all you need?'\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "✅ Retrieved 5 docs (after filtering).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'producer': 'pdfTeX-1.40.25',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'trapped': '/False',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'total_pages': 15,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 216},\n",
       "  'id': 'doc_c9ee3a27_24',\n",
       "  'distance': 0.8755648136138916,\n",
       "  'similarity_score': 0.5331727266311646},\n",
       " {'rank': 2,\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'producer': 'pdfTeX-1.40.25',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'trapped': '/False',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'total_pages': 15,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 216},\n",
       "  'id': 'doc_1e5da5cf_339',\n",
       "  'distance': 0.8755648136138916,\n",
       "  'similarity_score': 0.5331727266311646},\n",
       " {'rank': 3,\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'producer': 'pdfTeX-1.40.25',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'trapped': '/False',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'total_pages': 15,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 216},\n",
       "  'id': 'doc_4fadd668_654',\n",
       "  'distance': 0.8755648136138916,\n",
       "  'similarity_score': 0.5331727266311646},\n",
       " {'rank': 4,\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'producer': 'pdfTeX-1.40.25',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'trapped': '/False',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'total_pages': 15,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 216},\n",
       "  'id': 'doc_7507256b_969',\n",
       "  'distance': 0.8755648136138916,\n",
       "  'similarity_score': 0.5331727266311646},\n",
       " {'rank': 5,\n",
       "  'content': 'Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13',\n",
       "  'metadata': {'producer': 'pdfTeX-1.40.25',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'trapped': '/False',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'total_pages': 15,\n",
       "   'page': 12,\n",
       "   'page_label': '13',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 812},\n",
       "  'id': 'doc_cd549b72_98',\n",
       "  'distance': 1.0104721784591675,\n",
       "  'similarity_score': 0.4973955750465393}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)\n",
    "\n",
    "query = \"what is attention all you need?\"\n",
    "answer = rag_retriever.retrieve(query)\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "827dc9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Retrieving documents for query: 'Unified Multi-task Learning Framework'\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "✅ Retrieved 5 docs (after filtering).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'total_pages': 27,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941},\n",
       "  'id': 'doc_bdd94aad_122',\n",
       "  'distance': 0.8619899749755859,\n",
       "  'similarity_score': 0.5370598435401917},\n",
       " {'rank': 2,\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'total_pages': 27,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941},\n",
       "  'id': 'doc_44ad8b06_437',\n",
       "  'distance': 0.8619899749755859,\n",
       "  'similarity_score': 0.5370598435401917},\n",
       " {'rank': 3,\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'total_pages': 27,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941},\n",
       "  'id': 'doc_3878b72f_752',\n",
       "  'distance': 0.8619899749755859,\n",
       "  'similarity_score': 0.5370598435401917},\n",
       " {'rank': 4,\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'total_pages': 27,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941},\n",
       "  'id': 'doc_f65542b5_1067',\n",
       "  'distance': 0.8619899749755859,\n",
       "  'similarity_score': 0.5370598435401917},\n",
       " {'rank': 5,\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'total_pages': 27,\n",
       "   'page': 4,\n",
       "   'page_label': '5',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 937},\n",
       "  'id': 'doc_073e1199_142',\n",
       "  'distance': 0.89375239610672,\n",
       "  'similarity_score': 0.5280521512031555}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63f413",
   "metadata": {},
   "source": [
    "### RAG Pipeline - VectorDB to LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f43d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\RAG\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60eb632c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if os.getenv(\"GROQ_API_KEY\"):\n",
    "    print(\"API key loaded successfully\")\n",
    "else:\n",
    "    print(\"API key NOT found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca27fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e689862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"gemma2-9b-it\", api_key: str =None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30a09375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: gemma2-9b-it\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0948819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Retrieving documents for query: 'Unified Multi-task Learning Framework'\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "✅ Retrieved 5 docs (after filtering).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'total_pages': 27,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941},\n",
       "  'id': 'doc_bdd94aad_122',\n",
       "  'distance': 0.8619899749755859,\n",
       "  'similarity_score': 0.5370598435401917},\n",
       " {'rank': 2,\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'total_pages': 27,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941},\n",
       "  'id': 'doc_44ad8b06_437',\n",
       "  'distance': 0.8619899749755859,\n",
       "  'similarity_score': 0.5370598435401917},\n",
       " {'rank': 3,\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'total_pages': 27,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941},\n",
       "  'id': 'doc_3878b72f_752',\n",
       "  'distance': 0.8619899749755859,\n",
       "  'similarity_score': 0.5370598435401917},\n",
       " {'rank': 4,\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'total_pages': 27,\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941},\n",
       "  'id': 'doc_f65542b5_1067',\n",
       "  'distance': 0.8619899749755859,\n",
       "  'similarity_score': 0.5370598435401917},\n",
       " {'rank': 5,\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'total_pages': 27,\n",
       "   'page': 4,\n",
       "   'page_label': '5',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 937},\n",
       "  'id': 'doc_073e1199_142',\n",
       "  'distance': 0.89375239610672,\n",
       "  'similarity_score': 0.5280521512031555}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb210c56",
   "metadata": {},
   "source": [
    "### Integration VectorDB Context Pipeline With LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3804431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG Pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name = \"llama-3.1-8b-instant\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query, retriever, llm):\n",
    "    result = retriever.retrieve(query)\n",
    "\n",
    "    if not result or \"content\" not in result:\n",
    "        return {\"answer\": \"No relevant context found.\", \"similarity_score\": 0.0}\n",
    "\n",
    "    context = result[\"content\"]\n",
    "    score = result[\"similarity_score\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Use the following retrieved context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"similarity_score\": score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f450cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7d41511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Retrieving documents for query: 'What is attention mechanism?'\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "✅ Retrieved 5 docs (after filtering).\n",
      "{'answer': 'No relevant context found.', 'similarity_score': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"What is attention mechanism?\", rag_retriever, llm)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965e17d",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca499fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Retrieving documents for query: 'Hard Negative Mining Techniques'\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "✅ Retrieved 3 docs (after filtering).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Hard negatives serve as essential components in contrastive learning for retrieval model training. Several techniques have been proposed to mine hard negatives, including:\n",
      "\n",
      "1. **ANCE**: Proposed an asynchronous ANN indexing mechanism that periodically updates hard negatives using checkpoint states to maintain optimally challenging samples.\n",
      "2. **Conan-Embedding**: Implemented a dynamic hard negative sampling strategy by excluding and refreshing samples when their scores fall below a threshold.\n",
      "3. **NV-Retriever**: Proposed positive-aware negative mining, introducing TopK-MarginPos and TopKPercPos filtering criteria to minimize false negatives.\n",
      "4. **LGAI-Embedding**: Built upon NV-Retriever's strategy with adaptive margin-based mining strategies, employing ANNA IR as a teacher retrieval model to identify high-quality hard negatives while using TopKPercPos filtering to eliminate false negatives.\n",
      "Sources: [{'source': 'embeddings.pdf', 'page': 4, 'score': 0.6049174666404724, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}, {'source': 'embeddings.pdf', 'page': 4, 'score': 0.6049174666404724, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}, {'source': 'embeddings.pdf', 'page': 4, 'score': 0.6049174666404724, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}]\n",
      "Confidence: 0.6049174666404724\n",
      "Context Preview: QZhou-Embedding Technical Report\n",
      " Kingsoft AI\n",
      "2.4 Hard Negative Mining Techniques\n",
      "Hard negatives serve as essential components in contrastive lear ning for retrieval model\n",
      "training. Early work like ANCE[\n",
      "46] proposed an asynchronous ANN indexing mech-\n",
      "anism that periodically updates hard negatives u\n"
     ]
    }
   ],
   "source": [
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold= min_score)\n",
    "    if not results:\n",
    "        return {'answer':'No relevant context found.', 'sources':[], 'confidence':0.0, 'context': ''}\n",
    "    \n",
    "    #Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page','unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    }for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "\n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question accurately and concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "\n",
    "    output = {\n",
    "        'answer' : response.content,\n",
    "        'sources':sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "result = rag_advanced(\"Hard Negative Mining Techniques\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9e7a567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Retrieving documents for query: 'what is attention is all you need'\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "✅ Retrieved 3 docs (after filtering).\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "3.2 Attenti"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vector"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "Question: what is attention is all you need\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: \"Attention is All You Need\" is a research paper published in 2017 by Vaswani et al. that introduced the Transformer model, a type of neural network architecture that uses self-attention mechanisms to process sequential data.\n",
      "\n",
      "Citations:\n",
      "[1] attention.pdf (page 2)\n",
      "[2] attention.pdf (page 2)\n",
      "[3] attention.pdf (page 2)\n",
      "Summary: The \"Attention is All You Need\" research paper, published in 2017 by Vaswani et al., introduced the Transformer model, a neural network architecture that uses self-attention mechanisms. This architecture is designed to process sequential data, offering an alternative to traditional recurrent neural networks (RNNs).\n",
      "History: {'question': 'what is attention is all you need', 'answer': '\"Attention is All You Need\" is a research paper published in 2017 by Vaswani et al. that introduced the Transformer model, a type of neural network architecture that uses self-attention mechanisms to process sequential data.', 'sources': [{'source': 'attention.pdf', 'page': 2, 'score': 0.5376213192939758, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}, {'source': 'attention.pdf', 'page': 2, 'score': 0.5376213192939758, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}, {'source': 'attention.pdf', 'page': 2, 'score': 0.5376213192939758, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}], 'summary': 'The \"Attention is All You Need\" research paper, published in 2017 by Vaswani et al., introduced the Transformer model, a neural network architecture that uses self-attention mechanisms. This architecture is designed to process sequential data, offering an alternative to traditional recurrent neural networks (RNNs).'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is attention is all you need\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d5e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
